{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMarZ/+9R2A4GrRBqOc1zSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jdzubina/Python---Data-and-AI/blob/main/SingleNeuron_neuralnetworks_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning**"
      ],
      "metadata": {
        "id": "fCT88cEdlj_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning is a type of machine learning that uses layers of neural networks. Hence \"deep\". It originated from the concept of emulating the human mind and brain. These networks excel in handling large datasets and complex patterns, making them ideal for tasks like image recognition, natural language processing, and IoT applications. New techniques like convolutional neural networks (CNNs) and recurrent neural networks (RNNs)have advanced these networks even futher. The growth of deep learning has been helped by the availability of large datasets and faster computing power from GPUs, allowing for more efficient training of these deep networks."
      ],
      "metadata": {
        "id": "hpdnGWKAlnuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Neuron Code Sample**"
      ],
      "metadata": {
        "id": "nNKhckUd3Syq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By-Bg1V6le-y"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the artificial neuron\n",
        "class ArtificialNeuron:\n",
        "    def __init__(self,N=2, act_func=tf.nn.sigmoid, learning_rate= 0.001):\n",
        "        self.N = N # Number of inputs to the neuron\n",
        "        self.act_fn = act_func # Activation function\n",
        "\n",
        "        # Build the graph for a single neuron\n",
        "        self.W = tf.Variable(tf.random_normal([N,1], stddev=2, seed = 0))\n",
        "        self.bias = tf.Variable(0.0, dtype=tf.float32)\n",
        "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,N])\n",
        "        self.y = tf.placeholder(tf.float32, name='Y')\n",
        "\n",
        "        # Compute the activity of the neuron\n",
        "        activity = tf.matmul(self.X, self.W) + self.bias\n",
        "        self.y_hat = self.act_fn(activity)\n",
        "\n",
        "        # Compute the error\n",
        "        error = self.y - self.y_hat\n",
        "\n",
        "        # Define the loss function (Mean Squared Error)\n",
        "        self.loss = tf.reduce_mean(tf.square(error))\n",
        "        # Define the optimizer (Gradient Descent)\n",
        "        self.opt =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
        "\n",
        "        # Initialize all variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Create a TensorFlow session and initialize variables\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        # Train the neuron on the given data.\n",
        "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
        "        epoch = 0\n",
        "        X, Y = shuffle(X,Y) # Shuffle the training data\n",
        "        loss = [] # List to store training loss\n",
        "        loss_val = [] # List to store validation loss\n",
        "        while epoch < epochs:\n",
        "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)\n",
        "            _, l = self.sess.run([self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
        "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
        "\n",
        "            # Store the losses\n",
        "            loss.append(l)\n",
        "            loss_val.append(l_val)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
        "                      format(epoch,epochs,l, l_val ))\n",
        "\n",
        "            epoch += 1\n",
        "        return loss, loss_val\n",
        "    # Make predictions using the trained neuron.\n",
        "    def predict(self, X):\n",
        "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
      ],
      "metadata": {
        "id": "3ZMb2vOH10Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset from the Excel file\n",
        "filename = 'Folds5x2_pp.xlsx'\n",
        "df = pd.read_excel(filename, sheet_name='Sheet1')  # Read the Excel file into a DataFrame\n",
        "\n",
        "# Split the data into features (X) and target variable (Y)\n",
        "X, Y = df[['AT', 'V', 'AP', 'RH']], df['PE']\n",
        "\n",
        "# Initialize a MinMaxScaler for normalizing the features\n",
        "scaler = MinMaxScaler()\n",
        "X_new = scaler.fit_transform(X)  # Normalize the features\n",
        "\n",
        "# Initialize another MinMaxScaler for normalizing the target variable\n",
        "target_scaler = MinMaxScaler()\n",
        "Y_new = target_scaler.fit_transform(Y.values.reshape(-1, 1))  # Normalize the target variable\n",
        "\n",
        "# Split the normalized data into training and validation sets\n",
        "X_train, X_val, Y_train, y_val = train_test_split(X_new, Y_new, test_size=0.4, random_state=333)\n"
      ],
      "metadata": {
        "id": "7_QrIDZp3bDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, d = X_train.shape\n",
        "# Initialize the Artificial Neuron model with the determined number of inputs\n",
        "model = ArtificialNeuron(N=d)\n",
        "\n",
        "# Train the model on the training data\n",
        "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 30000)\n",
        "\n",
        "# Plot the training and validation\n",
        "plt.plot(loss, label=\"Taining Loss\")\n",
        "plt.plot(loss_val, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mean Square Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uZ4pCpI53rTI",
        "outputId": "8ee8f209-2813-45c9-fb26-99ba04665360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/30000  training loss: 0.288099080324173 Validation loss 0.2842104136943817\n",
            "Epoch 10/30000  training loss: 0.28804391622543335 Validation loss 0.28415507078170776\n",
            "Epoch 20/30000  training loss: 0.28798869252204895 Validation loss 0.2840996980667114\n",
            "Epoch 30/30000  training loss: 0.287933349609375 Validation loss 0.2840442359447479\n",
            "Epoch 40/30000  training loss: 0.28787800669670105 Validation loss 0.28398874402046204\n",
            "Epoch 50/30000  training loss: 0.28782257437705994 Validation loss 0.2839331328868866\n",
            "Epoch 60/30000  training loss: 0.28776705265045166 Validation loss 0.28387755155563354\n",
            "Epoch 70/30000  training loss: 0.2877114713191986 Validation loss 0.28382179141044617\n",
            "Epoch 80/30000  training loss: 0.2876558303833008 Validation loss 0.2837660014629364\n",
            "Epoch 90/30000  training loss: 0.2876001000404358 Validation loss 0.2837101221084595\n",
            "Epoch 100/30000  training loss: 0.2875443398952484 Validation loss 0.28365418314933777\n",
            "Epoch 110/30000  training loss: 0.2874884605407715 Validation loss 0.2835982143878937\n",
            "Epoch 120/30000  training loss: 0.28743264079093933 Validation loss 0.2835421562194824\n",
            "Epoch 130/30000  training loss: 0.28737664222717285 Validation loss 0.2834860384464264\n",
            "Epoch 140/30000  training loss: 0.2873205840587616 Validation loss 0.2834298610687256\n",
            "Epoch 150/30000  training loss: 0.2872644364833832 Validation loss 0.2833735942840576\n",
            "Epoch 160/30000  training loss: 0.2872082591056824 Validation loss 0.2833172678947449\n",
            "Epoch 170/30000  training loss: 0.2871519923210144 Validation loss 0.28326088190078735\n",
            "Epoch 180/30000  training loss: 0.28709566593170166 Validation loss 0.28320440649986267\n",
            "Epoch 190/30000  training loss: 0.2870393395423889 Validation loss 0.2831479012966156\n",
            "Epoch 200/30000  training loss: 0.28698286414146423 Validation loss 0.28309133648872375\n",
            "Epoch 210/30000  training loss: 0.2869263291358948 Validation loss 0.2830345928668976\n",
            "Epoch 220/30000  training loss: 0.28686973452568054 Validation loss 0.2829778492450714\n",
            "Epoch 230/30000  training loss: 0.28681308031082153 Validation loss 0.28292107582092285\n",
            "Epoch 240/30000  training loss: 0.2867561876773834 Validation loss 0.2828640937805176\n",
            "Epoch 250/30000  training loss: 0.2866992950439453 Validation loss 0.28280696272850037\n",
            "Epoch 260/30000  training loss: 0.28664231300354004 Validation loss 0.28274986147880554\n",
            "Epoch 270/30000  training loss: 0.2865852415561676 Validation loss 0.28269267082214355\n",
            "Epoch 280/30000  training loss: 0.286528080701828 Validation loss 0.2826354205608368\n",
            "Epoch 290/30000  training loss: 0.28647086024284363 Validation loss 0.2825780212879181\n",
            "Epoch 300/30000  training loss: 0.28641360998153687 Validation loss 0.2825206220149994\n",
            "Epoch 310/30000  training loss: 0.28635627031326294 Validation loss 0.2824631631374359\n",
            "Epoch 320/30000  training loss: 0.28629887104034424 Validation loss 0.2824055850505829\n",
            "Epoch 330/30000  training loss: 0.28624141216278076 Validation loss 0.28234797716140747\n",
            "Epoch 340/30000  training loss: 0.28618383407592773 Validation loss 0.2822902798652649\n",
            "Epoch 350/30000  training loss: 0.28612619638442993 Validation loss 0.28223252296447754\n",
            "Epoch 360/30000  training loss: 0.28606849908828735 Validation loss 0.2821747064590454\n",
            "Epoch 370/30000  training loss: 0.2860107421875 Validation loss 0.28211677074432373\n",
            "Epoch 380/30000  training loss: 0.28595292568206787 Validation loss 0.28205883502960205\n",
            "Epoch 390/30000  training loss: 0.2858950197696686 Validation loss 0.2820007801055908\n",
            "Epoch 400/30000  training loss: 0.2858370542526245 Validation loss 0.2819426953792572\n",
            "Epoch 410/30000  training loss: 0.2857789993286133 Validation loss 0.28188449144363403\n",
            "Epoch 420/30000  training loss: 0.2857208847999573 Validation loss 0.2818262279033661\n",
            "Epoch 430/30000  training loss: 0.2856627106666565 Validation loss 0.28176790475845337\n",
            "Epoch 440/30000  training loss: 0.28560447692871094 Validation loss 0.2817095220088959\n",
            "Epoch 450/30000  training loss: 0.2855461537837982 Validation loss 0.281651109457016\n",
            "Epoch 460/30000  training loss: 0.28548768162727356 Validation loss 0.2815924882888794\n",
            "Epoch 470/30000  training loss: 0.28542909026145935 Validation loss 0.28153371810913086\n",
            "Epoch 480/30000  training loss: 0.28537043929100037 Validation loss 0.28147491812705994\n",
            "Epoch 490/30000  training loss: 0.28531166911125183 Validation loss 0.28141605854034424\n",
            "Epoch 500/30000  training loss: 0.2852528989315033 Validation loss 0.2813571095466614\n",
            "Epoch 510/30000  training loss: 0.2851940095424652 Validation loss 0.28129810094833374\n",
            "Epoch 520/30000  training loss: 0.28513503074645996 Validation loss 0.28123900294303894\n",
            "Epoch 530/30000  training loss: 0.28507599234580994 Validation loss 0.28117984533309937\n",
            "Epoch 540/30000  training loss: 0.28501686453819275 Validation loss 0.2811205983161926\n",
            "Epoch 550/30000  training loss: 0.28495773673057556 Validation loss 0.2810613214969635\n",
            "Epoch 560/30000  training loss: 0.2848985195159912 Validation loss 0.2810019254684448\n",
            "Epoch 570/30000  training loss: 0.2848391830921173 Validation loss 0.28094246983528137\n",
            "Epoch 580/30000  training loss: 0.284779816865921 Validation loss 0.28088295459747314\n",
            "Epoch 590/30000  training loss: 0.2847203314304352 Validation loss 0.28082334995269775\n",
            "Epoch 600/30000  training loss: 0.28466081619262695 Validation loss 0.2807636857032776\n",
            "Epoch 610/30000  training loss: 0.2846011817455292 Validation loss 0.28070396184921265\n",
            "Epoch 620/30000  training loss: 0.2845415472984314 Validation loss 0.28064417839050293\n",
            "Epoch 630/30000  training loss: 0.28448179364204407 Validation loss 0.28058427572250366\n",
            "Epoch 640/30000  training loss: 0.28442198038101196 Validation loss 0.280524343252182\n",
            "Epoch 650/30000  training loss: 0.2843620777130127 Validation loss 0.2804643213748932\n",
            "Epoch 660/30000  training loss: 0.28430208563804626 Validation loss 0.2804042100906372\n",
            "Epoch 670/30000  training loss: 0.28424206376075745 Validation loss 0.28034406900405884\n",
            "Epoch 680/30000  training loss: 0.2841819226741791 Validation loss 0.28028377890586853\n",
            "Epoch 690/30000  training loss: 0.284121572971344 Validation loss 0.2802233397960663\n",
            "Epoch 700/30000  training loss: 0.2840612232685089 Validation loss 0.28016284108161926\n",
            "Epoch 710/30000  training loss: 0.2840007543563843 Validation loss 0.2801022231578827\n",
            "Epoch 720/30000  training loss: 0.2839401960372925 Validation loss 0.28004151582717896\n",
            "Epoch 730/30000  training loss: 0.2838795781135559 Validation loss 0.27998074889183044\n",
            "Epoch 740/30000  training loss: 0.28381890058517456 Validation loss 0.27991998195648193\n",
            "Epoch 750/30000  training loss: 0.28375816345214844 Validation loss 0.27985912561416626\n",
            "Epoch 760/30000  training loss: 0.28369730710983276 Validation loss 0.2797981798648834\n",
            "Epoch 770/30000  training loss: 0.2836364209651947 Validation loss 0.27973708510398865\n",
            "Epoch 780/30000  training loss: 0.2835754156112671 Validation loss 0.2796759605407715\n",
            "Epoch 790/30000  training loss: 0.2835143506526947 Validation loss 0.27961480617523193\n",
            "Epoch 800/30000  training loss: 0.28345322608947754 Validation loss 0.2795535624027252\n",
            "Epoch 810/30000  training loss: 0.2833919823169708 Validation loss 0.27949219942092896\n",
            "Epoch 820/30000  training loss: 0.2833307981491089 Validation loss 0.2794308364391327\n",
            "Epoch 830/30000  training loss: 0.28326940536499023 Validation loss 0.2793693244457245\n",
            "Epoch 840/30000  training loss: 0.2832079827785492 Validation loss 0.2793077826499939\n",
            "Epoch 850/30000  training loss: 0.2831464409828186 Validation loss 0.27924615144729614\n",
            "Epoch 860/30000  training loss: 0.2830848693847656 Validation loss 0.2791844606399536\n",
            "Epoch 870/30000  training loss: 0.28302323818206787 Validation loss 0.2791226804256439\n",
            "Epoch 880/30000  training loss: 0.28296148777008057 Validation loss 0.27906084060668945\n",
            "Epoch 890/30000  training loss: 0.2828997075557709 Validation loss 0.2789989113807678\n",
            "Epoch 900/30000  training loss: 0.2828376889228821 Validation loss 0.2789367735385895\n",
            "Epoch 910/30000  training loss: 0.2827756106853485 Validation loss 0.27887457609176636\n",
            "Epoch 920/30000  training loss: 0.28271347284317017 Validation loss 0.2788122892379761\n",
            "Epoch 930/30000  training loss: 0.28265124559402466 Validation loss 0.278749942779541\n",
            "Epoch 940/30000  training loss: 0.2825889587402344 Validation loss 0.2786875069141388\n",
            "Epoch 950/30000  training loss: 0.28252652287483215 Validation loss 0.2786250114440918\n",
            "Epoch 960/30000  training loss: 0.28246405720710754 Validation loss 0.2785623371601105\n",
            "Epoch 970/30000  training loss: 0.28240153193473816 Validation loss 0.2784997224807739\n",
            "Epoch 980/30000  training loss: 0.2823388874530792 Validation loss 0.2784369885921478\n",
            "Epoch 990/30000  training loss: 0.2822762131690979 Validation loss 0.27837419509887695\n",
            "Epoch 1000/30000  training loss: 0.2822134494781494 Validation loss 0.2783113121986389\n",
            "Epoch 1010/30000  training loss: 0.2821505665779114 Validation loss 0.2782483398914337\n",
            "Epoch 1020/30000  training loss: 0.28208768367767334 Validation loss 0.27818527817726135\n",
            "Epoch 1030/30000  training loss: 0.28202465176582336 Validation loss 0.2781221270561218\n",
            "Epoch 1040/30000  training loss: 0.281961590051651 Validation loss 0.2780589461326599\n",
            "Epoch 1050/30000  training loss: 0.2818984091281891 Validation loss 0.2779957056045532\n",
            "Epoch 1060/30000  training loss: 0.2818352282047272 Validation loss 0.277932345867157\n",
            "Epoch 1070/30000  training loss: 0.28177186846733093 Validation loss 0.27786892652511597\n",
            "Epoch 1080/30000  training loss: 0.2817085087299347 Validation loss 0.2778054475784302\n",
            "Epoch 1090/30000  training loss: 0.2816450297832489 Validation loss 0.27774184942245483\n",
            "Epoch 1100/30000  training loss: 0.28158149123191833 Validation loss 0.27767816185951233\n",
            "Epoch 1110/30000  training loss: 0.28151780366897583 Validation loss 0.27761438488960266\n",
            "Epoch 1120/30000  training loss: 0.2814539670944214 Validation loss 0.27755042910575867\n",
            "Epoch 1130/30000  training loss: 0.28139007091522217 Validation loss 0.2774863839149475\n",
            "Epoch 1140/30000  training loss: 0.2813260853290558 Validation loss 0.2774222791194916\n",
            "Epoch 1150/30000  training loss: 0.28126195073127747 Validation loss 0.27735814452171326\n",
            "Epoch 1160/30000  training loss: 0.28119784593582153 Validation loss 0.2772938013076782\n",
            "Epoch 1170/30000  training loss: 0.28113362193107605 Validation loss 0.27722951769828796\n",
            "Epoch 1180/30000  training loss: 0.2810693085193634 Validation loss 0.27716514468193054\n",
            "Epoch 1190/30000  training loss: 0.28100496530532837 Validation loss 0.2771005928516388\n",
            "Epoch 1200/30000  training loss: 0.2809404730796814 Validation loss 0.27703604102134705\n",
            "Epoch 1210/30000  training loss: 0.28087592124938965 Validation loss 0.27697136998176575\n",
            "Epoch 1220/30000  training loss: 0.2808113396167755 Validation loss 0.2769066393375397\n",
            "Epoch 1230/30000  training loss: 0.2807466387748718 Validation loss 0.2768418788909912\n",
            "Epoch 1240/30000  training loss: 0.28068187832832336 Validation loss 0.2767770290374756\n",
            "Epoch 1250/30000  training loss: 0.28061699867248535 Validation loss 0.276712030172348\n",
            "Epoch 1260/30000  training loss: 0.28055211901664734 Validation loss 0.2766469717025757\n",
            "Epoch 1270/30000  training loss: 0.280487060546875 Validation loss 0.27658185362815857\n",
            "Epoch 1280/30000  training loss: 0.2804219722747803 Validation loss 0.2765166461467743\n",
            "Epoch 1290/30000  training loss: 0.2803567945957184 Validation loss 0.27645143866539\n",
            "Epoch 1300/30000  training loss: 0.2802915871143341 Validation loss 0.2763860523700714\n",
            "Epoch 1310/30000  training loss: 0.28022629022598267 Validation loss 0.27632060647010803\n",
            "Epoch 1320/30000  training loss: 0.28016069531440735 Validation loss 0.2762549817562103\n",
            "Epoch 1330/30000  training loss: 0.28009510040283203 Validation loss 0.27618923783302307\n",
            "Epoch 1340/30000  training loss: 0.28002938628196716 Validation loss 0.27612343430519104\n",
            "Epoch 1350/30000  training loss: 0.27996358275413513 Validation loss 0.27605751156806946\n",
            "Epoch 1360/30000  training loss: 0.2798977196216583 Validation loss 0.2759915888309479\n",
            "Epoch 1370/30000  training loss: 0.27983179688453674 Validation loss 0.27592551708221436\n",
            "Epoch 1380/30000  training loss: 0.2797657549381256 Validation loss 0.27585935592651367\n",
            "Epoch 1390/30000  training loss: 0.2796996533870697 Validation loss 0.2757931649684906\n",
            "Epoch 1400/30000  training loss: 0.27963346242904663 Validation loss 0.27572688460350037\n",
            "Epoch 1410/30000  training loss: 0.2795672118663788 Validation loss 0.2756604850292206\n",
            "Epoch 1420/30000  training loss: 0.27950090169906616 Validation loss 0.2755940854549408\n",
            "Epoch 1430/30000  training loss: 0.279434472322464 Validation loss 0.27552756667137146\n",
            "Epoch 1440/30000  training loss: 0.27936792373657227 Validation loss 0.2754609286785126\n",
            "Epoch 1450/30000  training loss: 0.27930134534835815 Validation loss 0.2753942608833313\n",
            "Epoch 1460/30000  training loss: 0.2792346775531769 Validation loss 0.2753274738788605\n",
            "Epoch 1470/30000  training loss: 0.27916795015335083 Validation loss 0.2752606272697449\n",
            "Epoch 1480/30000  training loss: 0.27910110354423523 Validation loss 0.2751937210559845\n",
            "Epoch 1490/30000  training loss: 0.27903416752815247 Validation loss 0.27512672543525696\n",
            "Epoch 1500/30000  training loss: 0.2789672315120697 Validation loss 0.2750595808029175\n",
            "Epoch 1510/30000  training loss: 0.2789000868797302 Validation loss 0.274992436170578\n",
            "Epoch 1520/30000  training loss: 0.2788328230381012 Validation loss 0.2749250531196594\n",
            "Epoch 1530/30000  training loss: 0.27876541018486023 Validation loss 0.2748575508594513\n",
            "Epoch 1540/30000  training loss: 0.27869799733161926 Validation loss 0.2747899889945984\n",
            "Epoch 1550/30000  training loss: 0.27863043546676636 Validation loss 0.2747223377227783\n",
            "Epoch 1560/30000  training loss: 0.2785628139972687 Validation loss 0.2746545970439911\n",
            "Epoch 1570/30000  training loss: 0.27849507331848145 Validation loss 0.2745867967605591\n",
            "Epoch 1580/30000  training loss: 0.2784273028373718 Validation loss 0.2745189070701599\n",
            "Epoch 1590/30000  training loss: 0.27835941314697266 Validation loss 0.27445095777511597\n",
            "Epoch 1600/30000  training loss: 0.2782914936542511 Validation loss 0.27438294887542725\n",
            "Epoch 1610/30000  training loss: 0.27822345495224 Validation loss 0.2743147611618042\n",
            "Epoch 1620/30000  training loss: 0.27815529704093933 Validation loss 0.27424657344818115\n",
            "Epoch 1630/30000  training loss: 0.2780870497226715 Validation loss 0.27417826652526855\n",
            "Epoch 1640/30000  training loss: 0.2780188024044037 Validation loss 0.2741098701953888\n",
            "Epoch 1650/30000  training loss: 0.2779504060745239 Validation loss 0.27404147386550903\n",
            "Epoch 1660/30000  training loss: 0.27788203954696655 Validation loss 0.27397289872169495\n",
            "Epoch 1670/30000  training loss: 0.27781346440315247 Validation loss 0.27390429377555847\n",
            "Epoch 1680/30000  training loss: 0.2777448296546936 Validation loss 0.27383553981781006\n",
            "Epoch 1690/30000  training loss: 0.27767613530158997 Validation loss 0.27376678586006165\n",
            "Epoch 1700/30000  training loss: 0.2776073217391968 Validation loss 0.2736979126930237\n",
            "Epoch 1710/30000  training loss: 0.2775384485721588 Validation loss 0.2736288905143738\n",
            "Epoch 1720/30000  training loss: 0.27746933698654175 Validation loss 0.27355971932411194\n",
            "Epoch 1730/30000  training loss: 0.2774001955986023 Validation loss 0.27349042892456055\n",
            "Epoch 1740/30000  training loss: 0.2773309051990509 Validation loss 0.27342110872268677\n",
            "Epoch 1750/30000  training loss: 0.27726152539253235 Validation loss 0.27335166931152344\n",
            "Epoch 1760/30000  training loss: 0.2771921455860138 Validation loss 0.27328214049339294\n",
            "Epoch 1770/30000  training loss: 0.2771226167678833 Validation loss 0.2732125222682953\n",
            "Epoch 1780/30000  training loss: 0.27705302834510803 Validation loss 0.27314287424087524\n",
            "Epoch 1790/30000  training loss: 0.2769833207130432 Validation loss 0.27307307720184326\n",
            "Epoch 1800/30000  training loss: 0.27691352367401123 Validation loss 0.2730032503604889\n",
            "Epoch 1810/30000  training loss: 0.27684369683265686 Validation loss 0.27293330430984497\n",
            "Epoch 1820/30000  training loss: 0.27677375078201294 Validation loss 0.2728632688522339\n",
            "Epoch 1830/30000  training loss: 0.27670374512672424 Validation loss 0.272793173789978\n",
            "Epoch 1840/30000  training loss: 0.276633620262146 Validation loss 0.272722989320755\n",
            "Epoch 1850/30000  training loss: 0.2765634059906006 Validation loss 0.2726527154445648\n",
            "Epoch 1860/30000  training loss: 0.2764931321144104 Validation loss 0.27258235216140747\n",
            "Epoch 1870/30000  training loss: 0.27642273902893066 Validation loss 0.27251186966896057\n",
            "Epoch 1880/30000  training loss: 0.27635228633880615 Validation loss 0.2724413573741913\n",
            "Epoch 1890/30000  training loss: 0.27628183364868164 Validation loss 0.2723708152770996\n",
            "Epoch 1900/30000  training loss: 0.2762111723423004 Validation loss 0.2723000943660736\n",
            "Epoch 1910/30000  training loss: 0.27614036202430725 Validation loss 0.2722291648387909\n",
            "Epoch 1920/30000  training loss: 0.27606940269470215 Validation loss 0.27215811610221863\n",
            "Epoch 1930/30000  training loss: 0.2759983539581299 Validation loss 0.2720870077610016\n",
            "Epoch 1940/30000  training loss: 0.27592727541923523 Validation loss 0.27201583981513977\n",
            "Epoch 1950/30000  training loss: 0.275856077671051 Validation loss 0.2719446122646332\n",
            "Epoch 1960/30000  training loss: 0.27578479051589966 Validation loss 0.27187326550483704\n",
            "Epoch 1970/30000  training loss: 0.27571341395378113 Validation loss 0.27180179953575134\n",
            "Epoch 1980/30000  training loss: 0.27564191818237305 Validation loss 0.2717302739620209\n",
            "Epoch 1990/30000  training loss: 0.2755703628063202 Validation loss 0.27165865898132324\n",
            "Epoch 2000/30000  training loss: 0.27549877762794495 Validation loss 0.27158695459365845\n",
            "Epoch 2010/30000  training loss: 0.27542704343795776 Validation loss 0.2715151607990265\n",
            "Epoch 2020/30000  training loss: 0.2753552794456482 Validation loss 0.27144330739974976\n",
            "Epoch 2030/30000  training loss: 0.2752833664417267 Validation loss 0.27137133479118347\n",
            "Epoch 2040/30000  training loss: 0.2752113938331604 Validation loss 0.2712993323802948\n",
            "Epoch 2050/30000  training loss: 0.27513930201530457 Validation loss 0.2712271511554718\n",
            "Epoch 2060/30000  training loss: 0.27506721019744873 Validation loss 0.2711549401283264\n",
            "Epoch 2070/30000  training loss: 0.27499496936798096 Validation loss 0.27108266949653625\n",
            "Epoch 2080/30000  training loss: 0.274922639131546 Validation loss 0.27101027965545654\n",
            "Epoch 2090/30000  training loss: 0.2748502194881439 Validation loss 0.27093780040740967\n",
            "Epoch 2100/30000  training loss: 0.2747775614261627 Validation loss 0.2708650529384613\n",
            "Epoch 2110/30000  training loss: 0.27470484375953674 Validation loss 0.27079224586486816\n",
            "Epoch 2120/30000  training loss: 0.2746320068836212 Validation loss 0.27071934938430786\n",
            "Epoch 2130/30000  training loss: 0.2745591104030609 Validation loss 0.2706463932991028\n",
            "Epoch 2140/30000  training loss: 0.27448612451553345 Validation loss 0.27057334780693054\n",
            "Epoch 2150/30000  training loss: 0.27441298961639404 Validation loss 0.27050018310546875\n",
            "Epoch 2160/30000  training loss: 0.27433982491493225 Validation loss 0.27042698860168457\n",
            "Epoch 2170/30000  training loss: 0.2742665708065033 Validation loss 0.2703535854816437\n",
            "Epoch 2180/30000  training loss: 0.2741932272911072 Validation loss 0.27028021216392517\n",
            "Epoch 2190/30000  training loss: 0.2741197943687439 Validation loss 0.2702067494392395\n",
            "Epoch 2200/30000  training loss: 0.27404627203941345 Validation loss 0.2701331377029419\n",
            "Epoch 2210/30000  training loss: 0.27397263050079346 Validation loss 0.2700594961643219\n",
            "Epoch 2220/30000  training loss: 0.2738989293575287 Validation loss 0.26998573541641235\n",
            "Epoch 2230/30000  training loss: 0.27382513880729675 Validation loss 0.26991185545921326\n",
            "Epoch 2240/30000  training loss: 0.27375125885009766 Validation loss 0.269837886095047\n",
            "Epoch 2250/30000  training loss: 0.273677259683609 Validation loss 0.26976388692855835\n",
            "Epoch 2260/30000  training loss: 0.2736032009124756 Validation loss 0.26968979835510254\n",
            "Epoch 2270/30000  training loss: 0.2735290229320526 Validation loss 0.2696155607700348\n",
            "Epoch 2280/30000  training loss: 0.27345478534698486 Validation loss 0.2695412039756775\n",
            "Epoch 2290/30000  training loss: 0.2733802795410156 Validation loss 0.26946666836738586\n",
            "Epoch 2300/30000  training loss: 0.2733057141304016 Validation loss 0.26939207315444946\n",
            "Epoch 2310/30000  training loss: 0.27323102951049805 Validation loss 0.2693173587322235\n",
            "Epoch 2320/30000  training loss: 0.2731562852859497 Validation loss 0.269242525100708\n",
            "Epoch 2330/30000  training loss: 0.2730814516544342 Validation loss 0.2691676616668701\n",
            "Epoch 2340/30000  training loss: 0.27300649881362915 Validation loss 0.2690926790237427\n",
            "Epoch 2350/30000  training loss: 0.2729315161705017 Validation loss 0.26901760697364807\n",
            "Epoch 2360/30000  training loss: 0.27285635471343994 Validation loss 0.2689424455165863\n",
            "Epoch 2370/30000  training loss: 0.27278122305870056 Validation loss 0.2688671946525574\n",
            "Epoch 2380/30000  training loss: 0.27270591259002686 Validation loss 0.26879188418388367\n",
            "Epoch 2390/30000  training loss: 0.2726305425167084 Validation loss 0.268716424703598\n",
            "Epoch 2400/30000  training loss: 0.27255505323410034 Validation loss 0.2686408758163452\n",
            "Epoch 2410/30000  training loss: 0.27247947454452515 Validation loss 0.2685653269290924\n",
            "Epoch 2420/30000  training loss: 0.2724038064479828 Validation loss 0.26848962903022766\n",
            "Epoch 2430/30000  training loss: 0.27232807874679565 Validation loss 0.26841384172439575\n",
            "Epoch 2440/30000  training loss: 0.27225223183631897 Validation loss 0.2683379650115967\n",
            "Epoch 2450/30000  training loss: 0.2721762955188751 Validation loss 0.26826199889183044\n",
            "Epoch 2460/30000  training loss: 0.2721002697944641 Validation loss 0.26818588376045227\n",
            "Epoch 2470/30000  training loss: 0.272024005651474 Validation loss 0.26810961961746216\n",
            "Epoch 2480/30000  training loss: 0.2719476819038391 Validation loss 0.2680332660675049\n",
            "Epoch 2490/30000  training loss: 0.2718712091445923 Validation loss 0.26795676350593567\n",
            "Epoch 2500/30000  training loss: 0.2717946469783783 Validation loss 0.2678801417350769\n",
            "Epoch 2510/30000  training loss: 0.2717180848121643 Validation loss 0.26780351996421814\n",
            "Epoch 2520/30000  training loss: 0.2716413140296936 Validation loss 0.26772674918174744\n",
            "Epoch 2530/30000  training loss: 0.2715644836425781 Validation loss 0.26764991879463196\n",
            "Epoch 2540/30000  training loss: 0.27148762345314026 Validation loss 0.26757296919822693\n",
            "Epoch 2550/30000  training loss: 0.27141061425209045 Validation loss 0.2674959599971771\n",
            "Epoch 2560/30000  training loss: 0.2713335156440735 Validation loss 0.2674188017845154\n",
            "Epoch 2570/30000  training loss: 0.27125632762908936 Validation loss 0.26734158396720886\n",
            "Epoch 2580/30000  training loss: 0.27117905020713806 Validation loss 0.26726433634757996\n",
            "Epoch 2590/30000  training loss: 0.271101713180542 Validation loss 0.2671869397163391\n",
            "Epoch 2600/30000  training loss: 0.271024227142334 Validation loss 0.2671094536781311\n",
            "Epoch 2610/30000  training loss: 0.2709466814994812 Validation loss 0.26703184843063354\n",
            "Epoch 2620/30000  training loss: 0.27086904644966125 Validation loss 0.2669541835784912\n",
            "Epoch 2630/30000  training loss: 0.27079132199287415 Validation loss 0.2668763995170593\n",
            "Epoch 2640/30000  training loss: 0.2707134485244751 Validation loss 0.2667985260486603\n",
            "Epoch 2650/30000  training loss: 0.2706353962421417 Validation loss 0.26672041416168213\n",
            "Epoch 2660/30000  training loss: 0.2705572247505188 Validation loss 0.2666422724723816\n",
            "Epoch 2670/30000  training loss: 0.2704789340496063 Validation loss 0.26656395196914673\n",
            "Epoch 2680/30000  training loss: 0.2704005539417267 Validation loss 0.2664855718612671\n",
            "Epoch 2690/30000  training loss: 0.27032214403152466 Validation loss 0.2664071023464203\n",
            "Epoch 2700/30000  training loss: 0.2702435851097107 Validation loss 0.2663285434246063\n",
            "Epoch 2710/30000  training loss: 0.27016496658325195 Validation loss 0.2662498950958252\n",
            "Epoch 2720/30000  training loss: 0.27008625864982605 Validation loss 0.2661711871623993\n",
            "Epoch 2730/30000  training loss: 0.2700074017047882 Validation loss 0.26609233021736145\n",
            "Epoch 2740/30000  training loss: 0.2699284553527832 Validation loss 0.26601335406303406\n",
            "Epoch 2750/30000  training loss: 0.2698494493961334 Validation loss 0.2659343481063843\n",
            "Epoch 2760/30000  training loss: 0.26977038383483887 Validation loss 0.26585519313812256\n",
            "Epoch 2770/30000  training loss: 0.26969113945961 Validation loss 0.26577600836753845\n",
            "Epoch 2780/30000  training loss: 0.2696118652820587 Validation loss 0.2656967043876648\n",
            "Epoch 2790/30000  training loss: 0.2695324718952179 Validation loss 0.265617311000824\n",
            "Epoch 2800/30000  training loss: 0.2694529891014099 Validation loss 0.2655377984046936\n",
            "Epoch 2810/30000  training loss: 0.26937335729599 Validation loss 0.26545819640159607\n",
            "Epoch 2820/30000  training loss: 0.2692936062812805 Validation loss 0.2653784155845642\n",
            "Epoch 2830/30000  training loss: 0.2692137360572815 Validation loss 0.2652984857559204\n",
            "Epoch 2840/30000  training loss: 0.26913368701934814 Validation loss 0.26521849632263184\n",
            "Epoch 2850/30000  training loss: 0.2690536081790924 Validation loss 0.2651383876800537\n",
            "Epoch 2860/30000  training loss: 0.26897335052490234 Validation loss 0.26505813002586365\n",
            "Epoch 2870/30000  training loss: 0.2688930928707123 Validation loss 0.2649778425693512\n",
            "Epoch 2880/30000  training loss: 0.2688126862049103 Validation loss 0.2648974359035492\n",
            "Epoch 2890/30000  training loss: 0.2687321901321411 Validation loss 0.26481693983078003\n",
            "Epoch 2900/30000  training loss: 0.26865154504776 Validation loss 0.2647363245487213\n",
            "Epoch 2910/30000  training loss: 0.2685708999633789 Validation loss 0.2646556794643402\n",
            "Epoch 2920/30000  training loss: 0.26849013566970825 Validation loss 0.26457488536834717\n",
            "Epoch 2930/30000  training loss: 0.26840925216674805 Validation loss 0.26449400186538696\n",
            "Epoch 2940/30000  training loss: 0.2683282494544983 Validation loss 0.2644130289554596\n",
            "Epoch 2950/30000  training loss: 0.26824721693992615 Validation loss 0.26433196663856506\n",
            "Epoch 2960/30000  training loss: 0.26816603541374207 Validation loss 0.264250785112381\n",
            "Epoch 2970/30000  training loss: 0.2680847942829132 Validation loss 0.2641695439815521\n",
            "Epoch 2980/30000  training loss: 0.2680034041404724 Validation loss 0.2640881836414337\n",
            "Epoch 2990/30000  training loss: 0.26792192459106445 Validation loss 0.26400670409202576\n",
            "Epoch 3000/30000  training loss: 0.2678402066230774 Validation loss 0.2639250159263611\n",
            "Epoch 3010/30000  training loss: 0.2677583694458008 Validation loss 0.2638431787490845\n",
            "Epoch 3020/30000  training loss: 0.2676765024662018 Validation loss 0.2637613117694855\n",
            "Epoch 3030/30000  training loss: 0.26759448647499084 Validation loss 0.2636793255805969\n",
            "Epoch 3040/30000  training loss: 0.26751238107681274 Validation loss 0.2635972201824188\n",
            "Epoch 3050/30000  training loss: 0.2674301862716675 Validation loss 0.26351505517959595\n",
            "Epoch 3060/30000  training loss: 0.26734790205955505 Validation loss 0.2634328007698059\n",
            "Epoch 3070/30000  training loss: 0.26726555824279785 Validation loss 0.26335039734840393\n",
            "Epoch 3080/30000  training loss: 0.2671830654144287 Validation loss 0.26326796412467957\n",
            "Epoch 3090/30000  training loss: 0.2671004831790924 Validation loss 0.26318538188934326\n",
            "Epoch 3100/30000  training loss: 0.26701778173446655 Validation loss 0.2631027102470398\n",
            "Epoch 3110/30000  training loss: 0.26693499088287354 Validation loss 0.26301997900009155\n",
            "Epoch 3120/30000  training loss: 0.26685214042663574 Validation loss 0.26293712854385376\n",
            "Epoch 3130/30000  training loss: 0.2667692005634308 Validation loss 0.2628541588783264\n",
            "Epoch 3140/30000  training loss: 0.2666861116886139 Validation loss 0.2627711296081543\n",
            "Epoch 3150/30000  training loss: 0.26660293340682983 Validation loss 0.2626879811286926\n",
            "Epoch 3160/30000  training loss: 0.2665196359157562 Validation loss 0.2626047134399414\n",
            "Epoch 3170/30000  training loss: 0.2664361000061035 Validation loss 0.2625211775302887\n",
            "Epoch 3180/30000  training loss: 0.26635247468948364 Validation loss 0.2624376118183136\n",
            "Epoch 3190/30000  training loss: 0.266268789768219 Validation loss 0.26235389709472656\n",
            "Epoch 3200/30000  training loss: 0.2661849558353424 Validation loss 0.26227012276649475\n",
            "Epoch 3210/30000  training loss: 0.26610103249549866 Validation loss 0.2621862292289734\n",
            "Epoch 3220/30000  training loss: 0.26601701974868774 Validation loss 0.26210224628448486\n",
            "Epoch 3230/30000  training loss: 0.26593291759490967 Validation loss 0.2620181739330292\n",
            "Epoch 3240/30000  training loss: 0.26584869623184204 Validation loss 0.2619340121746063\n",
            "Epoch 3250/30000  training loss: 0.26576438546180725 Validation loss 0.2618497610092163\n",
            "Epoch 3260/30000  training loss: 0.2656799852848053 Validation loss 0.26176536083221436\n",
            "Epoch 3270/30000  training loss: 0.2655954957008362 Validation loss 0.26168087124824524\n",
            "Epoch 3280/30000  training loss: 0.2655108571052551 Validation loss 0.26159632205963135\n",
            "Epoch 3290/30000  training loss: 0.2654261887073517 Validation loss 0.2615116536617279\n",
            "Epoch 3300/30000  training loss: 0.2653414011001587 Validation loss 0.2614268958568573\n",
            "Epoch 3310/30000  training loss: 0.26525646448135376 Validation loss 0.26134204864501953\n",
            "Epoch 3320/30000  training loss: 0.26517149806022644 Validation loss 0.2612570822238922\n",
            "Epoch 3330/30000  training loss: 0.26508626341819763 Validation loss 0.2611719071865082\n",
            "Epoch 3340/30000  training loss: 0.26500093936920166 Validation loss 0.2610866129398346\n",
            "Epoch 3350/30000  training loss: 0.26491546630859375 Validation loss 0.26100119948387146\n",
            "Epoch 3360/30000  training loss: 0.2648299038410187 Validation loss 0.2609156668186188\n",
            "Epoch 3370/30000  training loss: 0.26474425196647644 Validation loss 0.2608300745487213\n",
            "Epoch 3380/30000  training loss: 0.26465848088264465 Validation loss 0.2607443630695343\n",
            "Epoch 3390/30000  training loss: 0.2645725905895233 Validation loss 0.2606585621833801\n",
            "Epoch 3400/30000  training loss: 0.2644866704940796 Validation loss 0.2605726718902588\n",
            "Epoch 3410/30000  training loss: 0.2644006013870239 Validation loss 0.2604866623878479\n",
            "Epoch 3420/30000  training loss: 0.2643144428730011 Validation loss 0.26040053367614746\n",
            "Epoch 3430/30000  training loss: 0.2642281949520111 Validation loss 0.26031437516212463\n",
            "Epoch 3440/30000  training loss: 0.26414182782173157 Validation loss 0.2602280378341675\n",
            "Epoch 3450/30000  training loss: 0.2640553414821625 Validation loss 0.26014164090156555\n",
            "Epoch 3460/30000  training loss: 0.263968825340271 Validation loss 0.26005515456199646\n",
            "Epoch 3470/30000  training loss: 0.2638821303844452 Validation loss 0.25996851921081543\n",
            "Epoch 3480/30000  training loss: 0.263795405626297 Validation loss 0.259881854057312\n",
            "Epoch 3490/30000  training loss: 0.26370853185653687 Validation loss 0.25979503989219666\n",
            "Epoch 3500/30000  training loss: 0.26362144947052 Validation loss 0.2597079575061798\n",
            "Epoch 3510/30000  training loss: 0.26353418827056885 Validation loss 0.2596208155155182\n",
            "Epoch 3520/30000  training loss: 0.2634468376636505 Validation loss 0.25953352451324463\n",
            "Epoch 3530/30000  training loss: 0.2633594274520874 Validation loss 0.2594462037086487\n",
            "Epoch 3540/30000  training loss: 0.26327189803123474 Validation loss 0.2593587040901184\n",
            "Epoch 3550/30000  training loss: 0.26318424940109253 Validation loss 0.25927114486694336\n",
            "Epoch 3560/30000  training loss: 0.26309651136398315 Validation loss 0.25918346643447876\n",
            "Epoch 3570/30000  training loss: 0.2630086839199066 Validation loss 0.2590957581996918\n",
            "Epoch 3580/30000  training loss: 0.2629207670688629 Validation loss 0.25900790095329285\n",
            "Epoch 3590/30000  training loss: 0.26283273100852966 Validation loss 0.25891992449760437\n",
            "Epoch 3600/30000  training loss: 0.2627445459365845 Validation loss 0.25883182883262634\n",
            "Epoch 3610/30000  training loss: 0.2626563608646393 Validation loss 0.2587437033653259\n",
            "Epoch 3620/30000  training loss: 0.26256799697875977 Validation loss 0.2586554288864136\n",
            "Epoch 3630/30000  training loss: 0.2624795734882355 Validation loss 0.25856706500053406\n",
            "Epoch 3640/30000  training loss: 0.26239103078842163 Validation loss 0.2584786117076874\n",
            "Epoch 3650/30000  training loss: 0.26230236887931824 Validation loss 0.25839003920555115\n",
            "Epoch 3660/30000  training loss: 0.26221349835395813 Validation loss 0.2583012580871582\n",
            "Epoch 3670/30000  training loss: 0.2621244788169861 Validation loss 0.25821229815483093\n",
            "Epoch 3680/30000  training loss: 0.2620353698730469 Validation loss 0.2581232786178589\n",
            "Epoch 3690/30000  training loss: 0.2619461417198181 Validation loss 0.2580341398715973\n",
            "Epoch 3700/30000  training loss: 0.2618567943572998 Validation loss 0.25794491171836853\n",
            "Epoch 3710/30000  training loss: 0.2617674171924591 Validation loss 0.2578555643558502\n",
            "Epoch 3720/30000  training loss: 0.2616778612136841 Validation loss 0.25776615738868713\n",
            "Epoch 3730/30000  training loss: 0.2615882456302643 Validation loss 0.2576766014099121\n",
            "Epoch 3740/30000  training loss: 0.26149848103523254 Validation loss 0.2575869858264923\n",
            "Epoch 3750/30000  training loss: 0.2614086866378784 Validation loss 0.25749725103378296\n",
            "Epoch 3760/30000  training loss: 0.26131871342658997 Validation loss 0.25740739703178406\n",
            "Epoch 3770/30000  training loss: 0.26122865080833435 Validation loss 0.257317453622818\n",
            "Epoch 3780/30000  training loss: 0.26113858819007874 Validation loss 0.25722742080688477\n",
            "Epoch 3790/30000  training loss: 0.2610483169555664 Validation loss 0.257137268781662\n",
            "Epoch 3800/30000  training loss: 0.2609579563140869 Validation loss 0.25704702734947205\n",
            "Epoch 3810/30000  training loss: 0.26086750626564026 Validation loss 0.25695666670799255\n",
            "Epoch 3820/30000  training loss: 0.2607768476009369 Validation loss 0.25686609745025635\n",
            "Epoch 3830/30000  training loss: 0.2606859803199768 Validation loss 0.2567753791809082\n",
            "Epoch 3840/30000  training loss: 0.2605951130390167 Validation loss 0.2566845715045929\n",
            "Epoch 3850/30000  training loss: 0.2605040371417999 Validation loss 0.25659364461898804\n",
            "Epoch 3860/30000  training loss: 0.26041293144226074 Validation loss 0.25650259852409363\n",
            "Epoch 3870/30000  training loss: 0.2603216767311096 Validation loss 0.25641149282455444\n",
            "Epoch 3880/30000  training loss: 0.26023033261299133 Validation loss 0.2563202381134033\n",
            "Epoch 3890/30000  training loss: 0.2601388990879059 Validation loss 0.2562289535999298\n",
            "Epoch 3900/30000  training loss: 0.2600473165512085 Validation loss 0.25613752007484436\n",
            "Epoch 3910/30000  training loss: 0.25995567440986633 Validation loss 0.256045937538147\n",
            "Epoch 3920/30000  training loss: 0.259863942861557 Validation loss 0.2559543251991272\n",
            "Epoch 3930/30000  training loss: 0.25977206230163574 Validation loss 0.25586259365081787\n",
            "Epoch 3940/30000  training loss: 0.2596801221370697 Validation loss 0.255770742893219\n",
            "Epoch 3950/30000  training loss: 0.2595880329608917 Validation loss 0.25567880272865295\n",
            "Epoch 3960/30000  training loss: 0.25949588418006897 Validation loss 0.25558677315711975\n",
            "Epoch 3970/30000  training loss: 0.25940361618995667 Validation loss 0.2554945945739746\n",
            "Epoch 3980/30000  training loss: 0.2593110501766205 Validation loss 0.25540220737457275\n",
            "Epoch 3990/30000  training loss: 0.2592184245586395 Validation loss 0.25530967116355896\n",
            "Epoch 4000/30000  training loss: 0.259125679731369 Validation loss 0.255217045545578\n",
            "Epoch 4010/30000  training loss: 0.2590327858924866 Validation loss 0.2551243007183075\n",
            "Epoch 4020/30000  training loss: 0.25893983244895935 Validation loss 0.2550314962863922\n",
            "Epoch 4030/30000  training loss: 0.2588467597961426 Validation loss 0.2549385130405426\n",
            "Epoch 4040/30000  training loss: 0.25875356793403625 Validation loss 0.2548454999923706\n",
            "Epoch 4050/30000  training loss: 0.25866031646728516 Validation loss 0.25475236773490906\n",
            "Epoch 4060/30000  training loss: 0.2585669159889221 Validation loss 0.25465911626815796\n",
            "Epoch 4070/30000  training loss: 0.2584734559059143 Validation loss 0.2545657753944397\n",
            "Epoch 4080/30000  training loss: 0.25837981700897217 Validation loss 0.2544723153114319\n",
            "Epoch 4090/30000  training loss: 0.25828611850738525 Validation loss 0.2543787658214569\n",
            "Epoch 4100/30000  training loss: 0.2581923305988312 Validation loss 0.2542850971221924\n",
            "Epoch 4110/30000  training loss: 0.25809842348098755 Validation loss 0.2541913390159607\n",
            "Epoch 4120/30000  training loss: 0.25800439715385437 Validation loss 0.25409749150276184\n",
            "Epoch 4130/30000  training loss: 0.25791025161743164 Validation loss 0.2540034353733063\n",
            "Epoch 4140/30000  training loss: 0.2578158676624298 Validation loss 0.25390923023223877\n",
            "Epoch 4150/30000  training loss: 0.25772133469581604 Validation loss 0.2538148760795593\n",
            "Epoch 4160/30000  training loss: 0.2576267719268799 Validation loss 0.2537204325199127\n",
            "Epoch 4170/30000  training loss: 0.2575320303440094 Validation loss 0.25362586975097656\n",
            "Epoch 4180/30000  training loss: 0.25743719935417175 Validation loss 0.25353121757507324\n",
            "Epoch 4190/30000  training loss: 0.25734227895736694 Validation loss 0.253436416387558\n",
            "Epoch 4200/30000  training loss: 0.25724726915359497 Validation loss 0.25334158539772034\n",
            "Epoch 4210/30000  training loss: 0.25715214014053345 Validation loss 0.25324660539627075\n",
            "Epoch 4220/30000  training loss: 0.25705686211586 Validation loss 0.253151535987854\n",
            "Epoch 4230/30000  training loss: 0.25696155428886414 Validation loss 0.2530563771724701\n",
            "Epoch 4240/30000  training loss: 0.25686612725257874 Validation loss 0.25296109914779663\n",
            "Epoch 4250/30000  training loss: 0.2567705512046814 Validation loss 0.25286567211151123\n",
            "Epoch 4260/30000  training loss: 0.2566749155521393 Validation loss 0.25277021527290344\n",
            "Epoch 4270/30000  training loss: 0.25657913088798523 Validation loss 0.2526746392250061\n",
            "Epoch 4280/30000  training loss: 0.256483256816864 Validation loss 0.2525789141654968\n",
            "Epoch 4290/30000  training loss: 0.2563871443271637 Validation loss 0.25248298048973083\n",
            "Epoch 4300/30000  training loss: 0.25629091262817383 Validation loss 0.2523868680000305\n",
            "Epoch 4310/30000  training loss: 0.256194531917572 Validation loss 0.2522907257080078\n",
            "Epoch 4320/30000  training loss: 0.25609806180000305 Validation loss 0.2521944046020508\n",
            "Epoch 4330/30000  training loss: 0.2560015022754669 Validation loss 0.252098023891449\n",
            "Epoch 4340/30000  training loss: 0.25590482354164124 Validation loss 0.2520015239715576\n",
            "Epoch 4350/30000  training loss: 0.255808025598526 Validation loss 0.2519049346446991\n",
            "Epoch 4360/30000  training loss: 0.2557111382484436 Validation loss 0.251808226108551\n",
            "Epoch 4370/30000  training loss: 0.25561416149139404 Validation loss 0.2517113983631134\n",
            "Epoch 4380/30000  training loss: 0.25551700592041016 Validation loss 0.251614511013031\n",
            "Epoch 4390/30000  training loss: 0.2554197907447815 Validation loss 0.25151747465133667\n",
            "Epoch 4400/30000  training loss: 0.25532248616218567 Validation loss 0.25142034888267517\n",
            "Epoch 4410/30000  training loss: 0.2552250921726227 Validation loss 0.2513231337070465\n",
            "Epoch 4420/30000  training loss: 0.25512751936912537 Validation loss 0.2512258291244507\n",
            "Epoch 4430/30000  training loss: 0.2550298869609833 Validation loss 0.2511283755302429\n",
            "Epoch 4440/30000  training loss: 0.2549320459365845 Validation loss 0.25103071331977844\n",
            "Epoch 4450/30000  training loss: 0.25483399629592896 Validation loss 0.250932902097702\n",
            "Epoch 4460/30000  training loss: 0.25473591685295105 Validation loss 0.25083497166633606\n",
            "Epoch 4470/30000  training loss: 0.2546376585960388 Validation loss 0.25073692202568054\n",
            "Epoch 4480/30000  training loss: 0.25453928112983704 Validation loss 0.25063878297805786\n",
            "Epoch 4490/30000  training loss: 0.2544408142566681 Validation loss 0.250540554523468\n",
            "Epoch 4500/30000  training loss: 0.25434228777885437 Validation loss 0.2504422068595886\n",
            "Epoch 4510/30000  training loss: 0.2542436122894287 Validation loss 0.25034376978874207\n",
            "Epoch 4520/30000  training loss: 0.2541448473930359 Validation loss 0.25024518370628357\n",
            "Epoch 4530/30000  training loss: 0.2540459632873535 Validation loss 0.2501465380191803\n",
            "Epoch 4540/30000  training loss: 0.253946989774704 Validation loss 0.2500477731227875\n",
            "Epoch 4550/30000  training loss: 0.2538478970527649 Validation loss 0.2499489039182663\n",
            "Epoch 4560/30000  training loss: 0.25374871492385864 Validation loss 0.24984991550445557\n",
            "Epoch 4570/30000  training loss: 0.25364938378334045 Validation loss 0.24975082278251648\n",
            "Epoch 4580/30000  training loss: 0.2535499632358551 Validation loss 0.24965165555477142\n",
            "Epoch 4590/30000  training loss: 0.2534503638744354 Validation loss 0.24955224990844727\n",
            "Epoch 4600/30000  training loss: 0.25335055589675903 Validation loss 0.24945269525051117\n",
            "Epoch 4610/30000  training loss: 0.2532506585121155 Validation loss 0.24935300648212433\n",
            "Epoch 4620/30000  training loss: 0.25315061211586 Validation loss 0.24925321340560913\n",
            "Epoch 4630/30000  training loss: 0.25305047631263733 Validation loss 0.24915331602096558\n",
            "Epoch 4640/30000  training loss: 0.2529502511024475 Validation loss 0.24905332922935486\n",
            "Epoch 4650/30000  training loss: 0.25284990668296814 Validation loss 0.2489532232284546\n",
            "Epoch 4660/30000  training loss: 0.2527494728565216 Validation loss 0.24885302782058716\n",
            "Epoch 4670/30000  training loss: 0.2526489198207855 Validation loss 0.2487526834011078\n",
            "Epoch 4680/30000  training loss: 0.2525482475757599 Validation loss 0.24865229427814484\n",
            "Epoch 4690/30000  training loss: 0.2524474859237671 Validation loss 0.24855175614356995\n",
            "Epoch 4700/30000  training loss: 0.25234663486480713 Validation loss 0.2484511286020279\n",
            "Epoch 4710/30000  training loss: 0.25224563479423523 Validation loss 0.24835041165351868\n",
            "Epoch 4720/30000  training loss: 0.2521445155143738 Validation loss 0.2482495754957199\n",
            "Epoch 4730/30000  training loss: 0.25204336643218994 Validation loss 0.24814863502979279\n",
            "Epoch 4740/30000  training loss: 0.2519419193267822 Validation loss 0.24804747104644775\n",
            "Epoch 4750/30000  training loss: 0.25184035301208496 Validation loss 0.2479461282491684\n",
            "Epoch 4760/30000  training loss: 0.25173863768577576 Validation loss 0.24784469604492188\n",
            "Epoch 4770/30000  training loss: 0.2516368329524994 Validation loss 0.24774311482906342\n",
            "Epoch 4780/30000  training loss: 0.25153490900993347 Validation loss 0.24764150381088257\n",
            "Epoch 4790/30000  training loss: 0.251432865858078 Validation loss 0.2475397139787674\n",
            "Epoch 4800/30000  training loss: 0.25133076310157776 Validation loss 0.24743786454200745\n",
            "Epoch 4810/30000  training loss: 0.25122854113578796 Validation loss 0.24733589589595795\n",
            "Epoch 4820/30000  training loss: 0.25112617015838623 Validation loss 0.24723383784294128\n",
            "Epoch 4830/30000  training loss: 0.25102370977401733 Validation loss 0.24713164567947388\n",
            "Epoch 4840/30000  training loss: 0.2509211301803589 Validation loss 0.2470293790102005\n",
            "Epoch 4850/30000  training loss: 0.2508184611797333 Validation loss 0.24692697823047638\n",
            "Epoch 4860/30000  training loss: 0.2507157027721405 Validation loss 0.2468245029449463\n",
            "Epoch 4870/30000  training loss: 0.2506128251552582 Validation loss 0.24672187864780426\n",
            "Epoch 4880/30000  training loss: 0.2505098283290863 Validation loss 0.24661920964717865\n",
            "Epoch 4890/30000  training loss: 0.25040656328201294 Validation loss 0.24651619791984558\n",
            "Epoch 4900/30000  training loss: 0.25030317902565 Validation loss 0.24641309678554535\n",
            "Epoch 4910/30000  training loss: 0.25019970536231995 Validation loss 0.24630989134311676\n",
            "Epoch 4920/30000  training loss: 0.2500961124897003 Validation loss 0.24620655179023743\n",
            "Epoch 4930/30000  training loss: 0.24999237060546875 Validation loss 0.24610313773155212\n",
            "Epoch 4940/30000  training loss: 0.2498885691165924 Validation loss 0.24599964916706085\n",
            "Epoch 4950/30000  training loss: 0.24978461861610413 Validation loss 0.24589602649211884\n",
            "Epoch 4960/30000  training loss: 0.24968057870864868 Validation loss 0.24579228460788727\n",
            "Epoch 4970/30000  training loss: 0.24957643449306488 Validation loss 0.24568843841552734\n",
            "Epoch 4980/30000  training loss: 0.24947218596935272 Validation loss 0.24558450281620026\n",
            "Epoch 4990/30000  training loss: 0.2493678331375122 Validation loss 0.24548043310642242\n",
            "Epoch 5000/30000  training loss: 0.24926334619522095 Validation loss 0.24537627398967743\n",
            "Epoch 5010/30000  training loss: 0.24915875494480133 Validation loss 0.24527202546596527\n",
            "Epoch 5020/30000  training loss: 0.24905411899089813 Validation loss 0.24516764283180237\n",
            "Epoch 5030/30000  training loss: 0.24894927442073822 Validation loss 0.24506314098834991\n",
            "Epoch 5040/30000  training loss: 0.2488442063331604 Validation loss 0.24495840072631836\n",
            "Epoch 5050/30000  training loss: 0.24873901903629303 Validation loss 0.24485349655151367\n",
            "Epoch 5060/30000  training loss: 0.2486337274312973 Validation loss 0.2447485327720642\n",
            "Epoch 5070/30000  training loss: 0.24852833151817322 Validation loss 0.2446434497833252\n",
            "Epoch 5080/30000  training loss: 0.2484228014945984 Validation loss 0.24453826248645782\n",
            "Epoch 5090/30000  training loss: 0.2483171671628952 Validation loss 0.2444329857826233\n",
            "Epoch 5100/30000  training loss: 0.24821144342422485 Validation loss 0.244327574968338\n",
            "Epoch 5110/30000  training loss: 0.24810561537742615 Validation loss 0.24422208964824677\n",
            "Epoch 5120/30000  training loss: 0.2479996681213379 Validation loss 0.24411647021770477\n",
            "Epoch 5130/30000  training loss: 0.24789360165596008 Validation loss 0.24401074647903442\n",
            "Epoch 5140/30000  training loss: 0.2477874606847763 Validation loss 0.2439049482345581\n",
            "Epoch 5150/30000  training loss: 0.2476811707019806 Validation loss 0.24379901587963104\n",
            "Epoch 5160/30000  training loss: 0.2475748062133789 Validation loss 0.24369297921657562\n",
            "Epoch 5170/30000  training loss: 0.24746833741664886 Validation loss 0.24358685314655304\n",
            "Epoch 5180/30000  training loss: 0.24736164510250092 Validation loss 0.24348048865795135\n",
            "Epoch 5190/30000  training loss: 0.24725472927093506 Validation loss 0.24337394535541534\n",
            "Epoch 5200/30000  training loss: 0.24714772403240204 Validation loss 0.24326731264591217\n",
            "Epoch 5210/30000  training loss: 0.24704064428806305 Validation loss 0.24316057562828064\n",
            "Epoch 5220/30000  training loss: 0.2469334453344345 Validation loss 0.24305370450019836\n",
            "Epoch 5230/30000  training loss: 0.24682611227035522 Validation loss 0.24294674396514893\n",
            "Epoch 5240/30000  training loss: 0.24671868979930878 Validation loss 0.24283970892429352\n",
            "Epoch 5250/30000  training loss: 0.24661116302013397 Validation loss 0.24273252487182617\n",
            "Epoch 5260/30000  training loss: 0.24650351703166962 Validation loss 0.24262526631355286\n",
            "Epoch 5270/30000  training loss: 0.2463957816362381 Validation loss 0.24251788854599\n",
            "Epoch 5280/30000  training loss: 0.24628795683383942 Validation loss 0.24241042137145996\n",
            "Epoch 5290/30000  training loss: 0.2461799681186676 Validation loss 0.24230283498764038\n",
            "Epoch 5300/30000  training loss: 0.24607188999652863 Validation loss 0.24219515919685364\n",
            "Epoch 5310/30000  training loss: 0.24596372246742249 Validation loss 0.24208734929561615\n",
            "Epoch 5320/30000  training loss: 0.2458554357290268 Validation loss 0.24197940528392792\n",
            "Epoch 5330/30000  training loss: 0.24574680626392365 Validation loss 0.2418712079524994\n",
            "Epoch 5340/30000  training loss: 0.2456381618976593 Validation loss 0.2417629063129425\n",
            "Epoch 5350/30000  training loss: 0.24552935361862183 Validation loss 0.24165450036525726\n",
            "Epoch 5360/30000  training loss: 0.2454204261302948 Validation loss 0.24154599010944366\n",
            "Epoch 5370/30000  training loss: 0.2453114092350006 Validation loss 0.2414373755455017\n",
            "Epoch 5380/30000  training loss: 0.24520231783390045 Validation loss 0.2413286566734314\n",
            "Epoch 5390/30000  training loss: 0.24509309232234955 Validation loss 0.24121981859207153\n",
            "Epoch 5400/30000  training loss: 0.2449837327003479 Validation loss 0.2411108911037445\n",
            "Epoch 5410/30000  training loss: 0.24487432837486267 Validation loss 0.24100185930728912\n",
            "Epoch 5420/30000  training loss: 0.2447647750377655 Validation loss 0.24089273810386658\n",
            "Epoch 5430/30000  training loss: 0.24465513229370117 Validation loss 0.24078349769115448\n",
            "Epoch 5440/30000  training loss: 0.24454538524150848 Validation loss 0.24067412316799164\n",
            "Epoch 5450/30000  training loss: 0.24443548917770386 Validation loss 0.2405647337436676\n",
            "Epoch 5460/30000  training loss: 0.24432553350925446 Validation loss 0.24045512080192566\n",
            "Epoch 5470/30000  training loss: 0.24421529471874237 Validation loss 0.2403453141450882\n",
            "Epoch 5480/30000  training loss: 0.24410489201545715 Validation loss 0.2402353286743164\n",
            "Epoch 5490/30000  training loss: 0.24399441480636597 Validation loss 0.24012525379657745\n",
            "Epoch 5500/30000  training loss: 0.24388383328914642 Validation loss 0.24001511931419373\n",
            "Epoch 5510/30000  training loss: 0.24377316236495972 Validation loss 0.23990482091903687\n",
            "Epoch 5520/30000  training loss: 0.24366231262683868 Validation loss 0.23979446291923523\n",
            "Epoch 5530/30000  training loss: 0.24355141818523407 Validation loss 0.23968398571014404\n",
            "Epoch 5540/30000  training loss: 0.2434404045343399 Validation loss 0.2395733892917633\n",
            "Epoch 5550/30000  training loss: 0.243329256772995 Validation loss 0.2394627034664154\n",
            "Epoch 5560/30000  training loss: 0.24321801960468292 Validation loss 0.23935192823410034\n",
            "Epoch 5570/30000  training loss: 0.24310672283172607 Validation loss 0.23924103379249573\n",
            "Epoch 5580/30000  training loss: 0.2429952472448349 Validation loss 0.23913000524044037\n",
            "Epoch 5590/30000  training loss: 0.24288368225097656 Validation loss 0.23901893198490143\n",
            "Epoch 5600/30000  training loss: 0.24277205765247345 Validation loss 0.23890772461891174\n",
            "Epoch 5610/30000  training loss: 0.24266016483306885 Validation loss 0.23879626393318176\n",
            "Epoch 5620/30000  training loss: 0.24254804849624634 Validation loss 0.23868465423583984\n",
            "Epoch 5630/30000  training loss: 0.24243587255477905 Validation loss 0.23857294023036957\n",
            "Epoch 5640/30000  training loss: 0.2423236221075058 Validation loss 0.23846113681793213\n",
            "Epoch 5650/30000  training loss: 0.2422112375497818 Validation loss 0.23834918439388275\n",
            "Epoch 5660/30000  training loss: 0.24209873378276825 Validation loss 0.2382371723651886\n",
            "Epoch 5670/30000  training loss: 0.24198612570762634 Validation loss 0.2381250411272049\n",
            "Epoch 5680/30000  training loss: 0.24187345802783966 Validation loss 0.23801282048225403\n",
            "Epoch 5690/30000  training loss: 0.24176064133644104 Validation loss 0.2379004806280136\n",
            "Epoch 5700/30000  training loss: 0.24164773523807526 Validation loss 0.23778803646564484\n",
            "Epoch 5710/30000  training loss: 0.24153469502925873 Validation loss 0.2376755028963089\n",
            "Epoch 5720/30000  training loss: 0.24142156541347504 Validation loss 0.2375628799200058\n",
            "Epoch 5730/30000  training loss: 0.241308331489563 Validation loss 0.23745012283325195\n",
            "Epoch 5740/30000  training loss: 0.24119499325752258 Validation loss 0.23733727633953094\n",
            "Epoch 5750/30000  training loss: 0.24108146131038666 Validation loss 0.23722423613071442\n",
            "Epoch 5760/30000  training loss: 0.2409677356481552 Validation loss 0.23711100220680237\n",
            "Epoch 5770/30000  training loss: 0.24085387587547302 Validation loss 0.23699763417243958\n",
            "Epoch 5780/30000  training loss: 0.24073992669582367 Validation loss 0.23688417673110962\n",
            "Epoch 5790/30000  training loss: 0.24062584340572357 Validation loss 0.23677058517932892\n",
            "Epoch 5800/30000  training loss: 0.2405116707086563 Validation loss 0.23665696382522583\n",
            "Epoch 5810/30000  training loss: 0.24039742350578308 Validation loss 0.23654316365718842\n",
            "Epoch 5820/30000  training loss: 0.24028301239013672 Validation loss 0.23642930388450623\n",
            "Epoch 5830/30000  training loss: 0.2401685267686844 Validation loss 0.23631535470485687\n",
            "Epoch 5840/30000  training loss: 0.2400539666414261 Validation loss 0.23620130121707916\n",
            "Epoch 5850/30000  training loss: 0.23993924260139465 Validation loss 0.23608708381652832\n",
            "Epoch 5860/30000  training loss: 0.23982444405555725 Validation loss 0.2359728068113327\n",
            "Epoch 5870/30000  training loss: 0.23970955610275269 Validation loss 0.23585844039916992\n",
            "Epoch 5880/30000  training loss: 0.23959454894065857 Validation loss 0.2357439547777176\n",
            "Epoch 5890/30000  training loss: 0.23947931826114655 Validation loss 0.23562927544116974\n",
            "Epoch 5900/30000  training loss: 0.23936393857002258 Validation loss 0.23551438748836517\n",
            "Epoch 5910/30000  training loss: 0.23924845457077026 Validation loss 0.23539942502975464\n",
            "Epoch 5920/30000  training loss: 0.2391328066587448 Validation loss 0.23528432846069336\n",
            "Epoch 5930/30000  training loss: 0.239017054438591 Validation loss 0.23516912758350372\n",
            "Epoch 5940/30000  training loss: 0.23890124261379242 Validation loss 0.23505383729934692\n",
            "Epoch 5950/30000  training loss: 0.23878532648086548 Validation loss 0.23493845760822296\n",
            "Epoch 5960/30000  training loss: 0.2386692613363266 Validation loss 0.23482295870780945\n",
            "Epoch 5970/30000  training loss: 0.23855307698249817 Validation loss 0.23470735549926758\n",
            "Epoch 5980/30000  training loss: 0.23843678832054138 Validation loss 0.23459167778491974\n",
            "Epoch 5990/30000  training loss: 0.2383204847574234 Validation loss 0.23447588086128235\n",
            "Epoch 6000/30000  training loss: 0.23820404708385468 Validation loss 0.2343599647283554\n",
            "Epoch 6010/30000  training loss: 0.2380874752998352 Validation loss 0.2342439740896225\n",
            "Epoch 6020/30000  training loss: 0.23797078430652618 Validation loss 0.23412789404392242\n",
            "Epoch 6030/30000  training loss: 0.23785395920276642 Validation loss 0.23401159048080444\n",
            "Epoch 6040/30000  training loss: 0.23773689568042755 Validation loss 0.23389507830142975\n",
            "Epoch 6050/30000  training loss: 0.23761968314647675 Validation loss 0.2337784767150879\n",
            "Epoch 6060/30000  training loss: 0.23750239610671997 Validation loss 0.23366177082061768\n",
            "Epoch 6070/30000  training loss: 0.23738503456115723 Validation loss 0.2335449755191803\n",
            "Epoch 6080/30000  training loss: 0.23726750910282135 Validation loss 0.23342806100845337\n",
            "Epoch 6090/30000  training loss: 0.2371499240398407 Validation loss 0.23331105709075928\n",
            "Epoch 6100/30000  training loss: 0.2370322346687317 Validation loss 0.23319396376609802\n",
            "Epoch 6110/30000  training loss: 0.2369144707918167 Validation loss 0.2330767810344696\n",
            "Epoch 6120/30000  training loss: 0.2367965281009674 Validation loss 0.23295947909355164\n",
            "Epoch 6130/30000  training loss: 0.23667854070663452 Validation loss 0.23284204304218292\n",
            "Epoch 6140/30000  training loss: 0.2365604192018509 Validation loss 0.23272456228733063\n",
            "Epoch 6150/30000  training loss: 0.23644226789474487 Validation loss 0.23260696232318878\n",
            "Epoch 6160/30000  training loss: 0.23632389307022095 Validation loss 0.23248927295207977\n",
            "Epoch 6170/30000  training loss: 0.23620541393756866 Validation loss 0.23237137496471405\n",
            "Epoch 6180/30000  training loss: 0.23608675599098206 Validation loss 0.23225334286689758\n",
            "Epoch 6190/30000  training loss: 0.23596805334091187 Validation loss 0.23213523626327515\n",
            "Epoch 6200/30000  training loss: 0.23584924638271332 Validation loss 0.23201701045036316\n",
            "Epoch 6210/30000  training loss: 0.23573017120361328 Validation loss 0.23189866542816162\n",
            "Epoch 6220/30000  training loss: 0.23561108112335205 Validation loss 0.23178014159202576\n",
            "Epoch 6230/30000  training loss: 0.23549185693264008 Validation loss 0.2316615730524063\n",
            "Epoch 6240/30000  training loss: 0.23537252843379974 Validation loss 0.23154284060001373\n",
            "Epoch 6250/30000  training loss: 0.23525312542915344 Validation loss 0.23142409324645996\n",
            "Epoch 6260/30000  training loss: 0.2351335734128952 Validation loss 0.23130519688129425\n",
            "Epoch 6270/30000  training loss: 0.2350139319896698 Validation loss 0.2311861664056778\n",
            "Epoch 6280/30000  training loss: 0.23489408195018768 Validation loss 0.2310670018196106\n",
            "Epoch 6290/30000  training loss: 0.2347741723060608 Validation loss 0.23094771802425385\n",
            "Epoch 6300/30000  training loss: 0.23465412855148315 Validation loss 0.23082834482192993\n",
            "Epoch 6310/30000  training loss: 0.23453396558761597 Validation loss 0.23070885241031647\n",
            "Epoch 6320/30000  training loss: 0.234413743019104 Validation loss 0.23058925569057465\n",
            "Epoch 6330/30000  training loss: 0.23429344594478607 Validation loss 0.23046964406967163\n",
            "Epoch 6340/30000  training loss: 0.2341729700565338 Validation loss 0.2303498387336731\n",
            "Epoch 6350/30000  training loss: 0.23405231535434723 Validation loss 0.23022983968257904\n",
            "Epoch 6360/30000  training loss: 0.23393157124519348 Validation loss 0.2301097959280014\n",
            "Epoch 6370/30000  training loss: 0.23381078243255615 Validation loss 0.2299896478652954\n",
            "Epoch 6380/30000  training loss: 0.2336898148059845 Validation loss 0.22986936569213867\n",
            "Epoch 6390/30000  training loss: 0.23356874287128448 Validation loss 0.22974897921085358\n",
            "Epoch 6400/30000  training loss: 0.23344764113426208 Validation loss 0.2296285480260849\n",
            "Epoch 6410/30000  training loss: 0.23332640528678894 Validation loss 0.22950799763202667\n",
            "Epoch 6420/30000  training loss: 0.23320496082305908 Validation loss 0.22938723862171173\n",
            "Epoch 6430/30000  training loss: 0.23308339715003967 Validation loss 0.22926639020442963\n",
            "Epoch 6440/30000  training loss: 0.2329617440700531 Validation loss 0.22914542257785797\n",
            "Epoch 6450/30000  training loss: 0.23284001648426056 Validation loss 0.22902441024780273\n",
            "Epoch 6460/30000  training loss: 0.23271813988685608 Validation loss 0.22890326380729675\n",
            "Epoch 6470/30000  training loss: 0.2325962632894516 Validation loss 0.2287820726633072\n",
            "Epoch 6480/30000  training loss: 0.2324742078781128 Validation loss 0.2286607176065445\n",
            "Epoch 6490/30000  training loss: 0.23235194385051727 Validation loss 0.22853921353816986\n",
            "Epoch 6500/30000  training loss: 0.23222962021827698 Validation loss 0.2284175604581833\n",
            "Epoch 6510/30000  training loss: 0.23210720717906952 Validation loss 0.22829587757587433\n",
            "Epoch 6520/30000  training loss: 0.2319846749305725 Validation loss 0.22817406058311462\n",
            "Epoch 6530/30000  training loss: 0.23186200857162476 Validation loss 0.22805215418338776\n",
            "Epoch 6540/30000  training loss: 0.23173928260803223 Validation loss 0.22793015837669373\n",
            "Epoch 6550/30000  training loss: 0.23161643743515015 Validation loss 0.22780802845954895\n",
            "Epoch 6560/30000  training loss: 0.23149344325065613 Validation loss 0.22768576443195343\n",
            "Epoch 6570/30000  training loss: 0.23137030005455017 Validation loss 0.22756339609622955\n",
            "Epoch 6580/30000  training loss: 0.23124708235263824 Validation loss 0.22744090855121613\n",
            "Epoch 6590/30000  training loss: 0.23112376034259796 Validation loss 0.22731836140155792\n",
            "Epoch 6600/30000  training loss: 0.23100033402442932 Validation loss 0.22719568014144897\n",
            "Epoch 6610/30000  training loss: 0.2308768332004547 Validation loss 0.22707293927669525\n",
            "Epoch 6620/30000  training loss: 0.23075321316719055 Validation loss 0.2269500344991684\n",
            "Epoch 6630/30000  training loss: 0.23062936961650848 Validation loss 0.2268269956111908\n",
            "Epoch 6640/30000  training loss: 0.23050549626350403 Validation loss 0.22670386731624603\n",
            "Epoch 6650/30000  training loss: 0.23038144409656525 Validation loss 0.2265806347131729\n",
            "Epoch 6660/30000  training loss: 0.23025737702846527 Validation loss 0.22645729780197144\n",
            "Epoch 6670/30000  training loss: 0.23013319075107574 Validation loss 0.22633390128612518\n",
            "Epoch 6680/30000  training loss: 0.23000892996788025 Validation loss 0.22621041536331177\n",
            "Epoch 6690/30000  training loss: 0.22988449037075043 Validation loss 0.22608673572540283\n",
            "Epoch 6700/30000  training loss: 0.22975988686084747 Validation loss 0.22596295177936554\n",
            "Epoch 6710/30000  training loss: 0.22963520884513855 Validation loss 0.22583909332752228\n",
            "Epoch 6720/30000  training loss: 0.22951047122478485 Validation loss 0.22571510076522827\n",
            "Epoch 6730/30000  training loss: 0.2293855994939804 Validation loss 0.2255910187959671\n",
            "Epoch 6740/30000  training loss: 0.22926059365272522 Validation loss 0.22546684741973877\n",
            "Epoch 6750/30000  training loss: 0.22913554310798645 Validation loss 0.22534261643886566\n",
            "Epoch 6760/30000  training loss: 0.22901038825511932 Validation loss 0.22521823644638062\n",
            "Epoch 6770/30000  training loss: 0.2288849800825119 Validation loss 0.22509370744228363\n",
            "Epoch 6780/30000  training loss: 0.22875960171222687 Validation loss 0.2249690443277359\n",
            "Epoch 6790/30000  training loss: 0.2286340445280075 Validation loss 0.22484435141086578\n",
            "Epoch 6800/30000  training loss: 0.22850839793682098 Validation loss 0.2247195541858673\n",
            "Epoch 6810/30000  training loss: 0.2283826470375061 Validation loss 0.22459465265274048\n",
            "Epoch 6820/30000  training loss: 0.22825683653354645 Validation loss 0.2244696319103241\n",
            "Epoch 6830/30000  training loss: 0.22813093662261963 Validation loss 0.22434452176094055\n",
            "Epoch 6840/30000  training loss: 0.22800478339195251 Validation loss 0.22421926259994507\n",
            "Epoch 6850/30000  training loss: 0.22787858545780182 Validation loss 0.22409386932849884\n",
            "Epoch 6860/30000  training loss: 0.22775228321552277 Validation loss 0.22396844625473022\n",
            "Epoch 6870/30000  training loss: 0.22762589156627655 Validation loss 0.22384291887283325\n",
            "Epoch 6880/30000  training loss: 0.22749939560890198 Validation loss 0.22371728718280792\n",
            "Epoch 6890/30000  training loss: 0.22737286984920502 Validation loss 0.22359158098697662\n",
            "Epoch 6900/30000  training loss: 0.22724615037441254 Validation loss 0.223465695977211\n",
            "Epoch 6910/30000  training loss: 0.2271192967891693 Validation loss 0.2233397364616394\n",
            "Epoch 6920/30000  training loss: 0.22699229419231415 Validation loss 0.22321362793445587\n",
            "Epoch 6930/30000  training loss: 0.2268652319908142 Validation loss 0.22308743000030518\n",
            "Epoch 6940/30000  training loss: 0.22673813998699188 Validation loss 0.2229612171649933\n",
            "Epoch 6950/30000  training loss: 0.22661092877388 Validation loss 0.22283487021923065\n",
            "Epoch 6960/30000  training loss: 0.22648362815380096 Validation loss 0.22270844876766205\n",
            "Epoch 6970/30000  training loss: 0.2263561338186264 Validation loss 0.22258181869983673\n",
            "Epoch 6980/30000  training loss: 0.2262285053730011 Validation loss 0.22245514392852783\n",
            "Epoch 6990/30000  training loss: 0.2261008471250534 Validation loss 0.2223283350467682\n",
            "Epoch 7000/30000  training loss: 0.22597305476665497 Validation loss 0.22220145165920258\n",
            "Epoch 7010/30000  training loss: 0.22584515810012817 Validation loss 0.2220744639635086\n",
            "Epoch 7020/30000  training loss: 0.22571726143360138 Validation loss 0.22194743156433105\n",
            "Epoch 7030/30000  training loss: 0.22558918595314026 Validation loss 0.22182029485702515\n",
            "Epoch 7040/30000  training loss: 0.2254609316587448 Validation loss 0.22169296443462372\n",
            "Epoch 7050/30000  training loss: 0.2253326028585434 Validation loss 0.22156555950641632\n",
            "Epoch 7060/30000  training loss: 0.2252042293548584 Validation loss 0.22143806517124176\n",
            "Epoch 7070/30000  training loss: 0.22507569193840027 Validation loss 0.22131049633026123\n",
            "Epoch 7080/30000  training loss: 0.22494710981845856 Validation loss 0.22118282318115234\n",
            "Epoch 7090/30000  training loss: 0.2248183786869049 Validation loss 0.2210550606250763\n",
            "Epoch 7100/30000  training loss: 0.22468963265419006 Validation loss 0.22092722356319427\n",
            "Epoch 7110/30000  training loss: 0.2245607227087021 Validation loss 0.22079923748970032\n",
            "Epoch 7120/30000  training loss: 0.2244316041469574 Validation loss 0.22067110240459442\n",
            "Epoch 7130/30000  training loss: 0.2243024706840515 Validation loss 0.22054289281368256\n",
            "Epoch 7140/30000  training loss: 0.22417327761650085 Validation loss 0.2204146534204483\n",
            "Epoch 7150/30000  training loss: 0.22404398024082184 Validation loss 0.2202862948179245\n",
            "Epoch 7160/30000  training loss: 0.2239145040512085 Validation loss 0.22015784680843353\n",
            "Epoch 7170/30000  training loss: 0.22378504276275635 Validation loss 0.2200292944908142\n",
            "Epoch 7180/30000  training loss: 0.22365540266036987 Validation loss 0.21990065276622772\n",
            "Epoch 7190/30000  training loss: 0.22352561354637146 Validation loss 0.2197718471288681\n",
            "Epoch 7200/30000  training loss: 0.22339574992656708 Validation loss 0.21964295208454132\n",
            "Epoch 7210/30000  training loss: 0.2232658565044403 Validation loss 0.21951399743556976\n",
            "Epoch 7220/30000  training loss: 0.2231358140707016 Validation loss 0.21938496828079224\n",
            "Epoch 7230/30000  training loss: 0.2230057269334793 Validation loss 0.21925581991672516\n",
            "Epoch 7240/30000  training loss: 0.22287549078464508 Validation loss 0.2191266417503357\n",
            "Epoch 7250/30000  training loss: 0.2227451205253601 Validation loss 0.2189972698688507\n",
            "Epoch 7260/30000  training loss: 0.22261467576026917 Validation loss 0.21886779367923737\n",
            "Epoch 7270/30000  training loss: 0.22248414158821106 Validation loss 0.21873824298381805\n",
            "Epoch 7280/30000  training loss: 0.22235345840454102 Validation loss 0.21860858798027039\n",
            "Epoch 7290/30000  training loss: 0.2222227305173874 Validation loss 0.21847888827323914\n",
            "Epoch 7300/30000  training loss: 0.22209195792675018 Validation loss 0.2183491289615631\n",
            "Epoch 7310/30000  training loss: 0.22196106612682343 Validation loss 0.21821926534175873\n",
            "Epoch 7320/30000  training loss: 0.22182999551296234 Validation loss 0.21808922290802002\n",
            "Epoch 7330/30000  training loss: 0.2216988503932953 Validation loss 0.21795907616615295\n",
            "Epoch 7340/30000  training loss: 0.22156758606433868 Validation loss 0.2178288698196411\n",
            "Epoch 7350/30000  training loss: 0.2214362770318985 Validation loss 0.2176985740661621\n",
            "Epoch 7360/30000  training loss: 0.22130484879016876 Validation loss 0.21756820380687714\n",
            "Epoch 7370/30000  training loss: 0.22117334604263306 Validation loss 0.2174377590417862\n",
            "Epoch 7380/30000  training loss: 0.22104184329509735 Validation loss 0.21730726957321167\n",
            "Epoch 7390/30000  training loss: 0.22091011703014374 Validation loss 0.21717658638954163\n",
            "Epoch 7400/30000  training loss: 0.22077824175357819 Validation loss 0.21704581379890442\n",
            "Epoch 7410/30000  training loss: 0.22064632177352905 Validation loss 0.21691495180130005\n",
            "Epoch 7420/30000  training loss: 0.22051434218883514 Validation loss 0.2167840152978897\n",
            "Epoch 7430/30000  training loss: 0.22038225829601288 Validation loss 0.2166529893875122\n",
            "Epoch 7440/30000  training loss: 0.22025008499622345 Validation loss 0.21652191877365112\n",
            "Epoch 7450/30000  training loss: 0.22011780738830566 Validation loss 0.21639074385166168\n",
            "Epoch 7460/30000  training loss: 0.2199854850769043 Validation loss 0.2162594497203827\n",
            "Epoch 7470/30000  training loss: 0.2198529690504074 Validation loss 0.21612802147865295\n",
            "Epoch 7480/30000  training loss: 0.21972033381462097 Validation loss 0.21599653363227844\n",
            "Epoch 7490/30000  training loss: 0.21958769857883453 Validation loss 0.21586495637893677\n",
            "Epoch 7500/30000  training loss: 0.21945497393608093 Validation loss 0.2157333493232727\n",
            "Epoch 7510/30000  training loss: 0.21932215988636017 Validation loss 0.2156016230583191\n",
            "Epoch 7520/30000  training loss: 0.21918924152851105 Validation loss 0.21546980738639832\n",
            "Epoch 7530/30000  training loss: 0.2190561592578888 Validation loss 0.215337872505188\n",
            "Epoch 7540/30000  training loss: 0.21892303228378296 Validation loss 0.2152058333158493\n",
            "Epoch 7550/30000  training loss: 0.21878981590270996 Validation loss 0.21507373452186584\n",
            "Epoch 7560/30000  training loss: 0.21865645051002502 Validation loss 0.21494151651859283\n",
            "Epoch 7570/30000  training loss: 0.2185230851173401 Validation loss 0.21480928361415863\n",
            "Epoch 7580/30000  training loss: 0.2183896154165268 Validation loss 0.21467696130275726\n",
            "Epoch 7590/30000  training loss: 0.21825605630874634 Validation loss 0.21454456448554993\n",
            "Epoch 7600/30000  training loss: 0.2181224226951599 Validation loss 0.21441198885440826\n",
            "Epoch 7610/30000  training loss: 0.21798861026763916 Validation loss 0.21427935361862183\n",
            "Epoch 7620/30000  training loss: 0.21785476803779602 Validation loss 0.21414664387702942\n",
            "Epoch 7630/30000  training loss: 0.21772080659866333 Validation loss 0.21401384472846985\n",
            "Epoch 7640/30000  training loss: 0.21758674085140228 Validation loss 0.21388092637062073\n",
            "Epoch 7650/30000  training loss: 0.21745263040065765 Validation loss 0.2137480080127716\n",
            "Epoch 7660/30000  training loss: 0.21731846034526825 Validation loss 0.21361500024795532\n",
            "Epoch 7670/30000  training loss: 0.21718420088291168 Validation loss 0.21348190307617188\n",
            "Epoch 7680/30000  training loss: 0.2170497626066208 Validation loss 0.2133486121892929\n",
            "Epoch 7690/30000  training loss: 0.21691522002220154 Validation loss 0.21321530640125275\n",
            "Epoch 7700/30000  training loss: 0.2167806625366211 Validation loss 0.21308189630508423\n",
            "Epoch 7710/30000  training loss: 0.21664604544639587 Validation loss 0.2129484862089157\n",
            "Epoch 7720/30000  training loss: 0.2165113240480423 Validation loss 0.21281495690345764\n",
            "Epoch 7730/30000  training loss: 0.21637651324272156 Validation loss 0.2126813679933548\n",
            "Epoch 7740/30000  training loss: 0.21624164283275604 Validation loss 0.2125476747751236\n",
            "Epoch 7750/30000  training loss: 0.2161066234111786 Validation loss 0.21241389214992523\n",
            "Epoch 7760/30000  training loss: 0.2159714549779892 Validation loss 0.21227994561195374\n",
            "Epoch 7770/30000  training loss: 0.21583624184131622 Validation loss 0.21214593946933746\n",
            "Epoch 7780/30000  training loss: 0.21570101380348206 Validation loss 0.21201191842556\n",
            "Epoch 7790/30000  training loss: 0.21556568145751953 Validation loss 0.21187782287597656\n",
            "Epoch 7800/30000  training loss: 0.21543025970458984 Validation loss 0.21174363791942596\n",
            "Epoch 7810/30000  training loss: 0.21529480814933777 Validation loss 0.2116093933582306\n",
            "Epoch 7820/30000  training loss: 0.21515914797782898 Validation loss 0.2114749699831009\n",
            "Epoch 7830/30000  training loss: 0.21502342820167542 Validation loss 0.2113405168056488\n",
            "Epoch 7840/30000  training loss: 0.21488766372203827 Validation loss 0.21120600402355194\n",
            "Epoch 7850/30000  training loss: 0.21475183963775635 Validation loss 0.21107138693332672\n",
            "Epoch 7860/30000  training loss: 0.21461588144302368 Validation loss 0.21093672513961792\n",
            "Epoch 7870/30000  training loss: 0.21447992324829102 Validation loss 0.21080200374126434\n",
            "Epoch 7880/30000  training loss: 0.21434390544891357 Validation loss 0.21066723763942719\n",
            "Epoch 7890/30000  training loss: 0.2142076939344406 Validation loss 0.2105323076248169\n",
            "Epoch 7900/30000  training loss: 0.2140713930130005 Validation loss 0.21039727330207825\n",
            "Epoch 7910/30000  training loss: 0.2139350026845932 Validation loss 0.21026217937469482\n",
            "Epoch 7920/30000  training loss: 0.21379859745502472 Validation loss 0.2101270854473114\n",
            "Epoch 7930/30000  training loss: 0.21366208791732788 Validation loss 0.20999187231063843\n",
            "Epoch 7940/30000  training loss: 0.21352556347846985 Validation loss 0.20985662937164307\n",
            "Epoch 7950/30000  training loss: 0.21338894963264465 Validation loss 0.20972129702568054\n",
            "Epoch 7960/30000  training loss: 0.2132522165775299 Validation loss 0.20958583056926727\n",
            "Epoch 7970/30000  training loss: 0.21311530470848083 Validation loss 0.20945027470588684\n",
            "Epoch 7980/30000  training loss: 0.21297839283943176 Validation loss 0.20931462943553925\n",
            "Epoch 7990/30000  training loss: 0.21284139156341553 Validation loss 0.20917896926403046\n",
            "Epoch 8000/30000  training loss: 0.21270430088043213 Validation loss 0.2090432047843933\n",
            "Epoch 8010/30000  training loss: 0.21256718039512634 Validation loss 0.20890744030475616\n",
            "Epoch 8020/30000  training loss: 0.2124299854040146 Validation loss 0.20877157151699066\n",
            "Epoch 8030/30000  training loss: 0.21229273080825806 Validation loss 0.2086356282234192\n",
            "Epoch 8040/30000  training loss: 0.2121553272008896 Validation loss 0.20849956572055817\n",
            "Epoch 8050/30000  training loss: 0.21201784908771515 Validation loss 0.20836344361305237\n",
            "Epoch 8060/30000  training loss: 0.21188026666641235 Validation loss 0.20822718739509583\n",
            "Epoch 8070/30000  training loss: 0.21174265444278717 Validation loss 0.20809093117713928\n",
            "Epoch 8080/30000  training loss: 0.2116049975156784 Validation loss 0.20795466005802155\n",
            "Epoch 8090/30000  training loss: 0.21146729588508606 Validation loss 0.20781831443309784\n",
            "Epoch 8100/30000  training loss: 0.21132953464984894 Validation loss 0.20768190920352936\n",
            "Epoch 8110/30000  training loss: 0.2111915647983551 Validation loss 0.20754529535770416\n",
            "Epoch 8120/30000  training loss: 0.21105355024337769 Validation loss 0.20740865170955658\n",
            "Epoch 8130/30000  training loss: 0.2109154462814331 Validation loss 0.2072719782590866\n",
            "Epoch 8140/30000  training loss: 0.21077734231948853 Validation loss 0.20713526010513306\n",
            "Epoch 8150/30000  training loss: 0.21063919365406036 Validation loss 0.20699845254421234\n",
            "Epoch 8160/30000  training loss: 0.21050095558166504 Validation loss 0.20686161518096924\n",
            "Epoch 8170/30000  training loss: 0.21036262810230255 Validation loss 0.20672471821308136\n",
            "Epoch 8180/30000  training loss: 0.2102242410182953 Validation loss 0.20658770203590393\n",
            "Epoch 8190/30000  training loss: 0.2100857049226761 Validation loss 0.20645056664943695\n",
            "Epoch 8200/30000  training loss: 0.2099471092224121 Validation loss 0.2063133865594864\n",
            "Epoch 8210/30000  training loss: 0.20980848371982574 Validation loss 0.20617617666721344\n",
            "Epoch 8220/30000  training loss: 0.2096697837114334 Validation loss 0.2060389667749405\n",
            "Epoch 8230/30000  training loss: 0.20953106880187988 Validation loss 0.205901637673378\n",
            "Epoch 8240/30000  training loss: 0.209392249584198 Validation loss 0.20576423406600952\n",
            "Epoch 8250/30000  training loss: 0.20925337076187134 Validation loss 0.20562680065631866\n",
            "Epoch 8260/30000  training loss: 0.20911431312561035 Validation loss 0.20548921823501587\n",
            "Epoch 8270/30000  training loss: 0.20897527039051056 Validation loss 0.20535160601139069\n",
            "Epoch 8280/30000  training loss: 0.2088361531496048 Validation loss 0.20521391928195953\n",
            "Epoch 8290/30000  training loss: 0.20869696140289307 Validation loss 0.2050762176513672\n",
            "Epoch 8300/30000  training loss: 0.20855772495269775 Validation loss 0.2049383968114853\n",
            "Epoch 8310/30000  training loss: 0.20841841399669647 Validation loss 0.2048005908727646\n",
            "Epoch 8320/30000  training loss: 0.2082790732383728 Validation loss 0.20466271042823792\n",
            "Epoch 8330/30000  training loss: 0.2081395983695984 Validation loss 0.2045247107744217\n",
            "Epoch 8340/30000  training loss: 0.2080000340938568 Validation loss 0.2043866217136383\n",
            "Epoch 8350/30000  training loss: 0.20786045491695404 Validation loss 0.20424851775169373\n",
            "Epoch 8360/30000  training loss: 0.20772075653076172 Validation loss 0.20411036908626556\n",
            "Epoch 8370/30000  training loss: 0.2075810730457306 Validation loss 0.20397216081619263\n",
            "Epoch 8380/30000  training loss: 0.2074413150548935 Validation loss 0.20383389294147491\n",
            "Epoch 8390/30000  training loss: 0.20730146765708923 Validation loss 0.20369555056095123\n",
            "Epoch 8400/30000  training loss: 0.20716162025928497 Validation loss 0.20355722308158875\n",
            "Epoch 8410/30000  training loss: 0.2070215940475464 Validation loss 0.20341870188713074\n",
            "Epoch 8420/30000  training loss: 0.20688149333000183 Validation loss 0.20328010618686676\n",
            "Epoch 8430/30000  training loss: 0.20674139261245728 Validation loss 0.20314154028892517\n",
            "Epoch 8440/30000  training loss: 0.20660117268562317 Validation loss 0.20300288498401642\n",
            "Epoch 8450/30000  training loss: 0.20646102726459503 Validation loss 0.20286427438259125\n",
            "Epoch 8460/30000  training loss: 0.20632079243659973 Validation loss 0.20272555947303772\n",
            "Epoch 8470/30000  training loss: 0.20618052780628204 Validation loss 0.2025867998600006\n",
            "Epoch 8480/30000  training loss: 0.206040158867836 Validation loss 0.20244799554347992\n",
            "Epoch 8490/30000  training loss: 0.2058996558189392 Validation loss 0.20230905711650848\n",
            "Epoch 8500/30000  training loss: 0.20575910806655884 Validation loss 0.20217005908489227\n",
            "Epoch 8510/30000  training loss: 0.20561851561069489 Validation loss 0.20203101634979248\n",
            "Epoch 8520/30000  training loss: 0.20547784864902496 Validation loss 0.2018919438123703\n",
            "Epoch 8530/30000  training loss: 0.20533715188503265 Validation loss 0.20175282657146454\n",
            "Epoch 8540/30000  training loss: 0.20519642531871796 Validation loss 0.20161369442939758\n",
            "Epoch 8550/30000  training loss: 0.20505568385124207 Validation loss 0.20147453248500824\n",
            "Epoch 8560/30000  training loss: 0.20491482317447662 Validation loss 0.20133522152900696\n",
            "Epoch 8570/30000  training loss: 0.20477384328842163 Validation loss 0.2011958509683609\n",
            "Epoch 8580/30000  training loss: 0.20463281869888306 Validation loss 0.20105643570423126\n",
            "Epoch 8590/30000  training loss: 0.20449182391166687 Validation loss 0.20091702044010162\n",
            "Epoch 8600/30000  training loss: 0.2043507695198059 Validation loss 0.2007775604724884\n",
            "Epoch 8610/30000  training loss: 0.2042096108198166 Validation loss 0.2006380558013916\n",
            "Epoch 8620/30000  training loss: 0.2040684074163437 Validation loss 0.20049847662448883\n",
            "Epoch 8630/30000  training loss: 0.2039271742105484 Validation loss 0.20035886764526367\n",
            "Epoch 8640/30000  training loss: 0.20378586649894714 Validation loss 0.20021916925907135\n",
            "Epoch 8650/30000  training loss: 0.20364445447921753 Validation loss 0.20007938146591187\n",
            "Epoch 8660/30000  training loss: 0.20350299775600433 Validation loss 0.19993956387043\n",
            "Epoch 8670/30000  training loss: 0.20336148142814636 Validation loss 0.19979970157146454\n",
            "Epoch 8680/30000  training loss: 0.20321998000144958 Validation loss 0.19965985417366028\n",
            "Epoch 8690/30000  training loss: 0.20307841897010803 Validation loss 0.19951997697353363\n",
            "Epoch 8700/30000  training loss: 0.2029368281364441 Validation loss 0.19938001036643982\n",
            "Epoch 8710/30000  training loss: 0.20279517769813538 Validation loss 0.19924002885818481\n",
            "Epoch 8720/30000  training loss: 0.2026534080505371 Validation loss 0.19909991323947906\n",
            "Epoch 8730/30000  training loss: 0.20251160860061646 Validation loss 0.19895979762077332\n",
            "Epoch 8740/30000  training loss: 0.20236976444721222 Validation loss 0.1988196074962616\n",
            "Epoch 8750/30000  training loss: 0.2022278755903244 Validation loss 0.19867941737174988\n",
            "Epoch 8760/30000  training loss: 0.20208589732646942 Validation loss 0.19853916764259338\n",
            "Epoch 8770/30000  training loss: 0.20194396376609802 Validation loss 0.1983988881111145\n",
            "Epoch 8780/30000  training loss: 0.20180203020572662 Validation loss 0.1982586532831192\n",
            "Epoch 8790/30000  training loss: 0.20166000723838806 Validation loss 0.19811837375164032\n",
            "Epoch 8800/30000  training loss: 0.20151787996292114 Validation loss 0.1979779154062271\n",
            "Epoch 8810/30000  training loss: 0.20137567818164825 Validation loss 0.19783742725849152\n",
            "Epoch 8820/30000  training loss: 0.20123346149921417 Validation loss 0.19769690930843353\n",
            "Epoch 8830/30000  training loss: 0.20109118521213531 Validation loss 0.19755639135837555\n",
            "Epoch 8840/30000  training loss: 0.20094890892505646 Validation loss 0.19741585850715637\n",
            "Epoch 8850/30000  training loss: 0.2008066028356552 Validation loss 0.19727525115013123\n",
            "Epoch 8860/30000  training loss: 0.200664222240448 Validation loss 0.1971346139907837\n",
            "Epoch 8870/30000  training loss: 0.20052185654640198 Validation loss 0.19699397683143616\n",
            "Epoch 8880/30000  training loss: 0.2003793567419052 Validation loss 0.19685323536396027\n",
            "Epoch 8890/30000  training loss: 0.20023682713508606 Validation loss 0.19671247899532318\n",
            "Epoch 8900/30000  training loss: 0.20009425282478333 Validation loss 0.19657164812088013\n",
            "Epoch 8910/30000  training loss: 0.19995160400867462 Validation loss 0.19643080234527588\n",
            "Epoch 8920/30000  training loss: 0.1998089849948883 Validation loss 0.19628992676734924\n",
            "Epoch 8930/30000  training loss: 0.19966630637645721 Validation loss 0.1961490511894226\n",
            "Epoch 8940/30000  training loss: 0.19952364265918732 Validation loss 0.19600816071033478\n",
            "Epoch 8950/30000  training loss: 0.19938088953495026 Validation loss 0.19586719572544098\n",
            "Epoch 8960/30000  training loss: 0.19923806190490723 Validation loss 0.1957261562347412\n",
            "Epoch 8970/30000  training loss: 0.19909518957138062 Validation loss 0.19558507204055786\n",
            "Epoch 8980/30000  training loss: 0.19895227253437042 Validation loss 0.19544395804405212\n",
            "Epoch 8990/30000  training loss: 0.19880937039852142 Validation loss 0.19530287384986877\n",
            "Epoch 9000/30000  training loss: 0.19866640865802765 Validation loss 0.19516170024871826\n",
            "Epoch 9010/30000  training loss: 0.1985234171152115 Validation loss 0.19502049684524536\n",
            "Epoch 9020/30000  training loss: 0.19838038086891174 Validation loss 0.19487929344177246\n",
            "Epoch 9030/30000  training loss: 0.1982373148202896 Validation loss 0.19473806023597717\n",
            "Epoch 9040/30000  training loss: 0.19809427857398987 Validation loss 0.19459684193134308\n",
            "Epoch 9050/30000  training loss: 0.19795109331607819 Validation loss 0.19445548951625824\n",
            "Epoch 9060/30000  training loss: 0.19780783355236053 Validation loss 0.19431409239768982\n",
            "Epoch 9070/30000  training loss: 0.19766460359096527 Validation loss 0.1941726803779602\n",
            "Epoch 9080/30000  training loss: 0.1975213587284088 Validation loss 0.19403128325939178\n",
            "Epoch 9090/30000  training loss: 0.19737809896469116 Validation loss 0.19388991594314575\n",
            "Epoch 9100/30000  training loss: 0.19723482429981232 Validation loss 0.19374850392341614\n",
            "Epoch 9110/30000  training loss: 0.19709156453609467 Validation loss 0.19360709190368652\n",
            "Epoch 9120/30000  training loss: 0.19694821536540985 Validation loss 0.19346565008163452\n",
            "Epoch 9130/30000  training loss: 0.19680477678775787 Validation loss 0.19332408905029297\n",
            "Epoch 9140/30000  training loss: 0.1966613382101059 Validation loss 0.19318249821662903\n",
            "Epoch 9150/30000  training loss: 0.19651782512664795 Validation loss 0.1930408924818039\n",
            "Epoch 9160/30000  training loss: 0.19637435674667358 Validation loss 0.19289928674697876\n",
            "Epoch 9170/30000  training loss: 0.19623076915740967 Validation loss 0.19275762140750885\n",
            "Epoch 9180/30000  training loss: 0.19608718156814575 Validation loss 0.19261597096920013\n",
            "Epoch 9190/30000  training loss: 0.1959436535835266 Validation loss 0.19247429072856903\n",
            "Epoch 9200/30000  training loss: 0.1958000510931015 Validation loss 0.19233262538909912\n",
            "Epoch 9210/30000  training loss: 0.19565650820732117 Validation loss 0.1921909749507904\n",
            "Epoch 9220/30000  training loss: 0.1955127865076065 Validation loss 0.19204919040203094\n",
            "Epoch 9230/30000  training loss: 0.19536904990673065 Validation loss 0.1919073611497879\n",
            "Epoch 9240/30000  training loss: 0.1952253133058548 Validation loss 0.19176557660102844\n",
            "Epoch 9250/30000  training loss: 0.19508156180381775 Validation loss 0.1916237473487854\n",
            "Epoch 9260/30000  training loss: 0.1949378103017807 Validation loss 0.19148194789886475\n",
            "Epoch 9270/30000  training loss: 0.19479405879974365 Validation loss 0.1913401484489441\n",
            "Epoch 9280/30000  training loss: 0.19465021789073944 Validation loss 0.19119828939437866\n",
            "Epoch 9290/30000  training loss: 0.19450639188289642 Validation loss 0.19105641543865204\n",
            "Epoch 9300/30000  training loss: 0.19436253607273102 Validation loss 0.19091452658176422\n",
            "Epoch 9310/30000  training loss: 0.19421865046024323 Validation loss 0.19077257812023163\n",
            "Epoch 9320/30000  training loss: 0.19407470524311066 Validation loss 0.19063061475753784\n",
            "Epoch 9330/30000  training loss: 0.19393078982830048 Validation loss 0.19048866629600525\n",
            "Epoch 9340/30000  training loss: 0.19378678500652313 Validation loss 0.19034668803215027\n",
            "Epoch 9350/30000  training loss: 0.19364280998706818 Validation loss 0.1902046650648117\n",
            "Epoch 9360/30000  training loss: 0.19349882006645203 Validation loss 0.1900627166032791\n",
            "Epoch 9370/30000  training loss: 0.19335483014583588 Validation loss 0.18992067873477936\n",
            "Epoch 9380/30000  training loss: 0.19321085512638092 Validation loss 0.18977874517440796\n",
            "Epoch 9390/30000  training loss: 0.19306689500808716 Validation loss 0.1896367371082306\n",
            "Epoch 9400/30000  training loss: 0.19292275607585907 Validation loss 0.18949463963508606\n",
            "Epoch 9410/30000  training loss: 0.1927785873413086 Validation loss 0.18935251235961914\n",
            "Epoch 9420/30000  training loss: 0.1926344633102417 Validation loss 0.18921039998531342\n",
            "Epoch 9430/30000  training loss: 0.19249030947685242 Validation loss 0.1890682727098465\n",
            "Epoch 9440/30000  training loss: 0.19234618544578552 Validation loss 0.18892619013786316\n",
            "Epoch 9450/30000  training loss: 0.19220209121704102 Validation loss 0.1887841522693634\n",
            "Epoch 9460/30000  training loss: 0.19205793738365173 Validation loss 0.18864206969738007\n",
            "Epoch 9470/30000  training loss: 0.19191379845142365 Validation loss 0.18849997222423553\n",
            "Epoch 9480/30000  training loss: 0.1917695850133896 Validation loss 0.18835783004760742\n",
            "Epoch 9490/30000  training loss: 0.19162534177303314 Validation loss 0.18821561336517334\n",
            "Epoch 9500/30000  training loss: 0.1914810836315155 Validation loss 0.18807341158390045\n",
            "Epoch 9510/30000  training loss: 0.19133684039115906 Validation loss 0.18793128430843353\n",
            "Epoch 9520/30000  training loss: 0.19119258224964142 Validation loss 0.18778908252716064\n",
            "Epoch 9530/30000  training loss: 0.19104833900928497 Validation loss 0.18764692544937134\n",
            "Epoch 9540/30000  training loss: 0.19090402126312256 Validation loss 0.18750469386577606\n",
            "Epoch 9550/30000  training loss: 0.19075976312160492 Validation loss 0.18736253678798676\n",
            "Epoch 9560/30000  training loss: 0.19061553478240967 Validation loss 0.18722040951251984\n",
            "Epoch 9570/30000  training loss: 0.1904713213443756 Validation loss 0.18707828223705292\n",
            "Epoch 9580/30000  training loss: 0.190326988697052 Validation loss 0.18693606555461884\n",
            "Epoch 9590/30000  training loss: 0.19018259644508362 Validation loss 0.18679378926753998\n",
            "Epoch 9600/30000  training loss: 0.19003821909427643 Validation loss 0.18665151298046112\n",
            "Epoch 9610/30000  training loss: 0.18989385664463043 Validation loss 0.18650928139686584\n",
            "Epoch 9620/30000  training loss: 0.18974952399730682 Validation loss 0.18636710941791534\n",
            "Epoch 9630/30000  training loss: 0.1896052062511444 Validation loss 0.18622490763664246\n",
            "Epoch 9640/30000  training loss: 0.189460888504982 Validation loss 0.18608273565769196\n",
            "Epoch 9650/30000  training loss: 0.189316526055336 Validation loss 0.18594051897525787\n",
            "Epoch 9660/30000  training loss: 0.18917223811149597 Validation loss 0.18579834699630737\n",
            "Epoch 9670/30000  training loss: 0.18902787566184998 Validation loss 0.18565616011619568\n",
            "Epoch 9680/30000  training loss: 0.188883438706398 Validation loss 0.18551388382911682\n",
            "Epoch 9690/30000  training loss: 0.18873903155326843 Validation loss 0.18537166714668274\n",
            "Epoch 9700/30000  training loss: 0.18859463930130005 Validation loss 0.18522943556308746\n",
            "Epoch 9710/30000  training loss: 0.18845027685165405 Validation loss 0.18508724868297577\n",
            "Epoch 9720/30000  training loss: 0.18830589950084686 Validation loss 0.1849450320005417\n",
            "Epoch 9730/30000  training loss: 0.18816149234771729 Validation loss 0.1848028302192688\n",
            "Epoch 9740/30000  training loss: 0.1880171149969101 Validation loss 0.1846606433391571\n",
            "Epoch 9750/30000  training loss: 0.1878727823495865 Validation loss 0.1845184862613678\n",
            "Epoch 9760/30000  training loss: 0.18772846460342407 Validation loss 0.18437635898590088\n",
            "Epoch 9770/30000  training loss: 0.18758413195610046 Validation loss 0.18423426151275635\n",
            "Epoch 9780/30000  training loss: 0.1874396950006485 Validation loss 0.18409204483032227\n",
            "Epoch 9790/30000  training loss: 0.18729528784751892 Validation loss 0.1839497983455658\n",
            "Epoch 9800/30000  training loss: 0.18715086579322815 Validation loss 0.1838076263666153\n",
            "Epoch 9810/30000  training loss: 0.18700645864009857 Validation loss 0.183665469288826\n",
            "Epoch 9820/30000  training loss: 0.1868620663881302 Validation loss 0.18352331221103668\n",
            "Epoch 9830/30000  training loss: 0.18671773374080658 Validation loss 0.18338122963905334\n",
            "Epoch 9840/30000  training loss: 0.18657341599464417 Validation loss 0.1832391768693924\n",
            "Epoch 9850/30000  training loss: 0.18642914295196533 Validation loss 0.18309710919857025\n",
            "Epoch 9860/30000  training loss: 0.18628481030464172 Validation loss 0.18295502662658691\n",
            "Epoch 9870/30000  training loss: 0.1861405074596405 Validation loss 0.18281300365924835\n",
            "Epoch 9880/30000  training loss: 0.1859961301088333 Validation loss 0.18267086148262024\n",
            "Epoch 9890/30000  training loss: 0.18585170805454254 Validation loss 0.18252874910831451\n",
            "Epoch 9900/30000  training loss: 0.18570737540721893 Validation loss 0.18238665163516998\n",
            "Epoch 9910/30000  training loss: 0.1855630874633789 Validation loss 0.1822446584701538\n",
            "Epoch 9920/30000  training loss: 0.18541878461837769 Validation loss 0.18210263550281525\n",
            "Epoch 9930/30000  training loss: 0.18527449667453766 Validation loss 0.18196064233779907\n",
            "Epoch 9940/30000  training loss: 0.18513023853302002 Validation loss 0.18181867897510529\n",
            "Epoch 9950/30000  training loss: 0.18498599529266357 Validation loss 0.1816767007112503\n",
            "Epoch 9960/30000  training loss: 0.18484172224998474 Validation loss 0.1815347820520401\n",
            "Epoch 9970/30000  training loss: 0.18469756841659546 Validation loss 0.18139289319515228\n",
            "Epoch 9980/30000  training loss: 0.1845533698797226 Validation loss 0.18125098943710327\n",
            "Epoch 9990/30000  training loss: 0.18440909683704376 Validation loss 0.18110904097557068\n",
            "Epoch 10000/30000  training loss: 0.18426485359668732 Validation loss 0.18096715211868286\n",
            "Epoch 10010/30000  training loss: 0.18412065505981445 Validation loss 0.18082527816295624\n",
            "Epoch 10020/30000  training loss: 0.1839764267206192 Validation loss 0.18068340420722961\n",
            "Epoch 10030/30000  training loss: 0.18383222818374634 Validation loss 0.1805415153503418\n",
            "Epoch 10040/30000  training loss: 0.18368808925151825 Validation loss 0.18039973080158234\n",
            "Epoch 10050/30000  training loss: 0.18354400992393494 Validation loss 0.18025799095630646\n",
            "Epoch 10060/30000  training loss: 0.18339988589286804 Validation loss 0.18011625111103058\n",
            "Epoch 10070/30000  training loss: 0.1832558661699295 Validation loss 0.17997460067272186\n",
            "Epoch 10080/30000  training loss: 0.18311187624931335 Validation loss 0.17983296513557434\n",
            "Epoch 10090/30000  training loss: 0.18296781182289124 Validation loss 0.17969122529029846\n",
            "Epoch 10100/30000  training loss: 0.18282368779182434 Validation loss 0.17954951524734497\n",
            "Epoch 10110/30000  training loss: 0.18267960846424103 Validation loss 0.17940779030323029\n",
            "Epoch 10120/30000  training loss: 0.1825355589389801 Validation loss 0.17926612496376038\n",
            "Epoch 10130/30000  training loss: 0.18239156901836395 Validation loss 0.17912451922893524\n",
            "Epoch 10140/30000  training loss: 0.182247593998909 Validation loss 0.1789829432964325\n",
            "Epoch 10150/30000  training loss: 0.18210364878177643 Validation loss 0.17884144186973572\n",
            "Epoch 10160/30000  training loss: 0.18195979297161102 Validation loss 0.17869997024536133\n",
            "Epoch 10170/30000  training loss: 0.1818159520626068 Validation loss 0.17855854332447052\n",
            "Epoch 10180/30000  training loss: 0.1816721260547638 Validation loss 0.17841710150241852\n",
            "Epoch 10190/30000  training loss: 0.18152830004692078 Validation loss 0.1782757043838501\n",
            "Epoch 10200/30000  training loss: 0.18138450384140015 Validation loss 0.17813430726528168\n",
            "Epoch 10210/30000  training loss: 0.18124061822891235 Validation loss 0.17799289524555206\n",
            "Epoch 10220/30000  training loss: 0.18109679222106934 Validation loss 0.17785149812698364\n",
            "Epoch 10230/30000  training loss: 0.1809530258178711 Validation loss 0.17771016061306\n",
            "Epoch 10240/30000  training loss: 0.18080928921699524 Validation loss 0.17756888270378113\n",
            "Epoch 10250/30000  training loss: 0.18066564202308655 Validation loss 0.17742763459682465\n",
            "Epoch 10260/30000  training loss: 0.18052197992801666 Validation loss 0.17728647589683533\n",
            "Epoch 10270/30000  training loss: 0.18037831783294678 Validation loss 0.17714527249336243\n",
            "Epoch 10280/30000  training loss: 0.18023468554019928 Validation loss 0.1770041435956955\n",
            "Epoch 10290/30000  training loss: 0.18009115755558014 Validation loss 0.17686302959918976\n",
            "Epoch 10300/30000  training loss: 0.179947629570961 Validation loss 0.17672199010849\n",
            "Epoch 10310/30000  training loss: 0.17980414628982544 Validation loss 0.1765809953212738\n",
            "Epoch 10320/30000  training loss: 0.17966072261333466 Validation loss 0.1764400452375412\n",
            "Epoch 10330/30000  training loss: 0.17951719462871552 Validation loss 0.17629903554916382\n",
            "Epoch 10340/30000  training loss: 0.17937377095222473 Validation loss 0.17615807056427002\n",
            "Epoch 10350/30000  training loss: 0.17923034727573395 Validation loss 0.1760171353816986\n",
            "Epoch 10360/30000  training loss: 0.17908696830272675 Validation loss 0.17587627470493317\n",
            "Epoch 10370/30000  training loss: 0.1789436638355255 Validation loss 0.1757354438304901\n",
            "Epoch 10380/30000  training loss: 0.1788002997636795 Validation loss 0.17559462785720825\n",
            "Epoch 10390/30000  training loss: 0.17865705490112305 Validation loss 0.17545388638973236\n",
            "Epoch 10400/30000  training loss: 0.1785138100385666 Validation loss 0.17531318962574005\n",
            "Epoch 10410/30000  training loss: 0.1783706545829773 Validation loss 0.17517255246639252\n",
            "Epoch 10420/30000  training loss: 0.17822754383087158 Validation loss 0.17503197491168976\n",
            "Epoch 10430/30000  training loss: 0.17808449268341064 Validation loss 0.17489144206047058\n",
            "Epoch 10440/30000  training loss: 0.1779414862394333 Validation loss 0.17475098371505737\n",
            "Epoch 10450/30000  training loss: 0.17779847979545593 Validation loss 0.17461051046848297\n",
            "Epoch 10460/30000  training loss: 0.177655428647995 Validation loss 0.17447003722190857\n",
            "Epoch 10470/30000  training loss: 0.17751245200634003 Validation loss 0.17432957887649536\n",
            "Epoch 10480/30000  training loss: 0.17736946046352386 Validation loss 0.17418918013572693\n",
            "Epoch 10490/30000  training loss: 0.17722654342651367 Validation loss 0.17404881119728088\n",
            "Epoch 10500/30000  training loss: 0.17708367109298706 Validation loss 0.173908531665802\n",
            "Epoch 10510/30000  training loss: 0.17694085836410522 Validation loss 0.17376825213432312\n",
            "Epoch 10520/30000  training loss: 0.17679809033870697 Validation loss 0.17362812161445618\n",
            "Epoch 10530/30000  training loss: 0.17665539681911469 Validation loss 0.17348800599575043\n",
            "Epoch 10540/30000  training loss: 0.17651276290416718 Validation loss 0.17334794998168945\n",
            "Epoch 10550/30000  training loss: 0.17637017369270325 Validation loss 0.17320795357227325\n",
            "Epoch 10560/30000  training loss: 0.17622767388820648 Validation loss 0.1730680614709854\n",
            "Epoch 10570/30000  training loss: 0.1760852187871933 Validation loss 0.17292821407318115\n",
            "Epoch 10580/30000  training loss: 0.1759428083896637 Validation loss 0.17278839647769928\n",
            "Epoch 10590/30000  training loss: 0.17580029368400574 Validation loss 0.17264847457408905\n",
            "Epoch 10600/30000  training loss: 0.17565782368183136 Validation loss 0.1725086122751236\n",
            "Epoch 10610/30000  training loss: 0.17551541328430176 Validation loss 0.1723688244819641\n",
            "Epoch 10620/30000  training loss: 0.17537304759025574 Validation loss 0.1722290962934494\n",
            "Epoch 10630/30000  training loss: 0.17523077130317688 Validation loss 0.17208945751190186\n",
            "Epoch 10640/30000  training loss: 0.17508850991725922 Validation loss 0.1719498634338379\n",
            "Epoch 10650/30000  training loss: 0.1749463677406311 Validation loss 0.1718103438615799\n",
            "Epoch 10660/30000  training loss: 0.17480424046516418 Validation loss 0.17167088389396667\n",
            "Epoch 10670/30000  training loss: 0.17466221749782562 Validation loss 0.17153151333332062\n",
            "Epoch 10680/30000  training loss: 0.17452023923397064 Validation loss 0.17139218747615814\n",
            "Epoch 10690/30000  training loss: 0.17437833547592163 Validation loss 0.17125295102596283\n",
            "Epoch 10700/30000  training loss: 0.1742364764213562 Validation loss 0.1711137443780899\n",
            "Epoch 10710/30000  training loss: 0.17409470677375793 Validation loss 0.17097465693950653\n",
            "Epoch 10720/30000  training loss: 0.17395296692848206 Validation loss 0.17083556950092316\n",
            "Epoch 10730/30000  training loss: 0.17381122708320618 Validation loss 0.17069648206233978\n",
            "Epoch 10740/30000  training loss: 0.17366942763328552 Validation loss 0.17055737972259521\n",
            "Epoch 10750/30000  training loss: 0.17352771759033203 Validation loss 0.17041835188865662\n",
            "Epoch 10760/30000  training loss: 0.17338605225086212 Validation loss 0.17027941346168518\n",
            "Epoch 10770/30000  training loss: 0.17324449121952057 Validation loss 0.17014054954051971\n",
            "Epoch 10780/30000  training loss: 0.1731029897928238 Validation loss 0.17000174522399902\n",
            "Epoch 10790/30000  training loss: 0.17296157777309418 Validation loss 0.1698630303144455\n",
            "Epoch 10800/30000  training loss: 0.17282019555568695 Validation loss 0.16972436010837555\n",
            "Epoch 10810/30000  training loss: 0.1726789027452469 Validation loss 0.16958576440811157\n",
            "Epoch 10820/30000  training loss: 0.1725376844406128 Validation loss 0.16944727301597595\n",
            "Epoch 10830/30000  training loss: 0.17239652574062347 Validation loss 0.1693088263273239\n",
            "Epoch 10840/30000  training loss: 0.17225544154644012 Validation loss 0.16917051374912262\n",
            "Epoch 10850/30000  training loss: 0.17211441695690155 Validation loss 0.1690322309732437\n",
            "Epoch 10860/30000  training loss: 0.17197349667549133 Validation loss 0.16889402270317078\n",
            "Epoch 10870/30000  training loss: 0.1718326210975647 Validation loss 0.1687558889389038\n",
            "Epoch 10880/30000  training loss: 0.17169176042079926 Validation loss 0.16861778497695923\n",
            "Epoch 10890/30000  training loss: 0.171550914645195 Validation loss 0.16847966611385345\n",
            "Epoch 10900/30000  training loss: 0.17141009867191315 Validation loss 0.16834160685539246\n",
            "Epoch 10910/30000  training loss: 0.17126931250095367 Validation loss 0.16820359230041504\n",
            "Epoch 10920/30000  training loss: 0.17112863063812256 Validation loss 0.16806568205356598\n",
            "Epoch 10930/30000  training loss: 0.17098800837993622 Validation loss 0.1679278165102005\n",
            "Epoch 10940/30000  training loss: 0.17084746062755585 Validation loss 0.16779004037380219\n",
            "Epoch 10950/30000  training loss: 0.17070700228214264 Validation loss 0.16765236854553223\n",
            "Epoch 10960/30000  training loss: 0.1705666184425354 Validation loss 0.16751477122306824\n",
            "Epoch 10970/30000  training loss: 0.17042629420757294 Validation loss 0.16737721860408783\n",
            "Epoch 10980/30000  training loss: 0.17028608918190002 Validation loss 0.16723978519439697\n",
            "Epoch 10990/30000  training loss: 0.1701459139585495 Validation loss 0.16710242629051208\n",
            "Epoch 11000/30000  training loss: 0.17000584304332733 Validation loss 0.16696514189243317\n",
            "Epoch 11010/30000  training loss: 0.16986583173274994 Validation loss 0.1668279618024826\n",
            "Epoch 11020/30000  training loss: 0.1697259098291397 Validation loss 0.16669084131717682\n",
            "Epoch 11030/30000  training loss: 0.16958606243133545 Validation loss 0.166553795337677\n",
            "Epoch 11040/30000  training loss: 0.16944628953933716 Validation loss 0.16641685366630554\n",
            "Epoch 11050/30000  training loss: 0.16930663585662842 Validation loss 0.16628001630306244\n",
            "Epoch 11060/30000  training loss: 0.16916702687740326 Validation loss 0.16614322364330292\n",
            "Epoch 11070/30000  training loss: 0.1690274327993393 Validation loss 0.1660064458847046\n",
            "Epoch 11080/30000  training loss: 0.16888780891895294 Validation loss 0.16586962342262268\n",
            "Epoch 11090/30000  training loss: 0.16874824464321136 Validation loss 0.16573292016983032\n",
            "Epoch 11100/30000  training loss: 0.16860873997211456 Validation loss 0.16559629142284393\n",
            "Epoch 11110/30000  training loss: 0.1684693694114685 Validation loss 0.1654597520828247\n",
            "Epoch 11120/30000  training loss: 0.16833007335662842 Validation loss 0.16532328724861145\n",
            "Epoch 11130/30000  training loss: 0.1681908518075943 Validation loss 0.16518694162368774\n",
            "Epoch 11140/30000  training loss: 0.16805170476436615 Validation loss 0.1650506556034088\n",
            "Epoch 11150/30000  training loss: 0.16791266202926636 Validation loss 0.16491444408893585\n",
            "Epoch 11160/30000  training loss: 0.16777367889881134 Validation loss 0.16477835178375244\n",
            "Epoch 11170/30000  training loss: 0.1676347851753235 Validation loss 0.164642333984375\n",
            "Epoch 11180/30000  training loss: 0.1674959808588028 Validation loss 0.16450640559196472\n",
            "Epoch 11190/30000  training loss: 0.16735729575157166 Validation loss 0.164370596408844\n",
            "Epoch 11200/30000  training loss: 0.1672186404466629 Validation loss 0.16423483192920685\n",
            "Epoch 11210/30000  training loss: 0.1670801192522049 Validation loss 0.16409918665885925\n",
            "Epoch 11220/30000  training loss: 0.16694164276123047 Validation loss 0.16396363079547882\n",
            "Epoch 11230/30000  training loss: 0.1668033003807068 Validation loss 0.16382814943790436\n",
            "Epoch 11240/30000  training loss: 0.16666501760482788 Validation loss 0.16369277238845825\n",
            "Epoch 11250/30000  training loss: 0.16652683913707733 Validation loss 0.16355746984481812\n",
            "Epoch 11260/30000  training loss: 0.16638872027397156 Validation loss 0.16342231631278992\n",
            "Epoch 11270/30000  training loss: 0.16625072062015533 Validation loss 0.1632872372865677\n",
            "Epoch 11280/30000  training loss: 0.1661127358675003 Validation loss 0.16315214335918427\n",
            "Epoch 11290/30000  training loss: 0.16597478091716766 Validation loss 0.16301709413528442\n",
            "Epoch 11300/30000  training loss: 0.165836900472641 Validation loss 0.16288211941719055\n",
            "Epoch 11310/30000  training loss: 0.16569912433624268 Validation loss 0.16274727880954742\n",
            "Epoch 11320/30000  training loss: 0.16556143760681152 Validation loss 0.16261249780654907\n",
            "Epoch 11330/30000  training loss: 0.16542381048202515 Validation loss 0.16247786581516266\n",
            "Epoch 11340/30000  training loss: 0.16528631746768951 Validation loss 0.16234327852725983\n",
            "Epoch 11350/30000  training loss: 0.16514889895915985 Validation loss 0.16220881044864655\n",
            "Epoch 11360/30000  training loss: 0.16501159965991974 Validation loss 0.16207440197467804\n",
            "Epoch 11370/30000  training loss: 0.1648743599653244 Validation loss 0.16194017231464386\n",
            "Epoch 11380/30000  training loss: 0.16473720967769623 Validation loss 0.16180597245693207\n",
            "Epoch 11390/30000  training loss: 0.16460014879703522 Validation loss 0.16167187690734863\n",
            "Epoch 11400/30000  training loss: 0.16446322202682495 Validation loss 0.16153791546821594\n",
            "Epoch 11410/30000  training loss: 0.16432635486125946 Validation loss 0.16140401363372803\n",
            "Epoch 11420/30000  training loss: 0.16418959200382233 Validation loss 0.16127020120620728\n",
            "Epoch 11430/30000  training loss: 0.16405294835567474 Validation loss 0.16113652288913727\n",
            "Epoch 11440/30000  training loss: 0.16391637921333313 Validation loss 0.16100294888019562\n",
            "Epoch 11450/30000  training loss: 0.16377988457679749 Validation loss 0.16086944937705994\n",
            "Epoch 11460/30000  training loss: 0.1636435091495514 Validation loss 0.16073603928089142\n",
            "Epoch 11470/30000  training loss: 0.16350723803043365 Validation loss 0.16060277819633484\n",
            "Epoch 11480/30000  training loss: 0.16337105631828308 Validation loss 0.16046956181526184\n",
            "Epoch 11490/30000  training loss: 0.16323497891426086 Validation loss 0.1603364646434784\n",
            "Epoch 11500/30000  training loss: 0.16309894621372223 Validation loss 0.1602034568786621\n",
            "Epoch 11510/30000  training loss: 0.16296298801898956 Validation loss 0.1600704938173294\n",
            "Epoch 11520/30000  training loss: 0.16282713413238525 Validation loss 0.15993763506412506\n",
            "Epoch 11530/30000  training loss: 0.1626913845539093 Validation loss 0.15980489552021027\n",
            "Epoch 11540/30000  training loss: 0.1625557243824005 Validation loss 0.15967227518558502\n",
            "Epoch 11550/30000  training loss: 0.16242006421089172 Validation loss 0.1595396101474762\n",
            "Epoch 11560/30000  training loss: 0.1622844785451889 Validation loss 0.15940703451633453\n",
            "Epoch 11570/30000  training loss: 0.16214896738529205 Validation loss 0.15927456319332123\n",
            "Epoch 11580/30000  training loss: 0.16201357543468475 Validation loss 0.15914221107959747\n",
            "Epoch 11590/30000  training loss: 0.161878302693367 Validation loss 0.15900994837284088\n",
            "Epoch 11600/30000  training loss: 0.16174310445785522 Validation loss 0.15887778997421265\n",
            "Epoch 11610/30000  training loss: 0.1616080105304718 Validation loss 0.15874576568603516\n",
            "Epoch 11620/30000  training loss: 0.16147305071353912 Validation loss 0.15861384570598602\n",
            "Epoch 11630/30000  training loss: 0.16133825480937958 Validation loss 0.15848208963871002\n",
            "Epoch 11640/30000  training loss: 0.1612035185098648 Validation loss 0.15835040807724\n",
            "Epoch 11650/30000  training loss: 0.16106893122196198 Validation loss 0.1582188606262207\n",
            "Epoch 11660/30000  training loss: 0.1609344333410263 Validation loss 0.15808741748332977\n",
            "Epoch 11670/30000  training loss: 0.1608000248670578 Validation loss 0.1579560786485672\n",
            "Epoch 11680/30000  training loss: 0.16066572070121765 Validation loss 0.15782484412193298\n",
            "Epoch 11690/30000  training loss: 0.16053153574466705 Validation loss 0.15769371390342712\n",
            "Epoch 11700/30000  training loss: 0.1603974550962448 Validation loss 0.15756270289421082\n",
            "Epoch 11710/30000  training loss: 0.16026347875595093 Validation loss 0.15743179619312286\n",
            "Epoch 11720/30000  training loss: 0.1601296067237854 Validation loss 0.15730100870132446\n",
            "Epoch 11730/30000  training loss: 0.15999582409858704 Validation loss 0.15717029571533203\n",
            "Epoch 11740/30000  training loss: 0.15986216068267822 Validation loss 0.15703973174095154\n",
            "Epoch 11750/30000  training loss: 0.15972861647605896 Validation loss 0.1569092571735382\n",
            "Epoch 11760/30000  training loss: 0.15959516167640686 Validation loss 0.15677890181541443\n",
            "Epoch 11770/30000  training loss: 0.15946181118488312 Validation loss 0.1566486358642578\n",
            "Epoch 11780/30000  training loss: 0.15932857990264893 Validation loss 0.15651851892471313\n",
            "Epoch 11790/30000  training loss: 0.1591954529285431 Validation loss 0.15638847649097443\n",
            "Epoch 11800/30000  training loss: 0.1590624302625656 Validation loss 0.15625858306884766\n",
            "Epoch 11810/30000  training loss: 0.1589295119047165 Validation loss 0.15612877905368805\n",
            "Epoch 11820/30000  training loss: 0.15879672765731812 Validation loss 0.155999094247818\n",
            "Epoch 11830/30000  training loss: 0.1586640328168869 Validation loss 0.1558694988489151\n",
            "Epoch 11840/30000  training loss: 0.15853145718574524 Validation loss 0.15574002265930176\n",
            "Epoch 11850/30000  training loss: 0.15839898586273193 Validation loss 0.15561069548130035\n",
            "Epoch 11860/30000  training loss: 0.1582666039466858 Validation loss 0.15548142790794373\n",
            "Epoch 11870/30000  training loss: 0.15813438594341278 Validation loss 0.15535232424736023\n",
            "Epoch 11880/30000  training loss: 0.15800222754478455 Validation loss 0.1552233099937439\n",
            "Epoch 11890/30000  training loss: 0.15787020325660706 Validation loss 0.15509441494941711\n",
            "Epoch 11900/30000  training loss: 0.15773828327655792 Validation loss 0.15496565401554108\n",
            "Epoch 11910/30000  training loss: 0.15760648250579834 Validation loss 0.15483695268630981\n",
            "Epoch 11920/30000  training loss: 0.15747477114200592 Validation loss 0.15470841526985168\n",
            "Epoch 11930/30000  training loss: 0.15734319388866425 Validation loss 0.1545799821615219\n",
            "Epoch 11940/30000  training loss: 0.15721173584461212 Validation loss 0.1544516682624817\n",
            "Epoch 11950/30000  training loss: 0.15708038210868835 Validation loss 0.15432344377040863\n",
            "Epoch 11960/30000  training loss: 0.15694914758205414 Validation loss 0.15419535338878632\n",
            "Epoch 11970/30000  training loss: 0.15681801736354828 Validation loss 0.15406736731529236\n",
            "Epoch 11980/30000  training loss: 0.15668700635433197 Validation loss 0.15393953025341034\n",
            "Epoch 11990/30000  training loss: 0.15655609965324402 Validation loss 0.15381179749965668\n",
            "Epoch 12000/30000  training loss: 0.15642531216144562 Validation loss 0.15368418395519257\n",
            "Epoch 12010/30000  training loss: 0.15629464387893677 Validation loss 0.153556689620018\n",
            "Epoch 12020/30000  training loss: 0.1561639904975891 Validation loss 0.15342918038368225\n",
            "Epoch 12030/30000  training loss: 0.15603341162204742 Validation loss 0.15330179035663605\n",
            "Epoch 12040/30000  training loss: 0.15590298175811768 Validation loss 0.1531745046377182\n",
            "Epoch 12050/30000  training loss: 0.15577271580696106 Validation loss 0.15304742753505707\n",
            "Epoch 12060/30000  training loss: 0.1556425392627716 Validation loss 0.1529204547405243\n",
            "Epoch 12070/30000  training loss: 0.1555124968290329 Validation loss 0.15279357135295868\n",
            "Epoch 12080/30000  training loss: 0.15538257360458374 Validation loss 0.1526668518781662\n",
            "Epoch 12090/30000  training loss: 0.15525275468826294 Validation loss 0.15254023671150208\n",
            "Epoch 12100/30000  training loss: 0.1551230549812317 Validation loss 0.1524137258529663\n",
            "Epoch 12110/30000  training loss: 0.15499348938465118 Validation loss 0.1522873491048813\n",
            "Epoch 12120/30000  training loss: 0.15486402809619904 Validation loss 0.15216109156608582\n",
            "Epoch 12130/30000  training loss: 0.15473468601703644 Validation loss 0.1520349383354187\n",
            "Epoch 12140/30000  training loss: 0.15460547804832458 Validation loss 0.1519089639186859\n",
            "Epoch 12150/30000  training loss: 0.1544763743877411 Validation loss 0.1517830491065979\n",
            "Epoch 12160/30000  training loss: 0.15434740483760834 Validation loss 0.15165729820728302\n",
            "Epoch 12170/30000  training loss: 0.15421852469444275 Validation loss 0.1515316516160965\n",
            "Epoch 12180/30000  training loss: 0.1540897786617279 Validation loss 0.15140610933303833\n",
            "Epoch 12190/30000  training loss: 0.1539611667394638 Validation loss 0.1512807011604309\n",
            "Epoch 12200/30000  training loss: 0.15383267402648926 Validation loss 0.15115542709827423\n",
            "Epoch 12210/30000  training loss: 0.15370428562164307 Validation loss 0.1510302722454071\n",
            "Epoch 12220/30000  training loss: 0.15357603132724762 Validation loss 0.15090525150299072\n",
            "Epoch 12230/30000  training loss: 0.15344788134098053 Validation loss 0.1507803201675415\n",
            "Epoch 12240/30000  training loss: 0.15331988036632538 Validation loss 0.1506555676460266\n",
            "Epoch 12250/30000  training loss: 0.15319204330444336 Validation loss 0.15053097903728485\n",
            "Epoch 12260/30000  training loss: 0.15306435525417328 Validation loss 0.15040647983551025\n",
            "Epoch 12270/30000  training loss: 0.15293675661087036 Validation loss 0.1502821445465088\n",
            "Epoch 12280/30000  training loss: 0.1528092920780182 Validation loss 0.15015791356563568\n",
            "Epoch 12290/30000  training loss: 0.15268194675445557 Validation loss 0.15003381669521332\n",
            "Epoch 12300/30000  training loss: 0.1525547355413437 Validation loss 0.1499098539352417\n",
            "Epoch 12310/30000  training loss: 0.15242765843868256 Validation loss 0.14978599548339844\n",
            "Epoch 12320/30000  training loss: 0.1523006558418274 Validation loss 0.14966227114200592\n",
            "Epoch 12330/30000  training loss: 0.15217381715774536 Validation loss 0.14953869581222534\n",
            "Epoch 12340/30000  training loss: 0.15204711258411407 Validation loss 0.14941522479057312\n",
            "Epoch 12350/30000  training loss: 0.15192049741744995 Validation loss 0.14929185807704926\n",
            "Epoch 12360/30000  training loss: 0.15179400146007538 Validation loss 0.14916865527629852\n",
            "Epoch 12370/30000  training loss: 0.15166765451431274 Validation loss 0.14904558658599854\n",
            "Epoch 12380/30000  training loss: 0.15154144167900085 Validation loss 0.1489226222038269\n",
            "Epoch 12390/30000  training loss: 0.15141531825065613 Validation loss 0.14879979193210602\n",
            "Epoch 12400/30000  training loss: 0.1512894332408905 Validation loss 0.14867714047431946\n",
            "Epoch 12410/30000  training loss: 0.15116363763809204 Validation loss 0.14855462312698364\n",
            "Epoch 12420/30000  training loss: 0.15103796124458313 Validation loss 0.14843223989009857\n",
            "Epoch 12430/30000  training loss: 0.15091243386268616 Validation loss 0.14830996096134186\n",
            "Epoch 12440/30000  training loss: 0.15078702569007874 Validation loss 0.14818786084651947\n",
            "Epoch 12450/30000  training loss: 0.15066173672676086 Validation loss 0.14806582033634186\n",
            "Epoch 12460/30000  training loss: 0.15053658187389374 Validation loss 0.14794397354125977\n",
            "Epoch 12470/30000  training loss: 0.15041157603263855 Validation loss 0.14782221615314484\n",
            "Epoch 12480/30000  training loss: 0.15028665959835052 Validation loss 0.14770062267780304\n",
            "Epoch 12490/30000  training loss: 0.15016187727451324 Validation loss 0.1475791186094284\n",
            "Epoch 12500/30000  training loss: 0.15003728866577148 Validation loss 0.14745782315731049\n",
            "Epoch 12510/30000  training loss: 0.14991290867328644 Validation loss 0.14733675122261047\n",
            "Epoch 12520/30000  training loss: 0.14978867769241333 Validation loss 0.1472157984972\n",
            "Epoch 12530/30000  training loss: 0.14966461062431335 Validation loss 0.14709502458572388\n",
            "Epoch 12540/30000  training loss: 0.14954067766666412 Validation loss 0.14697439968585968\n",
            "Epoch 12550/30000  training loss: 0.14941687881946564 Validation loss 0.14685387909412384\n",
            "Epoch 12560/30000  training loss: 0.1492932140827179 Validation loss 0.14673353731632233\n",
            "Epoch 12570/30000  training loss: 0.1491696834564209 Validation loss 0.14661328494548798\n",
            "Epoch 12580/30000  training loss: 0.14904625713825226 Validation loss 0.14649316668510437\n",
            "Epoch 12590/30000  training loss: 0.14892296493053436 Validation loss 0.1463731974363327\n",
            "Epoch 12600/30000  training loss: 0.14879979193210602 Validation loss 0.14625336229801178\n",
            "Epoch 12610/30000  training loss: 0.1486767679452896 Validation loss 0.14613361656665802\n",
            "Epoch 12620/30000  training loss: 0.14855387806892395 Validation loss 0.1460140347480774\n",
            "Epoch 12630/30000  training loss: 0.14843110740184784 Validation loss 0.14589457213878632\n",
            "Epoch 12640/30000  training loss: 0.14830850064754486 Validation loss 0.14577531814575195\n",
            "Epoch 12650/30000  training loss: 0.1481860727071762 Validation loss 0.14565619826316833\n",
            "Epoch 12660/30000  training loss: 0.1480637788772583 Validation loss 0.14553721249103546\n",
            "Epoch 12670/30000  training loss: 0.14794158935546875 Validation loss 0.14541836082935333\n",
            "Epoch 12680/30000  training loss: 0.14781954884529114 Validation loss 0.14529962837696075\n",
            "Epoch 12690/30000  training loss: 0.14769762754440308 Validation loss 0.14518104493618011\n",
            "Epoch 12700/30000  training loss: 0.14757582545280457 Validation loss 0.14506258070468903\n",
            "Epoch 12710/30000  training loss: 0.1474541872739792 Validation loss 0.1449442356824875\n",
            "Epoch 12720/30000  training loss: 0.14733265340328217 Validation loss 0.1448260247707367\n",
            "Epoch 12730/30000  training loss: 0.14721128344535828 Validation loss 0.14470800757408142\n",
            "Epoch 12740/30000  training loss: 0.14709003269672394 Validation loss 0.1445901095867157\n",
            "Epoch 12750/30000  training loss: 0.14696897566318512 Validation loss 0.14447236061096191\n",
            "Epoch 12760/30000  training loss: 0.14684803783893585 Validation loss 0.14435479044914246\n",
            "Epoch 12770/30000  training loss: 0.1467272788286209 Validation loss 0.14423732459545135\n",
            "Epoch 12780/30000  training loss: 0.14660660922527313 Validation loss 0.1441200077533722\n",
            "Epoch 12790/30000  training loss: 0.1464860737323761 Validation loss 0.14400283992290497\n",
            "Epoch 12800/30000  training loss: 0.146365687251091 Validation loss 0.1438857764005661\n",
            "Epoch 12810/30000  training loss: 0.14624541997909546 Validation loss 0.14376886188983917\n",
            "Epoch 12820/30000  training loss: 0.14612530171871185 Validation loss 0.14365209639072418\n",
            "Epoch 12830/30000  training loss: 0.1460052877664566 Validation loss 0.14353543519973755\n",
            "Epoch 12840/30000  training loss: 0.14588548243045807 Validation loss 0.14341896772384644\n",
            "Epoch 12850/30000  training loss: 0.14576584100723267 Validation loss 0.14330266416072845\n",
            "Epoch 12860/30000  training loss: 0.14564630389213562 Validation loss 0.14318649470806122\n",
            "Epoch 12870/30000  training loss: 0.1455269157886505 Validation loss 0.14307045936584473\n",
            "Epoch 12880/30000  training loss: 0.14540764689445496 Validation loss 0.14295457303524017\n",
            "Epoch 12890/30000  training loss: 0.14528852701187134 Validation loss 0.14283880591392517\n",
            "Epoch 12900/30000  training loss: 0.14516954123973846 Validation loss 0.14272315800189972\n",
            "Epoch 12910/30000  training loss: 0.14505068957805634 Validation loss 0.1426076889038086\n",
            "Epoch 12920/30000  training loss: 0.14493198692798615 Validation loss 0.14249232411384583\n",
            "Epoch 12930/30000  training loss: 0.1448134481906891 Validation loss 0.14237715303897858\n",
            "Epoch 12940/30000  training loss: 0.14469507336616516 Validation loss 0.14226216077804565\n",
            "Epoch 12950/30000  training loss: 0.1445768177509308 Validation loss 0.1421472579240799\n",
            "Epoch 12960/30000  training loss: 0.14445871114730835 Validation loss 0.14203251898288727\n",
            "Epoch 12970/30000  training loss: 0.14434075355529785 Validation loss 0.14191792905330658\n",
            "Epoch 12980/30000  training loss: 0.14422304928302765 Validation loss 0.1418035924434662\n",
            "Epoch 12990/30000  training loss: 0.14410549402236938 Validation loss 0.14168940484523773\n",
            "Epoch 13000/30000  training loss: 0.14398804306983948 Validation loss 0.14157535135746002\n",
            "Epoch 13010/30000  training loss: 0.14387080073356628 Validation loss 0.14146146178245544\n",
            "Epoch 13020/30000  training loss: 0.14375369250774384 Validation loss 0.1413477212190628\n",
            "Epoch 13030/30000  training loss: 0.1436367630958557 Validation loss 0.1412341594696045\n",
            "Epoch 13040/30000  training loss: 0.14351995289325714 Validation loss 0.14112071692943573\n",
            "Epoch 13050/30000  training loss: 0.14340326189994812 Validation loss 0.14100739359855652\n",
            "Epoch 13060/30000  training loss: 0.14328671991825104 Validation loss 0.14089423418045044\n",
            "Epoch 13070/30000  training loss: 0.1431703269481659 Validation loss 0.1407812088727951\n",
            "Epoch 13080/30000  training loss: 0.1430540382862091 Validation loss 0.1406683325767517\n",
            "Epoch 13090/30000  training loss: 0.14293792843818665 Validation loss 0.14055557548999786\n",
            "Epoch 13100/30000  training loss: 0.1428220123052597 Validation loss 0.14044304192066193\n",
            "Epoch 13110/30000  training loss: 0.1427062302827835 Validation loss 0.14033061265945435\n",
            "Epoch 13120/30000  training loss: 0.14259055256843567 Validation loss 0.14021837711334229\n",
            "Epoch 13130/30000  training loss: 0.14247503876686096 Validation loss 0.1401062309741974\n",
            "Epoch 13140/30000  training loss: 0.1423596739768982 Validation loss 0.13999421894550323\n",
            "Epoch 13150/30000  training loss: 0.14224442839622498 Validation loss 0.13988237082958221\n",
            "Epoch 13160/30000  training loss: 0.1421293318271637 Validation loss 0.13977065682411194\n",
            "Epoch 13170/30000  training loss: 0.14201439917087555 Validation loss 0.13965913653373718\n",
            "Epoch 13180/30000  training loss: 0.14189964532852173 Validation loss 0.13954776525497437\n",
            "Epoch 13190/30000  training loss: 0.14178502559661865 Validation loss 0.1394365429878235\n",
            "Epoch 13200/30000  training loss: 0.14167053997516632 Validation loss 0.13932542502880096\n",
            "Epoch 13210/30000  training loss: 0.14155620336532593 Validation loss 0.13921450078487396\n",
            "Epoch 13220/30000  training loss: 0.14144200086593628 Validation loss 0.13910366594791412\n",
            "Epoch 13230/30000  training loss: 0.14132793247699738 Validation loss 0.1389930099248886\n",
            "Epoch 13240/30000  training loss: 0.1412140280008316 Validation loss 0.13888250291347504\n",
            "Epoch 13250/30000  training loss: 0.1411004364490509 Validation loss 0.13877232372760773\n",
            "Epoch 13260/30000  training loss: 0.14098699390888214 Validation loss 0.13866226375102997\n",
            "Epoch 13270/30000  training loss: 0.14087368547916412 Validation loss 0.13855233788490295\n",
            "Epoch 13280/30000  training loss: 0.14076051115989685 Validation loss 0.13844254612922668\n",
            "Epoch 13290/30000  training loss: 0.14064748585224152 Validation loss 0.13833293318748474\n",
            "Epoch 13300/30000  training loss: 0.14053459465503693 Validation loss 0.13822343945503235\n",
            "Epoch 13310/30000  training loss: 0.14042186737060547 Validation loss 0.1381141096353531\n",
            "Epoch 13320/30000  training loss: 0.14030931890010834 Validation loss 0.13800495862960815\n",
            "Epoch 13330/30000  training loss: 0.14019691944122314 Validation loss 0.13789594173431396\n",
            "Epoch 13340/30000  training loss: 0.1400846242904663 Validation loss 0.13778705894947052\n",
            "Epoch 13350/30000  training loss: 0.139972522854805 Validation loss 0.1376783698797226\n",
            "Epoch 13360/30000  training loss: 0.13986052572727203 Validation loss 0.13756974041461945\n",
            "Epoch 13370/30000  training loss: 0.13974866271018982 Validation loss 0.13746128976345062\n",
            "Epoch 13380/30000  training loss: 0.13963700830936432 Validation loss 0.1373530477285385\n",
            "Epoch 13390/30000  training loss: 0.13952548801898956 Validation loss 0.13724492490291595\n",
            "Epoch 13400/30000  training loss: 0.13941414654254913 Validation loss 0.13713696599006653\n",
            "Epoch 13410/30000  training loss: 0.13930292427539825 Validation loss 0.13702915608882904\n",
            "Epoch 13420/30000  training loss: 0.13919185101985931 Validation loss 0.1369214802980423\n",
            "Epoch 13430/30000  training loss: 0.13908089697360992 Validation loss 0.1368139535188675\n",
            "Epoch 13440/30000  training loss: 0.13897010684013367 Validation loss 0.13670653104782104\n",
            "Epoch 13450/30000  training loss: 0.13885948061943054 Validation loss 0.1365993469953537\n",
            "Epoch 13460/30000  training loss: 0.1387491226196289 Validation loss 0.13649240136146545\n",
            "Epoch 13470/30000  training loss: 0.1386389434337616 Validation loss 0.13638561964035034\n",
            "Epoch 13480/30000  training loss: 0.13852889835834503 Validation loss 0.13627898693084717\n",
            "Epoch 13490/30000  training loss: 0.1384190022945404 Validation loss 0.13617248833179474\n",
            "Epoch 13500/30000  training loss: 0.1383092701435089 Validation loss 0.13606613874435425\n",
            "Epoch 13510/30000  training loss: 0.13819968700408936 Validation loss 0.1359599381685257\n",
            "Epoch 13520/30000  training loss: 0.13809028267860413 Validation loss 0.13585394620895386\n",
            "Epoch 13530/30000  training loss: 0.13798099756240845 Validation loss 0.13574807345867157\n",
            "Epoch 13540/30000  training loss: 0.1378718614578247 Validation loss 0.13564234972000122\n",
            "Epoch 13550/30000  training loss: 0.1377628892660141 Validation loss 0.135536789894104\n",
            "Epoch 13560/30000  training loss: 0.13765403628349304 Validation loss 0.13543134927749634\n",
            "Epoch 13570/30000  training loss: 0.13754534721374512 Validation loss 0.1353260576725006\n",
            "Epoch 13580/30000  training loss: 0.13743683695793152 Validation loss 0.1352209448814392\n",
            "Epoch 13590/30000  training loss: 0.13732849061489105 Validation loss 0.13511598110198975\n",
            "Epoch 13600/30000  training loss: 0.13722027838230133 Validation loss 0.13501118123531342\n",
            "Epoch 13610/30000  training loss: 0.13711218535900116 Validation loss 0.13490654528141022\n",
            "Epoch 13620/30000  training loss: 0.13700427114963531 Validation loss 0.13480201363563538\n",
            "Epoch 13630/30000  training loss: 0.13689647614955902 Validation loss 0.13469761610031128\n",
            "Epoch 13640/30000  training loss: 0.13678894937038422 Validation loss 0.13459351658821106\n",
            "Epoch 13650/30000  training loss: 0.13668164610862732 Validation loss 0.13448962569236755\n",
            "Epoch 13660/30000  training loss: 0.13657446205615997 Validation loss 0.1343858391046524\n",
            "Epoch 13670/30000  training loss: 0.13646742701530457 Validation loss 0.134282186627388\n",
            "Epoch 13680/30000  training loss: 0.1363605409860611 Validation loss 0.1341787427663803\n",
            "Epoch 13690/30000  training loss: 0.13625380396842957 Validation loss 0.13407540321350098\n",
            "Epoch 13700/30000  training loss: 0.13614727556705475 Validation loss 0.13397227227687836\n",
            "Epoch 13710/30000  training loss: 0.13604086637496948 Validation loss 0.13386929035186768\n",
            "Epoch 13720/30000  training loss: 0.13593460619449615 Validation loss 0.13376642763614655\n",
            "Epoch 13730/30000  training loss: 0.13582849502563477 Validation loss 0.13366371393203735\n",
            "Epoch 13740/30000  training loss: 0.13572253286838531 Validation loss 0.1335611343383789\n",
            "Epoch 13750/30000  training loss: 0.1356167197227478 Validation loss 0.13345874845981598\n",
            "Epoch 13760/30000  training loss: 0.13551107048988342 Validation loss 0.133356511592865\n",
            "Epoch 13770/30000  training loss: 0.13540561497211456 Validation loss 0.13325445353984833\n",
            "Epoch 13780/30000  training loss: 0.13530027866363525 Validation loss 0.13315249979496002\n",
            "Epoch 13790/30000  training loss: 0.1351950615644455 Validation loss 0.13305070996284485\n",
            "Epoch 13800/30000  training loss: 0.13509000837802887 Validation loss 0.1329490840435028\n",
            "Epoch 13810/30000  training loss: 0.13498526811599731 Validation loss 0.13284774124622345\n",
            "Epoch 13820/30000  training loss: 0.1348806917667389 Validation loss 0.13274656236171722\n",
            "Epoch 13830/30000  training loss: 0.1347762644290924 Validation loss 0.13264551758766174\n",
            "Epoch 13840/30000  training loss: 0.13467195630073547 Validation loss 0.1325446218252182\n",
            "Epoch 13850/30000  training loss: 0.13456781208515167 Validation loss 0.1324438601732254\n",
            "Epoch 13860/30000  training loss: 0.1344638168811798 Validation loss 0.13234324753284454\n",
            "Epoch 13870/30000  training loss: 0.13436000049114227 Validation loss 0.1322428584098816\n",
            "Epoch 13880/30000  training loss: 0.13425631821155548 Validation loss 0.13214260339736938\n",
            "Epoch 13890/30000  training loss: 0.13415279984474182 Validation loss 0.13204246759414673\n",
            "Epoch 13900/30000  training loss: 0.1340494155883789 Validation loss 0.131942480802536\n",
            "Epoch 13910/30000  training loss: 0.13394616544246674 Validation loss 0.13184262812137604\n",
            "Epoch 13920/30000  training loss: 0.1338430941104889 Validation loss 0.13174298405647278\n",
            "Epoch 13930/30000  training loss: 0.13374021649360657 Validation loss 0.13164348900318146\n",
            "Epoch 13940/30000  training loss: 0.1336374580860138 Validation loss 0.1315441131591797\n",
            "Epoch 13950/30000  training loss: 0.13353486359119415 Validation loss 0.13144494593143463\n",
            "Epoch 13960/30000  training loss: 0.1334325224161148 Validation loss 0.1313459873199463\n",
            "Epoch 13970/30000  training loss: 0.1333303451538086 Validation loss 0.13124720752239227\n",
            "Epoch 13980/30000  training loss: 0.1332283467054367 Validation loss 0.13114860653877258\n",
            "Epoch 13990/30000  training loss: 0.13312651216983795 Validation loss 0.13105015456676483\n",
            "Epoch 14000/30000  training loss: 0.13302478194236755 Validation loss 0.13095183670520782\n",
            "Epoch 14010/30000  training loss: 0.13292323052883148 Validation loss 0.13085366785526276\n",
            "Epoch 14020/30000  training loss: 0.13282179832458496 Validation loss 0.13075563311576843\n",
            "Epoch 14030/30000  training loss: 0.13272058963775635 Validation loss 0.13065779209136963\n",
            "Epoch 14040/30000  training loss: 0.1326194852590561 Validation loss 0.13056010007858276\n",
            "Epoch 14050/30000  training loss: 0.13251855969429016 Validation loss 0.13046255707740784\n",
            "Epoch 14060/30000  training loss: 0.13241775333881378 Validation loss 0.13036514818668365\n",
            "Epoch 14070/30000  training loss: 0.13231711089611053 Validation loss 0.1302678883075714\n",
            "Epoch 14080/30000  training loss: 0.1322166472673416 Validation loss 0.1301708221435547\n",
            "Epoch 14090/30000  training loss: 0.13211636245250702 Validation loss 0.1300739049911499\n",
            "Epoch 14100/30000  training loss: 0.13201630115509033 Validation loss 0.1299772709608078\n",
            "Epoch 14110/30000  training loss: 0.13191641867160797 Validation loss 0.12988074123859406\n",
            "Epoch 14120/30000  training loss: 0.13181667029857635 Validation loss 0.12978437542915344\n",
            "Epoch 14130/30000  training loss: 0.13171708583831787 Validation loss 0.12968820333480835\n",
            "Epoch 14140/30000  training loss: 0.13161766529083252 Validation loss 0.129592165350914\n",
            "Epoch 14150/30000  training loss: 0.1315184086561203 Validation loss 0.1294962614774704\n",
            "Epoch 14160/30000  training loss: 0.13141927123069763 Validation loss 0.12940052151679993\n",
            "Epoch 14170/30000  training loss: 0.1313202977180481 Validation loss 0.1293049156665802\n",
            "Epoch 14180/30000  training loss: 0.1312214732170105 Validation loss 0.129209503531456\n",
            "Epoch 14190/30000  training loss: 0.13112284243106842 Validation loss 0.12911422550678253\n",
            "Epoch 14200/30000  training loss: 0.1310243159532547 Validation loss 0.129019096493721\n",
            "Epoch 14210/30000  training loss: 0.1309259533882141 Validation loss 0.12892410159111023\n",
            "Epoch 14220/30000  training loss: 0.13082775473594666 Validation loss 0.12882930040359497\n",
            "Epoch 14230/30000  training loss: 0.13072985410690308 Validation loss 0.1287347674369812\n",
            "Epoch 14240/30000  training loss: 0.13063210248947144 Validation loss 0.12864038348197937\n",
            "Epoch 14250/30000  training loss: 0.13053449988365173 Validation loss 0.12854614853858948\n",
            "Epoch 14260/30000  training loss: 0.13043701648712158 Validation loss 0.12845204770565033\n",
            "Epoch 14270/30000  training loss: 0.13033969700336456 Validation loss 0.12835809588432312\n",
            "Epoch 14280/30000  training loss: 0.13024254143238068 Validation loss 0.12826433777809143\n",
            "Epoch 14290/30000  training loss: 0.13014554977416992 Validation loss 0.1281706988811493\n",
            "Epoch 14300/30000  training loss: 0.1300487071275711 Validation loss 0.12807722389698029\n",
            "Epoch 14310/30000  training loss: 0.12995201349258423 Validation loss 0.12798388302326202\n",
            "Epoch 14320/30000  training loss: 0.1298554688692093 Validation loss 0.1278906911611557\n",
            "Epoch 14330/30000  training loss: 0.12975908815860748 Validation loss 0.1277976781129837\n",
            "Epoch 14340/30000  training loss: 0.12966285645961761 Validation loss 0.12770482897758484\n",
            "Epoch 14350/30000  training loss: 0.12956687808036804 Validation loss 0.12761221826076508\n",
            "Epoch 14360/30000  training loss: 0.1294710636138916 Validation loss 0.12751977145671844\n",
            "Epoch 14370/30000  training loss: 0.1293753981590271 Validation loss 0.12742745876312256\n",
            "Epoch 14380/30000  training loss: 0.12927991151809692 Validation loss 0.1273353397846222\n",
            "Epoch 14390/30000  training loss: 0.12918458878993988 Validation loss 0.12724334001541138\n",
            "Epoch 14400/30000  training loss: 0.1290893852710724 Validation loss 0.1271515041589737\n",
            "Epoch 14410/30000  training loss: 0.12899433076381683 Validation loss 0.12705980241298676\n",
            "Epoch 14420/30000  training loss: 0.1288994550704956 Validation loss 0.12696827948093414\n",
            "Epoch 14430/30000  training loss: 0.12880472838878632 Validation loss 0.12687690556049347\n",
            "Epoch 14440/30000  training loss: 0.12871013581752777 Validation loss 0.12678566575050354\n",
            "Epoch 14450/30000  training loss: 0.12861570715904236 Validation loss 0.12669458985328674\n",
            "Epoch 14460/30000  training loss: 0.1285213828086853 Validation loss 0.1266036331653595\n",
            "Epoch 14470/30000  training loss: 0.1284274160861969 Validation loss 0.12651298940181732\n",
            "Epoch 14480/30000  training loss: 0.12833358347415924 Validation loss 0.12642250955104828\n",
            "Epoch 14490/30000  training loss: 0.12823988497257233 Validation loss 0.1263321489095688\n",
            "Epoch 14500/30000  training loss: 0.12814632058143616 Validation loss 0.12624193727970123\n",
            "Epoch 14510/30000  training loss: 0.12805292010307312 Validation loss 0.1261518895626068\n",
            "Epoch 14520/30000  training loss: 0.1279596984386444 Validation loss 0.12606202065944672\n",
            "Epoch 14530/30000  training loss: 0.12786662578582764 Validation loss 0.12597227096557617\n",
            "Epoch 14540/30000  training loss: 0.1277736872434616 Validation loss 0.12588265538215637\n",
            "Epoch 14550/30000  training loss: 0.12768086791038513 Validation loss 0.1257931888103485\n",
            "Epoch 14560/30000  training loss: 0.12758825719356537 Validation loss 0.12570390105247498\n",
            "Epoch 14570/30000  training loss: 0.12749578058719635 Validation loss 0.12561476230621338\n",
            "Epoch 14580/30000  training loss: 0.12740351259708405 Validation loss 0.1255258321762085\n",
            "Epoch 14590/30000  training loss: 0.12731148302555084 Validation loss 0.12543711066246033\n",
            "Epoch 14600/30000  training loss: 0.1272195428609848 Validation loss 0.1253485083580017\n",
            "Epoch 14610/30000  training loss: 0.12712782621383667 Validation loss 0.1252601146697998\n",
            "Epoch 14620/30000  training loss: 0.1270362287759781 Validation loss 0.12517184019088745\n",
            "Epoch 14630/30000  training loss: 0.12694475054740906 Validation loss 0.12508371472358704\n",
            "Epoch 14640/30000  training loss: 0.12685346603393555 Validation loss 0.12499570846557617\n",
            "Epoch 14650/30000  training loss: 0.12676231563091278 Validation loss 0.12490788847208023\n",
            "Epoch 14660/30000  training loss: 0.12667135894298553 Validation loss 0.12482023239135742\n",
            "Epoch 14670/30000  training loss: 0.12658050656318665 Validation loss 0.12473270297050476\n",
            "Epoch 14680/30000  training loss: 0.1264897882938385 Validation loss 0.12464531511068344\n",
            "Epoch 14690/30000  training loss: 0.12639929354190826 Validation loss 0.12455814331769943\n",
            "Epoch 14700/30000  training loss: 0.12630903720855713 Validation loss 0.12447120249271393\n",
            "Epoch 14710/30000  training loss: 0.12621894478797913 Validation loss 0.12438440322875977\n",
            "Epoch 14720/30000  training loss: 0.12612897157669067 Validation loss 0.12429773062467575\n",
            "Epoch 14730/30000  training loss: 0.12603911757469177 Validation loss 0.12421120703220367\n",
            "Epoch 14740/30000  training loss: 0.12594947218894958 Validation loss 0.12412485480308533\n",
            "Epoch 14750/30000  training loss: 0.12585996091365814 Validation loss 0.12403864413499832\n",
            "Epoch 14760/30000  training loss: 0.12577059864997864 Validation loss 0.12395257502794266\n",
            "Epoch 14770/30000  training loss: 0.12568138539791107 Validation loss 0.12386664748191833\n",
            "Epoch 14780/30000  training loss: 0.12559229135513306 Validation loss 0.12378086894750595\n",
            "Epoch 14790/30000  training loss: 0.12550337612628937 Validation loss 0.1236952617764473\n",
            "Epoch 14800/30000  training loss: 0.12541471421718597 Validation loss 0.12360990792512894\n",
            "Epoch 14810/30000  training loss: 0.1253262460231781 Validation loss 0.12352469563484192\n",
            "Epoch 14820/30000  training loss: 0.1252378672361374 Validation loss 0.12343961745500565\n",
            "Epoch 14830/30000  training loss: 0.1251496821641922 Validation loss 0.12335474044084549\n",
            "Epoch 14840/30000  training loss: 0.12506164610385895 Validation loss 0.12326997518539429\n",
            "Epoch 14850/30000  training loss: 0.12497374415397644 Validation loss 0.12318535149097443\n",
            "Epoch 14860/30000  training loss: 0.12488596886396408 Validation loss 0.12310086190700531\n",
            "Epoch 14870/30000  training loss: 0.12479839473962784 Validation loss 0.12301655858755112\n",
            "Epoch 14880/30000  training loss: 0.12471094727516174 Validation loss 0.12293239682912827\n",
            "Epoch 14890/30000  training loss: 0.1246236264705658 Validation loss 0.12284836918115616\n",
            "Epoch 14900/30000  training loss: 0.12453652918338776 Validation loss 0.12276452034711838\n",
            "Epoch 14910/30000  training loss: 0.12444963306188583 Validation loss 0.1226809173822403\n",
            "Epoch 14920/30000  training loss: 0.12436290830373764 Validation loss 0.12259747087955475\n",
            "Epoch 14930/30000  training loss: 0.12427634000778198 Validation loss 0.12251415103673935\n",
            "Epoch 14940/30000  training loss: 0.12418988347053528 Validation loss 0.1224309578537941\n",
            "Epoch 14950/30000  training loss: 0.12410356849431992 Validation loss 0.12234791368246078\n",
            "Epoch 14960/30000  training loss: 0.12401744723320007 Validation loss 0.1222650557756424\n",
            "Epoch 14970/30000  training loss: 0.12393144518136978 Validation loss 0.12218231707811356\n",
            "Epoch 14980/30000  training loss: 0.12384558469057083 Validation loss 0.12209971249103546\n",
            "Epoch 14990/30000  training loss: 0.12375985831022263 Validation loss 0.12201723456382751\n",
            "Epoch 15000/30000  training loss: 0.12367438524961472 Validation loss 0.12193502485752106\n",
            "Epoch 15010/30000  training loss: 0.12358909100294113 Validation loss 0.12185300886631012\n",
            "Epoch 15020/30000  training loss: 0.12350396811962128 Validation loss 0.12177109718322754\n",
            "Epoch 15030/30000  training loss: 0.12341894209384918 Validation loss 0.1216893196105957\n",
            "Epoch 15040/30000  training loss: 0.12333408743143082 Validation loss 0.12160774320363998\n",
            "Epoch 15050/30000  training loss: 0.12324941903352737 Validation loss 0.12152630090713501\n",
            "Epoch 15060/30000  training loss: 0.12316486239433289 Validation loss 0.1214449554681778\n",
            "Epoch 15070/30000  training loss: 0.12308043241500854 Validation loss 0.12136377394199371\n",
            "Epoch 15080/30000  training loss: 0.12299616634845734 Validation loss 0.12128276377916336\n",
            "Epoch 15090/30000  training loss: 0.12291206419467926 Validation loss 0.12120190262794495\n",
            "Epoch 15100/30000  training loss: 0.1228281706571579 Validation loss 0.12112124264240265\n",
            "Epoch 15110/30000  training loss: 0.12274445593357086 Validation loss 0.12104074656963348\n",
            "Epoch 15120/30000  training loss: 0.12266088277101517 Validation loss 0.12096041440963745\n",
            "Epoch 15130/30000  training loss: 0.1225774884223938 Validation loss 0.12088023871183395\n",
            "Epoch 15140/30000  training loss: 0.12249422073364258 Validation loss 0.1208001896739006\n",
            "Epoch 15150/30000  training loss: 0.1224110797047615 Validation loss 0.1207202672958374\n",
            "Epoch 15160/30000  training loss: 0.12232809513807297 Validation loss 0.12064050137996674\n",
            "Epoch 15170/30000  training loss: 0.12224525958299637 Validation loss 0.1205609068274498\n",
            "Epoch 15180/30000  training loss: 0.1221625879406929 Validation loss 0.12048143148422241\n",
            "Epoch 15190/30000  training loss: 0.12208004295825958 Validation loss 0.12040209770202637\n",
            "Epoch 15200/30000  training loss: 0.12199774384498596 Validation loss 0.12032300233840942\n",
            "Epoch 15210/30000  training loss: 0.12191563844680786 Validation loss 0.1202440932393074\n",
            "Epoch 15220/30000  training loss: 0.12183363735675812 Validation loss 0.12016531825065613\n",
            "Epoch 15230/30000  training loss: 0.12175179272890091 Validation loss 0.1200866550207138\n",
            "Epoch 15240/30000  training loss: 0.12167006731033325 Validation loss 0.12000813335180283\n",
            "Epoch 15250/30000  training loss: 0.12158852070569992 Validation loss 0.11992979794740677\n",
            "Epoch 15260/30000  training loss: 0.12150710821151733 Validation loss 0.11985156685113907\n",
            "Epoch 15270/30000  training loss: 0.12142582982778549 Validation loss 0.1197734996676445\n",
            "Epoch 15280/30000  training loss: 0.1213446781039238 Validation loss 0.1196955218911171\n",
            "Epoch 15290/30000  training loss: 0.1212637647986412 Validation loss 0.11961783468723297\n",
            "Epoch 15300/30000  training loss: 0.12118306010961533 Validation loss 0.11954028904438019\n",
            "Epoch 15310/30000  training loss: 0.1211024671792984 Validation loss 0.11946287006139755\n",
            "Epoch 15320/30000  training loss: 0.12102199345827103 Validation loss 0.11938560009002686\n",
            "Epoch 15330/30000  training loss: 0.12094172835350037 Validation loss 0.11930849403142929\n",
            "Epoch 15340/30000  training loss: 0.12086157500743866 Validation loss 0.11923151463270187\n",
            "Epoch 15350/30000  training loss: 0.1207815557718277 Validation loss 0.1191546767950058\n",
            "Epoch 15360/30000  training loss: 0.12070165574550629 Validation loss 0.11907795071601868\n",
            "Epoch 15370/30000  training loss: 0.1206219419836998 Validation loss 0.11900139600038528\n",
            "Epoch 15380/30000  training loss: 0.12054239213466644 Validation loss 0.11892504245042801\n",
            "Epoch 15390/30000  training loss: 0.1204630583524704 Validation loss 0.11884886026382446\n",
            "Epoch 15400/30000  training loss: 0.12038387358188629 Validation loss 0.11877282708883286\n",
            "Epoch 15410/30000  training loss: 0.12030484527349472 Validation loss 0.11869694292545319\n",
            "Epoch 15420/30000  training loss: 0.1202259361743927 Validation loss 0.11862120777368546\n",
            "Epoch 15430/30000  training loss: 0.12014717608690262 Validation loss 0.11854557693004608\n",
            "Epoch 15440/30000  training loss: 0.1200685054063797 Validation loss 0.11847008019685745\n",
            "Epoch 15450/30000  training loss: 0.1199900433421135 Validation loss 0.11839476972818375\n",
            "Epoch 15460/30000  training loss: 0.11991170793771744 Validation loss 0.11831957846879959\n",
            "Epoch 15470/30000  training loss: 0.11983351409435272 Validation loss 0.11824452131986618\n",
            "Epoch 15480/30000  training loss: 0.11975553631782532 Validation loss 0.11816971004009247\n",
            "Epoch 15490/30000  training loss: 0.11967775225639343 Validation loss 0.1180950403213501\n",
            "Epoch 15500/30000  training loss: 0.11960010230541229 Validation loss 0.11802051961421967\n",
            "Epoch 15510/30000  training loss: 0.1195225641131401 Validation loss 0.11794611811637878\n",
            "Epoch 15520/30000  training loss: 0.11944515258073807 Validation loss 0.11787184327840805\n",
            "Epoch 15530/30000  training loss: 0.11936793476343155 Validation loss 0.11779773235321045\n",
            "Epoch 15540/30000  training loss: 0.1192908063530922 Validation loss 0.117723748087883\n",
            "Epoch 15550/30000  training loss: 0.11921382695436478 Validation loss 0.1176498681306839\n",
            "Epoch 15560/30000  training loss: 0.1191369891166687 Validation loss 0.11757617443799973\n",
            "Epoch 15570/30000  training loss: 0.11906042695045471 Validation loss 0.11750272661447525\n",
            "Epoch 15580/30000  training loss: 0.11898399889469147 Validation loss 0.11742939800024033\n",
            "Epoch 15590/30000  training loss: 0.11890768259763718 Validation loss 0.11735618859529495\n",
            "Epoch 15600/30000  training loss: 0.11883150786161423 Validation loss 0.11728312075138092\n",
            "Epoch 15610/30000  training loss: 0.11875549703836441 Validation loss 0.11721022427082062\n",
            "Epoch 15620/30000  training loss: 0.11867959797382355 Validation loss 0.11713743209838867\n",
            "Epoch 15630/30000  training loss: 0.11860385537147522 Validation loss 0.11706475913524628\n",
            "Epoch 15640/30000  training loss: 0.11852820962667465 Validation loss 0.11699224263429642\n",
            "Epoch 15650/30000  training loss: 0.1184527799487114 Validation loss 0.11691991239786148\n",
            "Epoch 15660/30000  training loss: 0.11837755888700485 Validation loss 0.11684777587652206\n",
            "Epoch 15670/30000  training loss: 0.11830244213342667 Validation loss 0.116775743663311\n",
            "Epoch 15680/30000  training loss: 0.118227519094944 Validation loss 0.11670389771461487\n",
            "Epoch 15690/30000  training loss: 0.1181526929140091 Validation loss 0.11663217097520828\n",
            "Epoch 15700/30000  training loss: 0.11807801574468613 Validation loss 0.11656055599451065\n",
            "Epoch 15710/30000  training loss: 0.11800345778465271 Validation loss 0.11648907512426376\n",
            "Epoch 15720/30000  training loss: 0.11792904883623123 Validation loss 0.11641774326562881\n",
            "Epoch 15730/30000  training loss: 0.1178547814488411 Validation loss 0.11634654551744461\n",
            "Epoch 15740/30000  training loss: 0.11778067797422409 Validation loss 0.11627553403377533\n",
            "Epoch 15750/30000  training loss: 0.1177067682147026 Validation loss 0.11620467156171799\n",
            "Epoch 15760/30000  training loss: 0.11763304471969604 Validation loss 0.11613401025533676\n",
            "Epoch 15770/30000  training loss: 0.11755943298339844 Validation loss 0.11606345325708389\n",
            "Epoch 15780/30000  training loss: 0.11748593300580978 Validation loss 0.11599300056695938\n",
            "Epoch 15790/30000  training loss: 0.11741256713867188 Validation loss 0.1159227043390274\n",
            "Epoch 15800/30000  training loss: 0.1173393577337265 Validation loss 0.11585254222154617\n",
            "Epoch 15810/30000  training loss: 0.11726628243923187 Validation loss 0.11578250676393509\n",
            "Epoch 15820/30000  training loss: 0.1171933189034462 Validation loss 0.11571258306503296\n",
            "Epoch 15830/30000  training loss: 0.11712057143449783 Validation loss 0.11564292013645172\n",
            "Epoch 15840/30000  training loss: 0.11704803258180618 Validation loss 0.11557340621948242\n",
            "Epoch 15850/30000  training loss: 0.11697559058666229 Validation loss 0.11550399661064148\n",
            "Epoch 15860/30000  training loss: 0.11690329015254974 Validation loss 0.11543471366167068\n",
            "Epoch 15870/30000  training loss: 0.11683110147714615 Validation loss 0.11536557227373123\n",
            "Epoch 15880/30000  training loss: 0.11675907671451569 Validation loss 0.11529655009508133\n",
            "Epoch 15890/30000  training loss: 0.11668716371059418 Validation loss 0.11522767692804337\n",
            "Epoch 15900/30000  training loss: 0.11661536246538162 Validation loss 0.11515888571739197\n",
            "Epoch 15910/30000  training loss: 0.11654374748468399 Validation loss 0.11509031802415848\n",
            "Epoch 15920/30000  training loss: 0.11647237837314606 Validation loss 0.1150219589471817\n",
            "Epoch 15930/30000  training loss: 0.11640109121799469 Validation loss 0.1149536743760109\n",
            "Epoch 15940/30000  training loss: 0.11632994562387466 Validation loss 0.11488554626703262\n",
            "Epoch 15950/30000  training loss: 0.11625894904136658 Validation loss 0.1148175373673439\n",
            "Epoch 15960/30000  training loss: 0.11618808656930923 Validation loss 0.11474968492984772\n",
            "Epoch 15970/30000  training loss: 0.11611734330654144 Validation loss 0.1146819218993187\n",
            "Epoch 15980/30000  training loss: 0.11604669690132141 Validation loss 0.11461427807807922\n",
            "Epoch 15990/30000  training loss: 0.1159762293100357 Validation loss 0.11454681307077408\n",
            "Epoch 16000/30000  training loss: 0.11590597778558731 Validation loss 0.11447954922914505\n",
            "Epoch 16010/30000  training loss: 0.11583582311868668 Validation loss 0.11441241204738617\n",
            "Epoch 16020/30000  training loss: 0.11576586216688156 Validation loss 0.11434541642665863\n",
            "Epoch 16030/30000  training loss: 0.115696020424366 Validation loss 0.11427855491638184\n",
            "Epoch 16040/30000  training loss: 0.11562629789113998 Validation loss 0.11421181261539459\n",
            "Epoch 16050/30000  training loss: 0.11555667966604233 Validation loss 0.1141451820731163\n",
            "Epoch 16060/30000  training loss: 0.11548721045255661 Validation loss 0.11407867074012756\n",
            "Epoch 16070/30000  training loss: 0.11541789770126343 Validation loss 0.11401231586933136\n",
            "Epoch 16080/30000  training loss: 0.1153487116098404 Validation loss 0.11394611746072769\n",
            "Epoch 16090/30000  training loss: 0.11527973413467407 Validation loss 0.11388009041547775\n",
            "Epoch 16100/30000  training loss: 0.11521089822053909 Validation loss 0.11381423473358154\n",
            "Epoch 16110/30000  training loss: 0.11514219641685486 Validation loss 0.1137484684586525\n",
            "Epoch 16120/30000  training loss: 0.11507361382246017 Validation loss 0.1136828288435936\n",
            "Epoch 16130/30000  training loss: 0.11500512063503265 Validation loss 0.11361728608608246\n",
            "Epoch 16140/30000  training loss: 0.11493679881095886 Validation loss 0.11355192959308624\n",
            "Epoch 16150/30000  training loss: 0.11486860364675522 Validation loss 0.11348666995763779\n",
            "Epoch 16160/30000  training loss: 0.11480052769184113 Validation loss 0.11342152208089828\n",
            "Epoch 16170/30000  training loss: 0.11473266035318375 Validation loss 0.11335662752389908\n",
            "Epoch 16180/30000  training loss: 0.1146649494767189 Validation loss 0.11329183727502823\n",
            "Epoch 16190/30000  training loss: 0.1145973727107048 Validation loss 0.11322718113660812\n",
            "Epoch 16200/30000  training loss: 0.11452990770339966 Validation loss 0.11316262930631638\n",
            "Epoch 16210/30000  training loss: 0.11446255445480347 Validation loss 0.11309823393821716\n",
            "Epoch 16220/30000  training loss: 0.11439535766839981 Validation loss 0.1130339503288269\n",
            "Epoch 16230/30000  training loss: 0.11432825773954391 Validation loss 0.11296974122524261\n",
            "Epoch 16240/30000  training loss: 0.11426126956939697 Validation loss 0.11290568858385086\n",
            "Epoch 16250/30000  training loss: 0.11419453471899033 Validation loss 0.1128418818116188\n",
            "Epoch 16260/30000  training loss: 0.11412793397903442 Validation loss 0.1127781867980957\n",
            "Epoch 16270/30000  training loss: 0.11406144499778748 Validation loss 0.11271461099386215\n",
            "Epoch 16280/30000  training loss: 0.11399507522583008 Validation loss 0.11265112459659576\n",
            "Epoch 16290/30000  training loss: 0.11392884701490402 Validation loss 0.1125878095626831\n",
            "Epoch 16300/30000  training loss: 0.11386275291442871 Validation loss 0.1125245988368988\n",
            "Epoch 16310/30000  training loss: 0.11379675567150116 Validation loss 0.11246148496866226\n",
            "Epoch 16320/30000  training loss: 0.11373087018728256 Validation loss 0.11239849776029587\n",
            "Epoch 16330/30000  training loss: 0.11366520076990128 Validation loss 0.11233572661876678\n",
            "Epoch 16340/30000  training loss: 0.11359971761703491 Validation loss 0.11227311193943024\n",
            "Epoch 16350/30000  training loss: 0.11353430151939392 Validation loss 0.11221057921648026\n",
            "Epoch 16360/30000  training loss: 0.11346903443336487 Validation loss 0.11214819550514221\n",
            "Epoch 16370/30000  training loss: 0.11340390145778656 Validation loss 0.11208593845367432\n",
            "Epoch 16380/30000  training loss: 0.1133388951420784 Validation loss 0.11202377080917358\n",
            "Epoch 16390/30000  training loss: 0.11327396333217621 Validation loss 0.111961729824543\n",
            "Epoch 16400/30000  training loss: 0.11320918053388596 Validation loss 0.11189980804920197\n",
            "Epoch 16410/30000  training loss: 0.11314459145069122 Validation loss 0.11183807253837585\n",
            "Epoch 16420/30000  training loss: 0.11308015137910843 Validation loss 0.11177648603916168\n",
            "Epoch 16430/30000  training loss: 0.11301583051681519 Validation loss 0.11171503365039825\n",
            "Epoch 16440/30000  training loss: 0.11295165866613388 Validation loss 0.11165371537208557\n",
            "Epoch 16450/30000  training loss: 0.11288759112358093 Validation loss 0.11159248650074005\n",
            "Epoch 16460/30000  training loss: 0.11282363533973694 Validation loss 0.11153136193752289\n",
            "Epoch 16470/30000  training loss: 0.1127597913146019 Validation loss 0.11147036403417587\n",
            "Epoch 16480/30000  training loss: 0.11269611120223999 Validation loss 0.11140952259302139\n",
            "Epoch 16490/30000  training loss: 0.11263256520032883 Validation loss 0.11134880036115646\n",
            "Epoch 16500/30000  training loss: 0.1125691682100296 Validation loss 0.11128824949264526\n",
            "Epoch 16510/30000  training loss: 0.1125059574842453 Validation loss 0.111227847635746\n",
            "Epoch 16520/30000  training loss: 0.11244284361600876 Validation loss 0.1111675575375557\n",
            "Epoch 16530/30000  training loss: 0.11237981915473938 Validation loss 0.11110736429691315\n",
            "Epoch 16540/30000  training loss: 0.11231693625450134 Validation loss 0.11104726791381836\n",
            "Epoch 16550/30000  training loss: 0.11225415766239166 Validation loss 0.11098732799291611\n",
            "Epoch 16560/30000  training loss: 0.11219152063131332 Validation loss 0.11092748492956161\n",
            "Epoch 16570/30000  training loss: 0.11212899535894394 Validation loss 0.11086776852607727\n",
            "Epoch 16580/30000  training loss: 0.11206667870283127 Validation loss 0.11080826073884964\n",
            "Epoch 16590/30000  training loss: 0.11200449615716934 Validation loss 0.11074887961149216\n",
            "Epoch 16600/30000  training loss: 0.11194244027137756 Validation loss 0.11068959534168243\n",
            "Epoch 16610/30000  training loss: 0.11188047379255295 Validation loss 0.11063040792942047\n",
            "Epoch 16620/30000  training loss: 0.11181860417127609 Validation loss 0.11057135462760925\n",
            "Epoch 16630/30000  training loss: 0.11175690591335297 Validation loss 0.11051242798566818\n",
            "Epoch 16640/30000  training loss: 0.11169528216123581 Validation loss 0.11045358330011368\n",
            "Epoch 16650/30000  training loss: 0.1116337776184082 Validation loss 0.11039487272500992\n",
            "Epoch 16660/30000  training loss: 0.11157253384590149 Validation loss 0.11033640801906586\n",
            "Epoch 16670/30000  training loss: 0.11151137202978134 Validation loss 0.11027801036834717\n",
            "Epoch 16680/30000  training loss: 0.11145032197237015 Validation loss 0.11021973192691803\n",
            "Epoch 16690/30000  training loss: 0.1113893911242485 Validation loss 0.11016155034303665\n",
            "Epoch 16700/30000  training loss: 0.1113285943865776 Validation loss 0.11010351777076721\n",
            "Epoch 16710/30000  training loss: 0.11126788705587387 Validation loss 0.11004557460546494\n",
            "Epoch 16720/30000  training loss: 0.11120729148387909 Validation loss 0.10998772084712982\n",
            "Epoch 16730/30000  training loss: 0.11114681512117386 Validation loss 0.10993002355098724\n",
            "Epoch 16740/30000  training loss: 0.11108659207820892 Validation loss 0.10987251996994019\n",
            "Epoch 16750/30000  training loss: 0.11102643609046936 Validation loss 0.10981512814760208\n",
            "Epoch 16760/30000  training loss: 0.11096641421318054 Validation loss 0.10975781083106995\n",
            "Epoch 16770/30000  training loss: 0.11090650409460068 Validation loss 0.10970066487789154\n",
            "Epoch 16780/30000  training loss: 0.11084670573472977 Validation loss 0.1096436008810997\n",
            "Epoch 16790/30000  training loss: 0.11078700423240662 Validation loss 0.10958661884069443\n",
            "Epoch 16800/30000  training loss: 0.11072740703821182 Validation loss 0.1095297560095787\n",
            "Epoch 16810/30000  training loss: 0.11066795885562897 Validation loss 0.10947304219007492\n",
            "Epoch 16820/30000  training loss: 0.11060874164104462 Validation loss 0.10941652208566666\n",
            "Epoch 16830/30000  training loss: 0.11054958403110504 Validation loss 0.10936009138822556\n",
            "Epoch 16840/30000  training loss: 0.11049056053161621 Validation loss 0.109303779900074\n",
            "Epoch 16850/30000  training loss: 0.11043165624141693 Validation loss 0.10924758017063141\n",
            "Epoch 16860/30000  training loss: 0.11037284880876541 Validation loss 0.10919146239757538\n",
            "Epoch 16870/30000  training loss: 0.11031414568424225 Validation loss 0.1091354638338089\n",
            "Epoch 16880/30000  training loss: 0.11025555431842804 Validation loss 0.10907957702875137\n",
            "Epoch 16890/30000  training loss: 0.11019710451364517 Validation loss 0.10902383923530579\n",
            "Epoch 16900/30000  training loss: 0.11013886332511902 Validation loss 0.10896826535463333\n",
            "Epoch 16910/30000  training loss: 0.11008068919181824 Validation loss 0.10891279578208923\n",
            "Epoch 16920/30000  training loss: 0.1100226566195488 Validation loss 0.10885746031999588\n",
            "Epoch 16930/30000  training loss: 0.10996474325656891 Validation loss 0.1088021919131279\n",
            "Epoch 16940/30000  training loss: 0.10990691184997559 Validation loss 0.10874704271554947\n",
            "Epoch 16950/30000  training loss: 0.10984918475151062 Validation loss 0.1086919978260994\n",
            "Epoch 16960/30000  training loss: 0.1097915917634964 Validation loss 0.10863707214593887\n",
            "Epoch 16970/30000  training loss: 0.10973411053419113 Validation loss 0.1085822731256485\n",
            "Epoch 16980/30000  training loss: 0.10967681556940079 Validation loss 0.10852764546871185\n",
            "Epoch 16990/30000  training loss: 0.10961965471506119 Validation loss 0.10847313702106476\n",
            "Epoch 17000/30000  training loss: 0.10956256836652756 Validation loss 0.10841871798038483\n",
            "Epoch 17010/30000  training loss: 0.10950563102960587 Validation loss 0.10836440324783325\n",
            "Epoch 17020/30000  training loss: 0.10944873839616776 Validation loss 0.10831016302108765\n",
            "Epoch 17030/30000  training loss: 0.10939200967550278 Validation loss 0.10825607925653458\n",
            "Epoch 17040/30000  training loss: 0.10933537036180496 Validation loss 0.10820208489894867\n",
            "Epoch 17050/30000  training loss: 0.10927886515855789 Validation loss 0.10814821720123291\n",
            "Epoch 17060/30000  training loss: 0.10922253131866455 Validation loss 0.10809451341629028\n",
            "Epoch 17070/30000  training loss: 0.10916633158922195 Validation loss 0.10804092884063721\n",
            "Epoch 17080/30000  training loss: 0.10911020636558533 Validation loss 0.10798745602369308\n",
            "Epoch 17090/30000  training loss: 0.10905418545007706 Validation loss 0.10793405026197433\n",
            "Epoch 17100/30000  training loss: 0.10899829119443893 Validation loss 0.10788077861070633\n",
            "Epoch 17110/30000  training loss: 0.10894247889518738 Validation loss 0.1078275814652443\n",
            "Epoch 17120/30000  training loss: 0.10888679325580597 Validation loss 0.10777449607849121\n",
            "Epoch 17130/30000  training loss: 0.10883121192455292 Validation loss 0.10772155225276947\n",
            "Epoch 17140/30000  training loss: 0.10877584666013718 Validation loss 0.10766877979040146\n",
            "Epoch 17150/30000  training loss: 0.10872058570384979 Validation loss 0.1076161190867424\n",
            "Epoch 17160/30000  training loss: 0.10866539925336838 Validation loss 0.10756352543830872\n",
            "Epoch 17170/30000  training loss: 0.10861032456159592 Validation loss 0.10751103609800339\n",
            "Epoch 17180/30000  training loss: 0.10855536162853241 Validation loss 0.10745866596698761\n",
            "Epoch 17190/30000  training loss: 0.10850050300359726 Validation loss 0.10740639269351959\n",
            "Epoch 17200/30000  training loss: 0.10844571143388748 Validation loss 0.10735418647527695\n",
            "Epoch 17210/30000  training loss: 0.10839112102985382 Validation loss 0.10730217397212982\n",
            "Epoch 17220/30000  training loss: 0.1083366721868515 Validation loss 0.10725029557943344\n",
            "Epoch 17230/30000  training loss: 0.10828231275081635 Validation loss 0.10719852894544601\n",
            "Epoch 17240/30000  training loss: 0.10822805017232895 Validation loss 0.10714682936668396\n",
            "Epoch 17250/30000  training loss: 0.10817389190196991 Validation loss 0.10709524154663086\n",
            "Epoch 17260/30000  training loss: 0.10811985284090042 Validation loss 0.10704375803470612\n",
            "Epoch 17270/30000  training loss: 0.1080659031867981 Validation loss 0.10699234902858734\n",
            "Epoch 17280/30000  training loss: 0.10801203548908234 Validation loss 0.10694104433059692\n",
            "Epoch 17290/30000  training loss: 0.1079583540558815 Validation loss 0.10688994079828262\n",
            "Epoch 17300/30000  training loss: 0.1079048216342926 Validation loss 0.10683894902467728\n",
            "Epoch 17310/30000  training loss: 0.10785138607025146 Validation loss 0.1067880317568779\n",
            "Epoch 17320/30000  training loss: 0.1077980175614357 Validation loss 0.10673722624778748\n",
            "Epoch 17330/30000  training loss: 0.10774479061365128 Validation loss 0.10668652504682541\n",
            "Epoch 17340/30000  training loss: 0.10769163817167282 Validation loss 0.10663590580224991\n",
            "Epoch 17350/30000  training loss: 0.10763858258724213 Validation loss 0.10658537596464157\n",
            "Epoch 17360/30000  training loss: 0.1075856164097786 Validation loss 0.10653495043516159\n",
            "Epoch 17370/30000  training loss: 0.10753286629915237 Validation loss 0.10648472607135773\n",
            "Epoch 17380/30000  training loss: 0.10748022794723511 Validation loss 0.10643459856510162\n",
            "Epoch 17390/30000  training loss: 0.10742765665054321 Validation loss 0.10638453811407089\n",
            "Epoch 17400/30000  training loss: 0.10737522691488266 Validation loss 0.1063346192240715\n",
            "Epoch 17410/30000  training loss: 0.10732285678386688 Validation loss 0.10628476738929749\n",
            "Epoch 17420/30000  training loss: 0.10727060586214066 Validation loss 0.10623500496149063\n",
            "Epoch 17430/30000  training loss: 0.1072184294462204 Validation loss 0.10618533939123154\n",
            "Epoch 17440/30000  training loss: 0.10716638714075089 Validation loss 0.1061357855796814\n",
            "Epoch 17450/30000  training loss: 0.10711448639631271 Validation loss 0.10608639568090439\n",
            "Epoch 17460/30000  training loss: 0.10706272721290588 Validation loss 0.10603710263967514\n",
            "Epoch 17470/30000  training loss: 0.10701105743646622 Validation loss 0.10598792135715485\n",
            "Epoch 17480/30000  training loss: 0.1069594994187355 Validation loss 0.10593883693218231\n",
            "Epoch 17490/30000  training loss: 0.10690802335739136 Validation loss 0.10588984191417694\n",
            "Epoch 17500/30000  training loss: 0.10685660690069199 Validation loss 0.10584091395139694\n",
            "Epoch 17510/30000  training loss: 0.10680533200502396 Validation loss 0.1057921051979065\n",
            "Epoch 17520/30000  training loss: 0.10675413906574249 Validation loss 0.10574338585138321\n",
            "Epoch 17530/30000  training loss: 0.10670312494039536 Validation loss 0.10569483786821365\n",
            "Epoch 17540/30000  training loss: 0.10665219277143478 Validation loss 0.10564637184143066\n",
            "Epoch 17550/30000  training loss: 0.10660140961408615 Validation loss 0.10559803992509842\n",
            "Epoch 17560/30000  training loss: 0.10655070096254349 Validation loss 0.10554978251457214\n",
            "Epoch 17570/30000  training loss: 0.10650008171796799 Validation loss 0.10550159215927124\n",
            "Epoch 17580/30000  training loss: 0.10644953697919846 Validation loss 0.1054535061120987\n",
            "Epoch 17590/30000  training loss: 0.10639911144971848 Validation loss 0.1054055318236351\n",
            "Epoch 17600/30000  training loss: 0.10634878277778625 Validation loss 0.10535762459039688\n",
            "Epoch 17610/30000  training loss: 0.10629861801862717 Validation loss 0.10530990362167358\n",
            "Epoch 17620/30000  training loss: 0.10624856501817703 Validation loss 0.10526228696107864\n",
            "Epoch 17630/30000  training loss: 0.10619861632585526 Validation loss 0.10521476715803146\n",
            "Epoch 17640/30000  training loss: 0.10614874958992004 Validation loss 0.10516731441020966\n",
            "Epoch 17650/30000  training loss: 0.10609894245862961 Validation loss 0.10511993616819382\n",
            "Epoch 17660/30000  training loss: 0.10604928433895111 Validation loss 0.10507267713546753\n",
            "Epoch 17670/30000  training loss: 0.10599969327449799 Validation loss 0.10502550005912781\n",
            "Epoch 17680/30000  training loss: 0.10595018416643143 Validation loss 0.10497841984033585\n",
            "Epoch 17690/30000  training loss: 0.10590086877346039 Validation loss 0.10493149608373642\n",
            "Epoch 17700/30000  training loss: 0.1058516800403595 Validation loss 0.10488469898700714\n",
            "Epoch 17710/30000  training loss: 0.10580254346132278 Validation loss 0.10483797639608383\n",
            "Epoch 17720/30000  training loss: 0.10575349628925323 Validation loss 0.10479132831096649\n",
            "Epoch 17730/30000  training loss: 0.10570453107357025 Validation loss 0.10474475473165512\n",
            "Epoch 17740/30000  training loss: 0.10565570741891861 Validation loss 0.1046983152627945\n",
            "Epoch 17750/30000  training loss: 0.10560692846775055 Validation loss 0.10465192794799805\n",
            "Epoch 17760/30000  training loss: 0.10555825382471085 Validation loss 0.10460563749074936\n",
            "Epoch 17770/30000  training loss: 0.10550976544618607 Validation loss 0.104559525847435\n",
            "Epoch 17780/30000  training loss: 0.10546137392520905 Validation loss 0.1045135036110878\n",
            "Epoch 17790/30000  training loss: 0.10541307181119919 Validation loss 0.10446757078170776\n",
            "Epoch 17800/30000  training loss: 0.1053648293018341 Validation loss 0.1044216901063919\n",
            "Epoch 17810/30000  training loss: 0.10531670600175858 Validation loss 0.1043759286403656\n",
            "Epoch 17820/30000  training loss: 0.1052686870098114 Validation loss 0.10433024913072586\n",
            "Epoch 17830/30000  training loss: 0.1052207201719284 Validation loss 0.10428463667631149\n",
            "Epoch 17840/30000  training loss: 0.10517282783985138 Validation loss 0.10423911362886429\n",
            "Epoch 17850/30000  training loss: 0.10512516647577286 Validation loss 0.1041937917470932\n",
            "Epoch 17860/30000  training loss: 0.1050775796175003 Validation loss 0.10414854437112808\n",
            "Epoch 17870/30000  training loss: 0.10503009706735611 Validation loss 0.10410339385271072\n",
            "Epoch 17880/30000  training loss: 0.1049826368689537 Validation loss 0.10405827313661575\n",
            "Epoch 17890/30000  training loss: 0.10493531823158264 Validation loss 0.10401330888271332\n",
            "Epoch 17900/30000  training loss: 0.10488807410001755 Validation loss 0.10396838188171387\n",
            "Epoch 17910/30000  training loss: 0.10484090447425842 Validation loss 0.10392352938652039\n",
            "Epoch 17920/30000  training loss: 0.10479384660720825 Validation loss 0.10387879610061646\n",
            "Epoch 17930/30000  training loss: 0.10474696010351181 Validation loss 0.10383424907922745\n",
            "Epoch 17940/30000  training loss: 0.10470017790794373 Validation loss 0.10378974676132202\n",
            "Epoch 17950/30000  training loss: 0.10465344041585922 Validation loss 0.10374533385038376\n",
            "Epoch 17960/30000  training loss: 0.10460680723190308 Validation loss 0.10370101034641266\n",
            "Epoch 17970/30000  training loss: 0.10456027090549469 Validation loss 0.10365677624940872\n",
            "Epoch 17980/30000  training loss: 0.10451379418373108 Validation loss 0.10361260175704956\n",
            "Epoch 17990/30000  training loss: 0.10446742177009583 Validation loss 0.10356850177049637\n",
            "Epoch 18000/30000  training loss: 0.10442112386226654 Validation loss 0.10352452844381332\n",
            "Epoch 18010/30000  training loss: 0.10437504202127457 Validation loss 0.10348071157932281\n",
            "Epoch 18020/30000  training loss: 0.10432901233434677 Validation loss 0.10343696922063828\n",
            "Epoch 18030/30000  training loss: 0.10428304970264435 Validation loss 0.10339328646659851\n",
            "Epoch 18040/30000  training loss: 0.10423719882965088 Validation loss 0.1033497154712677\n",
            "Epoch 18050/30000  training loss: 0.10419141501188278 Validation loss 0.10330621898174286\n",
            "Epoch 18060/30000  training loss: 0.10414571315050125 Validation loss 0.10326278954744339\n",
            "Epoch 18070/30000  training loss: 0.10410008579492569 Validation loss 0.1032194048166275\n",
            "Epoch 18080/30000  training loss: 0.10405457764863968 Validation loss 0.10317618399858475\n",
            "Epoch 18090/30000  training loss: 0.1040092259645462 Validation loss 0.10313309729099274\n",
            "Epoch 18100/30000  training loss: 0.1039639487862587 Validation loss 0.1030900850892067\n",
            "Epoch 18110/30000  training loss: 0.10391876846551895 Validation loss 0.10304713249206543\n",
            "Epoch 18120/30000  training loss: 0.10387367755174637 Validation loss 0.10300427675247192\n",
            "Epoch 18130/30000  training loss: 0.10382864624261856 Validation loss 0.10296150296926498\n",
            "Epoch 18140/30000  training loss: 0.10378370434045792 Validation loss 0.10291879624128342\n",
            "Epoch 18150/30000  training loss: 0.10373881459236145 Validation loss 0.10287614911794662\n",
            "Epoch 18160/30000  training loss: 0.10369407385587692 Validation loss 0.10283365100622177\n",
            "Epoch 18170/30000  training loss: 0.10364947468042374 Validation loss 0.10279127955436707\n",
            "Epoch 18180/30000  training loss: 0.10360494256019592 Validation loss 0.10274896025657654\n",
            "Epoch 18190/30000  training loss: 0.10356048494577408 Validation loss 0.10270673036575317\n",
            "Epoch 18200/30000  training loss: 0.1035161241889 Validation loss 0.10266461223363876\n",
            "Epoch 18210/30000  training loss: 0.10347184538841248 Validation loss 0.10262253135442734\n",
            "Epoch 18220/30000  training loss: 0.10342763364315033 Validation loss 0.10258052498102188\n",
            "Epoch 18230/30000  training loss: 0.10338349640369415 Validation loss 0.10253860801458359\n",
            "Epoch 18240/30000  training loss: 0.10333948582410812 Validation loss 0.10249679535627365\n",
            "Epoch 18250/30000  training loss: 0.10329558700323105 Validation loss 0.10245511680841446\n",
            "Epoch 18260/30000  training loss: 0.10325179994106293 Validation loss 0.10241350531578064\n",
            "Epoch 18270/30000  training loss: 0.10320808738470078 Validation loss 0.10237199813127518\n",
            "Epoch 18280/30000  training loss: 0.10316447168588638 Validation loss 0.10233055800199509\n",
            "Epoch 18290/30000  training loss: 0.10312089323997498 Validation loss 0.10228917747735977\n",
            "Epoch 18300/30000  training loss: 0.10307739675045013 Validation loss 0.10224787145853043\n",
            "Epoch 18310/30000  training loss: 0.10303399711847305 Validation loss 0.10220664739608765\n",
            "Epoch 18320/30000  training loss: 0.10299067944288254 Validation loss 0.10216549783945084\n",
            "Epoch 18330/30000  training loss: 0.10294751077890396 Validation loss 0.10212451964616776\n",
            "Epoch 18340/30000  training loss: 0.10290444642305374 Validation loss 0.10208358615636826\n",
            "Epoch 18350/30000  training loss: 0.1028614416718483 Validation loss 0.1020427718758583\n",
            "Epoch 18360/30000  training loss: 0.10281852632761002 Validation loss 0.10200201719999313\n",
            "Epoch 18370/30000  training loss: 0.10277566313743591 Validation loss 0.10196130722761154\n",
            "Epoch 18380/30000  training loss: 0.10273288935422897 Validation loss 0.10192067921161652\n",
            "Epoch 18390/30000  training loss: 0.102690190076828 Validation loss 0.10188014060258865\n",
            "Epoch 18400/30000  training loss: 0.1026475727558136 Validation loss 0.10183968394994736\n",
            "Epoch 18410/30000  training loss: 0.10260510444641113 Validation loss 0.10179935395717621\n",
            "Epoch 18420/30000  training loss: 0.10256273299455643 Validation loss 0.10175912827253342\n",
            "Epoch 18430/30000  training loss: 0.10252044349908829 Validation loss 0.101718969643116\n",
            "Epoch 18440/30000  training loss: 0.10247822105884552 Validation loss 0.10167887806892395\n",
            "Epoch 18450/30000  training loss: 0.10243605822324753 Validation loss 0.10163883119821548\n",
            "Epoch 18460/30000  training loss: 0.1023939698934555 Validation loss 0.10159887373447418\n",
            "Epoch 18470/30000  training loss: 0.10235199332237244 Validation loss 0.10155899822711945\n",
            "Epoch 18480/30000  training loss: 0.10231003910303116 Validation loss 0.10151921212673187\n",
            "Epoch 18490/30000  training loss: 0.1022682711482048 Validation loss 0.10147953033447266\n",
            "Epoch 18500/30000  training loss: 0.10222656279802322 Validation loss 0.1014399603009224\n",
            "Epoch 18510/30000  training loss: 0.1021849736571312 Validation loss 0.1014004573225975\n",
            "Epoch 18520/30000  training loss: 0.10214345157146454 Validation loss 0.10136103630065918\n",
            "Epoch 18530/30000  training loss: 0.10210196673870087 Validation loss 0.10132164508104324\n",
            "Epoch 18540/30000  training loss: 0.10206056386232376 Validation loss 0.10128234326839447\n",
            "Epoch 18550/30000  training loss: 0.10201924294233322 Validation loss 0.10124312341213226\n",
            "Epoch 18560/30000  training loss: 0.10197799652814865 Validation loss 0.10120396316051483\n",
            "Epoch 18570/30000  training loss: 0.10193685442209244 Validation loss 0.10116492211818695\n",
            "Epoch 18580/30000  training loss: 0.10189585387706757 Validation loss 0.10112599283456802\n",
            "Epoch 18590/30000  training loss: 0.10185493528842926 Validation loss 0.10108713805675507\n",
            "Epoch 18600/30000  training loss: 0.10181405395269394 Validation loss 0.10104834288358688\n",
            "Epoch 18610/30000  training loss: 0.10177326202392578 Validation loss 0.10100961476564407\n",
            "Epoch 18620/30000  training loss: 0.10173250734806061 Validation loss 0.10097094625234604\n",
            "Epoch 18630/30000  training loss: 0.10169187188148499 Validation loss 0.10093236714601517\n",
            "Epoch 18640/30000  training loss: 0.10165128111839294 Validation loss 0.10089384019374847\n",
            "Epoch 18650/30000  training loss: 0.10161079466342926 Validation loss 0.10085543245077133\n",
            "Epoch 18660/30000  training loss: 0.10157044231891632 Validation loss 0.10081712156534195\n",
            "Epoch 18670/30000  training loss: 0.10153017193078995 Validation loss 0.10077891498804092\n",
            "Epoch 18680/30000  training loss: 0.10148998349905014 Validation loss 0.10074073821306229\n",
            "Epoch 18690/30000  training loss: 0.10144981741905212 Validation loss 0.10070263594388962\n",
            "Epoch 18700/30000  training loss: 0.10140974074602127 Validation loss 0.10066458582878113\n",
            "Epoch 18710/30000  training loss: 0.10136976838111877 Validation loss 0.10062665492296219\n",
            "Epoch 18720/30000  training loss: 0.10132981091737747 Validation loss 0.10058874636888504\n",
            "Epoch 18730/30000  training loss: 0.10128995031118393 Validation loss 0.10055090487003326\n",
            "Epoch 18740/30000  training loss: 0.10125023126602173 Validation loss 0.10051321983337402\n",
            "Epoch 18750/30000  training loss: 0.10121060907840729 Validation loss 0.10047563165426254\n",
            "Epoch 18760/30000  training loss: 0.10117105394601822 Validation loss 0.10043808072805405\n",
            "Epoch 18770/30000  training loss: 0.10113155096769333 Validation loss 0.10040058195590973\n",
            "Epoch 18780/30000  training loss: 0.10109212249517441 Validation loss 0.10036318004131317\n",
            "Epoch 18790/30000  training loss: 0.10105276107788086 Validation loss 0.1003258228302002\n",
            "Epoch 18800/30000  training loss: 0.10101347416639328 Validation loss 0.1002885177731514\n",
            "Epoch 18810/30000  training loss: 0.10097420960664749 Validation loss 0.10025127977132797\n",
            "Epoch 18820/30000  training loss: 0.10093514621257782 Validation loss 0.10021421313285828\n",
            "Epoch 18830/30000  training loss: 0.10089614987373352 Validation loss 0.10017721354961395\n",
            "Epoch 18840/30000  training loss: 0.10085722804069519 Validation loss 0.10014026612043381\n",
            "Epoch 18850/30000  training loss: 0.10081833600997925 Validation loss 0.10010336339473724\n",
            "Epoch 18860/30000  training loss: 0.10077954083681107 Validation loss 0.10006655007600784\n",
            "Epoch 18870/30000  training loss: 0.10074079781770706 Validation loss 0.10002979636192322\n",
            "Epoch 18880/30000  training loss: 0.10070215165615082 Validation loss 0.09999311715364456\n",
            "Epoch 18890/30000  training loss: 0.10066352784633636 Validation loss 0.0999564602971077\n",
            "Epoch 18900/30000  training loss: 0.10062501579523087 Validation loss 0.09991993755102158\n",
            "Epoch 18910/30000  training loss: 0.1005866527557373 Validation loss 0.0998835489153862\n",
            "Epoch 18920/30000  training loss: 0.10054835677146912 Validation loss 0.09984719008207321\n",
            "Epoch 18930/30000  training loss: 0.10051009058952332 Validation loss 0.099810890853405\n",
            "Epoch 18940/30000  training loss: 0.10047189891338348 Validation loss 0.09977465122938156\n",
            "Epoch 18950/30000  training loss: 0.10043377429246902 Validation loss 0.09973850101232529\n",
            "Epoch 18960/30000  training loss: 0.10039571672677994 Validation loss 0.099702388048172\n",
            "Epoch 18970/30000  training loss: 0.10035771131515503 Validation loss 0.09966631978750229\n",
            "Epoch 18980/30000  training loss: 0.1003197729587555 Validation loss 0.09963034093379974\n",
            "Epoch 18990/30000  training loss: 0.10028202831745148 Validation loss 0.09959452599287033\n",
            "Epoch 19000/30000  training loss: 0.10024431347846985 Validation loss 0.0995587483048439\n",
            "Epoch 19010/30000  training loss: 0.10020666569471359 Validation loss 0.09952302277088165\n",
            "Epoch 19020/30000  training loss: 0.10016905516386032 Validation loss 0.09948734939098358\n",
            "Epoch 19030/30000  training loss: 0.1001315787434578 Validation loss 0.09945178031921387\n",
            "Epoch 19040/30000  training loss: 0.10009410232305527 Validation loss 0.09941622614860535\n",
            "Epoch 19050/30000  training loss: 0.1000567078590393 Validation loss 0.0993807390332222\n",
            "Epoch 19060/30000  training loss: 0.10001934319734573 Validation loss 0.09934529662132263\n",
            "Epoch 19070/30000  training loss: 0.09998214989900589 Validation loss 0.0993100255727768\n",
            "Epoch 19080/30000  training loss: 0.09994504600763321 Validation loss 0.09927482157945633\n",
            "Epoch 19090/30000  training loss: 0.09990797936916351 Validation loss 0.09923966228961945\n",
            "Epoch 19100/30000  training loss: 0.09987098723649979 Validation loss 0.09920456260442734\n",
            "Epoch 19110/30000  training loss: 0.09983404725790024 Validation loss 0.0991695299744606\n",
            "Epoch 19120/30000  training loss: 0.09979718923568726 Validation loss 0.09913455694913864\n",
            "Epoch 19130/30000  training loss: 0.09976036846637726 Validation loss 0.09909964352846146\n",
            "Epoch 19140/30000  training loss: 0.09972360730171204 Validation loss 0.09906475245952606\n",
            "Epoch 19150/30000  training loss: 0.09968692064285278 Validation loss 0.09902997314929962\n",
            "Epoch 19160/30000  training loss: 0.09965040534734726 Validation loss 0.09899532794952393\n",
            "Epoch 19170/30000  training loss: 0.09961392730474472 Validation loss 0.09896072745323181\n",
            "Epoch 19180/30000  training loss: 0.09957749396562576 Validation loss 0.09892616420984268\n",
            "Epoch 19190/30000  training loss: 0.09954114258289337 Validation loss 0.09889166802167892\n",
            "Epoch 19200/30000  training loss: 0.09950485825538635 Validation loss 0.09885725378990173\n",
            "Epoch 19210/30000  training loss: 0.09946861118078232 Validation loss 0.09882288426160812\n",
            "Epoch 19220/30000  training loss: 0.09943241626024246 Validation loss 0.0987885445356369\n",
            "Epoch 19230/30000  training loss: 0.09939629584550858 Validation loss 0.09875427931547165\n",
            "Epoch 19240/30000  training loss: 0.09936030954122543 Validation loss 0.09872014820575714\n",
            "Epoch 19250/30000  training loss: 0.09932440519332886 Validation loss 0.0986860916018486\n",
            "Epoch 19260/30000  training loss: 0.09928854554891586 Validation loss 0.09865207225084305\n",
            "Epoch 19270/30000  training loss: 0.09925274550914764 Validation loss 0.09861811250448227\n",
            "Epoch 19280/30000  training loss: 0.0992170199751854 Validation loss 0.09858422726392746\n",
            "Epoch 19290/30000  training loss: 0.09918135404586792 Validation loss 0.09855038672685623\n",
            "Epoch 19300/30000  training loss: 0.09914571791887283 Validation loss 0.09851659089326859\n",
            "Epoch 19310/30000  training loss: 0.09911014884710312 Validation loss 0.09848284721374512\n",
            "Epoch 19320/30000  training loss: 0.09907466173171997 Validation loss 0.09844918549060822\n",
            "Epoch 19330/30000  training loss: 0.09903931617736816 Validation loss 0.09841565787792206\n",
            "Epoch 19340/30000  training loss: 0.09900400042533875 Validation loss 0.09838218986988068\n",
            "Epoch 19350/30000  training loss: 0.0989687591791153 Validation loss 0.09834874421358109\n",
            "Epoch 19360/30000  training loss: 0.09893357753753662 Validation loss 0.09831535816192627\n",
            "Epoch 19370/30000  training loss: 0.09889846295118332 Validation loss 0.09828207641839981\n",
            "Epoch 19380/30000  training loss: 0.098863385617733 Validation loss 0.09824880212545395\n",
            "Epoch 19390/30000  training loss: 0.09882836043834686 Validation loss 0.09821557998657227\n",
            "Epoch 19400/30000  training loss: 0.0987933874130249 Validation loss 0.09818241000175476\n",
            "Epoch 19410/30000  training loss: 0.09875853359699249 Validation loss 0.09814934432506561\n",
            "Epoch 19420/30000  training loss: 0.09872376918792725 Validation loss 0.09811637550592422\n",
            "Epoch 19430/30000  training loss: 0.09868907183408737 Validation loss 0.09808345884084702\n",
            "Epoch 19440/30000  training loss: 0.09865441918373108 Validation loss 0.09805060178041458\n",
            "Epoch 19450/30000  training loss: 0.09861982613801956 Validation loss 0.09801781177520752\n",
            "Epoch 19460/30000  training loss: 0.09858530014753342 Validation loss 0.09798504412174225\n",
            "Epoch 19470/30000  training loss: 0.09855082631111145 Validation loss 0.09795233607292175\n",
            "Epoch 19480/30000  training loss: 0.09851637482643127 Validation loss 0.09791965782642365\n",
            "Epoch 19490/30000  training loss: 0.09848202019929886 Validation loss 0.0978870764374733\n",
            "Epoch 19500/30000  training loss: 0.09844774752855301 Validation loss 0.09785458445549011\n",
            "Epoch 19510/30000  training loss: 0.09841358661651611 Validation loss 0.0978221744298935\n",
            "Epoch 19520/30000  training loss: 0.0983794629573822 Validation loss 0.09778978675603867\n",
            "Epoch 19530/30000  training loss: 0.09834539145231247 Validation loss 0.097757488489151\n",
            "Epoch 19540/30000  training loss: 0.0983114019036293 Validation loss 0.09772523492574692\n",
            "Epoch 19550/30000  training loss: 0.09827743470668793 Validation loss 0.09769302606582642\n",
            "Epoch 19560/30000  training loss: 0.09824351966381073 Validation loss 0.09766086935997009\n",
            "Epoch 19570/30000  training loss: 0.09820964932441711 Validation loss 0.09762874245643616\n",
            "Epoch 19580/30000  training loss: 0.09817587584257126 Validation loss 0.09759671241044998\n",
            "Epoch 19590/30000  training loss: 0.09814218431711197 Validation loss 0.09756476432085037\n",
            "Epoch 19600/30000  training loss: 0.09810860455036163 Validation loss 0.09753289073705673\n",
            "Epoch 19610/30000  training loss: 0.0980750322341919 Validation loss 0.09750105440616608\n",
            "Epoch 19620/30000  training loss: 0.09804155677556992 Validation loss 0.09746930003166199\n",
            "Epoch 19630/30000  training loss: 0.09800810366868973 Validation loss 0.09743759036064148\n",
            "Epoch 19640/30000  training loss: 0.09797471761703491 Validation loss 0.09740591794252396\n",
            "Epoch 19650/30000  training loss: 0.09794136881828308 Validation loss 0.09737427532672882\n",
            "Epoch 19660/30000  training loss: 0.09790806472301483 Validation loss 0.09734269231557846\n",
            "Epoch 19670/30000  training loss: 0.09787483513355255 Validation loss 0.09731117635965347\n",
            "Epoch 19680/30000  training loss: 0.09784174710512161 Validation loss 0.09727977961301804\n",
            "Epoch 19690/30000  training loss: 0.09780868887901306 Validation loss 0.09724843502044678\n",
            "Epoch 19700/30000  training loss: 0.09777569025754929 Validation loss 0.0972171351313591\n",
            "Epoch 19710/30000  training loss: 0.09774275124073029 Validation loss 0.0971858948469162\n",
            "Epoch 19720/30000  training loss: 0.09770987182855606 Validation loss 0.09715469181537628\n",
            "Epoch 19730/30000  training loss: 0.09767702966928482 Validation loss 0.09712354838848114\n",
            "Epoch 19740/30000  training loss: 0.09764420986175537 Validation loss 0.09709242731332779\n",
            "Epoch 19750/30000  training loss: 0.09761149436235428 Validation loss 0.09706137329339981\n",
            "Epoch 19760/30000  training loss: 0.09757879376411438 Validation loss 0.09703037142753601\n",
            "Epoch 19770/30000  training loss: 0.09754623472690582 Validation loss 0.09699950367212296\n",
            "Epoch 19780/30000  training loss: 0.09751372039318085 Validation loss 0.0969686508178711\n",
            "Epoch 19790/30000  training loss: 0.09748126566410065 Validation loss 0.0969378724694252\n",
            "Epoch 19800/30000  training loss: 0.09744887799024582 Validation loss 0.09690713882446289\n",
            "Epoch 19810/30000  training loss: 0.09741652011871338 Validation loss 0.09687644988298416\n",
            "Epoch 19820/30000  training loss: 0.09738422185182571 Validation loss 0.09684580564498901\n",
            "Epoch 19830/30000  training loss: 0.09735193848609924 Validation loss 0.09681519120931625\n",
            "Epoch 19840/30000  training loss: 0.09731975197792053 Validation loss 0.09678464382886887\n",
            "Epoch 19850/30000  training loss: 0.09728758037090302 Validation loss 0.09675414115190506\n",
            "Epoch 19860/30000  training loss: 0.09725556522607803 Validation loss 0.0967237576842308\n",
            "Epoch 19870/30000  training loss: 0.09722357988357544 Validation loss 0.09669343382120132\n",
            "Epoch 19880/30000  training loss: 0.09719164669513702 Validation loss 0.09666314721107483\n",
            "Epoch 19890/30000  training loss: 0.09715979546308517 Validation loss 0.09663291275501251\n",
            "Epoch 19900/30000  training loss: 0.09712796658277512 Validation loss 0.09660273045301437\n",
            "Epoch 19910/30000  training loss: 0.09709617495536804 Validation loss 0.09657255560159683\n",
            "Epoch 19920/30000  training loss: 0.09706443548202515 Validation loss 0.09654245525598526\n",
            "Epoch 19930/30000  training loss: 0.09703273326158524 Validation loss 0.09651239216327667\n",
            "Epoch 19940/30000  training loss: 0.09700111299753189 Validation loss 0.09648238122463226\n",
            "Epoch 19950/30000  training loss: 0.09696958214044571 Validation loss 0.0964524894952774\n",
            "Epoch 19960/30000  training loss: 0.0969381332397461 Validation loss 0.09642263501882553\n",
            "Epoch 19970/30000  training loss: 0.09690670669078827 Validation loss 0.09639284759759903\n",
            "Epoch 19980/30000  training loss: 0.09687536209821701 Validation loss 0.09636309742927551\n",
            "Epoch 19990/30000  training loss: 0.09684403240680695 Validation loss 0.09633338451385498\n",
            "Epoch 20000/30000  training loss: 0.09681277722120285 Validation loss 0.09630370885133743\n",
            "Epoch 20010/30000  training loss: 0.09678153693675995 Validation loss 0.09627407044172287\n",
            "Epoch 20020/30000  training loss: 0.09675034880638123 Validation loss 0.09624450653791428\n",
            "Epoch 20030/30000  training loss: 0.09671923518180847 Validation loss 0.09621497243642807\n",
            "Epoch 20040/30000  training loss: 0.09668819606304169 Validation loss 0.09618553519248962\n",
            "Epoch 20050/30000  training loss: 0.09665723145008087 Validation loss 0.09615615755319595\n",
            "Epoch 20060/30000  training loss: 0.09662631154060364 Validation loss 0.09612683206796646\n",
            "Epoch 20070/30000  training loss: 0.09659547358751297 Validation loss 0.09609756618738174\n",
            "Epoch 20080/30000  training loss: 0.09656465798616409 Validation loss 0.09606831520795822\n",
            "Epoch 20090/30000  training loss: 0.0965338721871376 Validation loss 0.09603912383317947\n",
            "Epoch 20100/30000  training loss: 0.0965031310915947 Validation loss 0.09600996226072311\n",
            "Epoch 20110/30000  training loss: 0.09647245705127716 Validation loss 0.09598084539175034\n",
            "Epoch 20120/30000  training loss: 0.0964418277144432 Validation loss 0.09595178812742233\n",
            "Epoch 20130/30000  training loss: 0.09641125053167343 Validation loss 0.0959227904677391\n",
            "Epoch 20140/30000  training loss: 0.09638078510761261 Validation loss 0.09589386731386185\n",
            "Epoch 20150/30000  training loss: 0.09635034948587418 Validation loss 0.09586501121520996\n",
            "Epoch 20160/30000  training loss: 0.09631999582052231 Validation loss 0.09583619982004166\n",
            "Epoch 20170/30000  training loss: 0.09628965705633163 Validation loss 0.09580743312835693\n",
            "Epoch 20180/30000  training loss: 0.09625938534736633 Validation loss 0.09577867388725281\n",
            "Epoch 20190/30000  training loss: 0.09622912108898163 Validation loss 0.09574995934963226\n",
            "Epoch 20200/30000  training loss: 0.0961989238858223 Validation loss 0.0957213044166565\n",
            "Epoch 20210/30000  training loss: 0.09616877138614655 Validation loss 0.0956926941871643\n",
            "Epoch 20220/30000  training loss: 0.09613866358995438 Validation loss 0.0956641137599945\n",
            "Epoch 20230/30000  training loss: 0.09610868245363235 Validation loss 0.09563565999269485\n",
            "Epoch 20240/30000  training loss: 0.09607870131731033 Validation loss 0.09560723602771759\n",
            "Epoch 20250/30000  training loss: 0.09604881703853607 Validation loss 0.0955788791179657\n",
            "Epoch 20260/30000  training loss: 0.09601897746324539 Validation loss 0.0955505445599556\n",
            "Epoch 20270/30000  training loss: 0.0959891602396965 Validation loss 0.09552225470542908\n",
            "Epoch 20280/30000  training loss: 0.0959593877196312 Validation loss 0.09549399465322495\n",
            "Epoch 20290/30000  training loss: 0.09592963755130768 Validation loss 0.0954657718539238\n",
            "Epoch 20300/30000  training loss: 0.09589996933937073 Validation loss 0.09543760120868683\n",
            "Epoch 20310/30000  training loss: 0.09587031602859497 Validation loss 0.09540946781635284\n",
            "Epoch 20320/30000  training loss: 0.09584074467420578 Validation loss 0.09538140892982483\n",
            "Epoch 20330/30000  training loss: 0.09581127762794495 Validation loss 0.09535342454910278\n",
            "Epoch 20340/30000  training loss: 0.0957818254828453 Validation loss 0.09532549232244492\n",
            "Epoch 20350/30000  training loss: 0.09575244039297104 Validation loss 0.09529761224985123\n",
            "Epoch 20360/30000  training loss: 0.09572310745716095 Validation loss 0.09526974707841873\n",
            "Epoch 20370/30000  training loss: 0.09569378197193146 Validation loss 0.09524191915988922\n",
            "Epoch 20380/30000  training loss: 0.09566449373960495 Validation loss 0.0952141284942627\n",
            "Epoch 20390/30000  training loss: 0.09563527256250381 Validation loss 0.09518638998270035\n",
            "Epoch 20400/30000  training loss: 0.09560608863830566 Validation loss 0.09515868872404099\n",
            "Epoch 20410/30000  training loss: 0.0955769419670105 Validation loss 0.09513100981712341\n",
            "Epoch 20420/30000  training loss: 0.09554789215326309 Validation loss 0.09510345757007599\n",
            "Epoch 20430/30000  training loss: 0.09551891684532166 Validation loss 0.09507592767477036\n",
            "Epoch 20440/30000  training loss: 0.0954899936914444 Validation loss 0.09504847973585129\n",
            "Epoch 20450/30000  training loss: 0.09546107798814774 Validation loss 0.09502103924751282\n",
            "Epoch 20460/30000  training loss: 0.09543222934007645 Validation loss 0.09499363601207733\n",
            "Epoch 20470/30000  training loss: 0.09540339559316635 Validation loss 0.09496627748012543\n",
            "Epoch 20480/30000  training loss: 0.09537458419799805 Validation loss 0.09493894129991531\n",
            "Epoch 20490/30000  training loss: 0.0953458771109581 Validation loss 0.09491167217493057\n",
            "Epoch 20500/30000  training loss: 0.09531717002391815 Validation loss 0.09488441795110703\n",
            "Epoch 20510/30000  training loss: 0.09528852254152298 Validation loss 0.09485720843076706\n",
            "Epoch 20520/30000  training loss: 0.09525996446609497 Validation loss 0.09483011811971664\n",
            "Epoch 20530/30000  training loss: 0.09523147344589233 Validation loss 0.09480305761098862\n",
            "Epoch 20540/30000  training loss: 0.09520299732685089 Validation loss 0.09477602690458298\n",
            "Epoch 20550/30000  training loss: 0.09517458826303482 Validation loss 0.09474906325340271\n",
            "Epoch 20560/30000  training loss: 0.09514620900154114 Validation loss 0.09472210705280304\n",
            "Epoch 20570/30000  training loss: 0.09511783719062805 Validation loss 0.09469517320394516\n",
            "Epoch 20580/30000  training loss: 0.09508951753377914 Validation loss 0.09466829895973206\n",
            "Epoch 20590/30000  training loss: 0.09506126493215561 Validation loss 0.09464146196842194\n",
            "Epoch 20600/30000  training loss: 0.09503303468227386 Validation loss 0.09461463987827301\n",
            "Epoch 20610/30000  training loss: 0.09500487893819809 Validation loss 0.09458790719509125\n",
            "Epoch 20620/30000  training loss: 0.09497677534818649 Validation loss 0.09456124156713486\n",
            "Epoch 20630/30000  training loss: 0.09494876861572266 Validation loss 0.09453462809324265\n",
            "Epoch 20640/30000  training loss: 0.09492076933383942 Validation loss 0.09450804442167282\n",
            "Epoch 20650/30000  training loss: 0.09489282220602036 Validation loss 0.09448150545358658\n",
            "Epoch 20660/30000  training loss: 0.09486488997936249 Validation loss 0.09445496648550034\n",
            "Epoch 20670/30000  training loss: 0.09483698755502701 Validation loss 0.09442847222089767\n",
            "Epoch 20680/30000  training loss: 0.09480912983417511 Validation loss 0.09440204501152039\n",
            "Epoch 20690/30000  training loss: 0.09478133171796799 Validation loss 0.0943756178021431\n",
            "Epoch 20700/30000  training loss: 0.09475354850292206 Validation loss 0.09434923529624939\n",
            "Epoch 20710/30000  training loss: 0.09472586959600449 Validation loss 0.09432293474674225\n",
            "Epoch 20720/30000  training loss: 0.0946982353925705 Validation loss 0.09429670125246048\n",
            "Epoch 20730/30000  training loss: 0.09467066079378128 Validation loss 0.09427051991224289\n",
            "Epoch 20740/30000  training loss: 0.09464313089847565 Validation loss 0.09424436092376709\n",
            "Epoch 20750/30000  training loss: 0.0946156233549118 Validation loss 0.09421823173761368\n",
            "Epoch 20760/30000  training loss: 0.09458814561367035 Validation loss 0.09419213235378265\n",
            "Epoch 20770/30000  training loss: 0.09456068277359009 Validation loss 0.09416605532169342\n",
            "Epoch 20780/30000  training loss: 0.09453330188989639 Validation loss 0.09414002299308777\n",
            "Epoch 20790/30000  training loss: 0.09450594335794449 Validation loss 0.0941140279173851\n",
            "Epoch 20800/30000  training loss: 0.09447859227657318 Validation loss 0.09408807009458542\n",
            "Epoch 20810/30000  training loss: 0.09445135295391083 Validation loss 0.0940621942281723\n",
            "Epoch 20820/30000  training loss: 0.09442415833473206 Validation loss 0.09403635561466217\n",
            "Epoch 20830/30000  training loss: 0.09439703822135925 Validation loss 0.09401058405637741\n",
            "Epoch 20840/30000  training loss: 0.09436993300914764 Validation loss 0.09398483484983444\n",
            "Epoch 20850/30000  training loss: 0.09434287250041962 Validation loss 0.09395911544561386\n",
            "Epoch 20860/30000  training loss: 0.09431581944227219 Validation loss 0.09393342584371567\n",
            "Epoch 20870/30000  training loss: 0.09428880363702774 Validation loss 0.09390775859355927\n",
            "Epoch 20880/30000  training loss: 0.09426184743642807 Validation loss 0.09388214349746704\n",
            "Epoch 20890/30000  training loss: 0.09423492103815079 Validation loss 0.09385655075311661\n",
            "Epoch 20900/30000  training loss: 0.0942080169916153 Validation loss 0.09383098781108856\n",
            "Epoch 20910/30000  training loss: 0.09418120235204697 Validation loss 0.09380550682544708\n",
            "Epoch 20920/30000  training loss: 0.09415443986654282 Validation loss 0.09378007799386978\n",
            "Epoch 20930/30000  training loss: 0.09412773698568344 Validation loss 0.09375470131635666\n",
            "Epoch 20940/30000  training loss: 0.09410105645656586 Validation loss 0.09372936189174652\n",
            "Epoch 20950/30000  training loss: 0.09407441318035126 Validation loss 0.09370402991771698\n",
            "Epoch 20960/30000  training loss: 0.09404779970645905 Validation loss 0.09367874264717102\n",
            "Epoch 20970/30000  training loss: 0.09402120113372803 Validation loss 0.09365346282720566\n",
            "Epoch 20980/30000  training loss: 0.09399466961622238 Validation loss 0.09362824261188507\n",
            "Epoch 20990/30000  training loss: 0.09396816045045853 Validation loss 0.09360304474830627\n",
            "Epoch 21000/30000  training loss: 0.09394167363643646 Validation loss 0.09357786923646927\n",
            "Epoch 21010/30000  training loss: 0.09391526132822037 Validation loss 0.09355276823043823\n",
            "Epoch 21020/30000  training loss: 0.09388891607522964 Validation loss 0.09352772682905197\n",
            "Epoch 21030/30000  training loss: 0.0938626304268837 Validation loss 0.09350274503231049\n",
            "Epoch 21040/30000  training loss: 0.09383636713027954 Validation loss 0.0934777781367302\n",
            "Epoch 21050/30000  training loss: 0.09381013363599777 Validation loss 0.09345286339521408\n",
            "Epoch 21060/30000  training loss: 0.09378392994403839 Validation loss 0.09342794120311737\n",
            "Epoch 21070/30000  training loss: 0.09375777095556259 Validation loss 0.09340304136276245\n",
            "Epoch 21080/30000  training loss: 0.09373161196708679 Validation loss 0.0933782085776329\n",
            "Epoch 21090/30000  training loss: 0.09370552003383636 Validation loss 0.09335339814424515\n",
            "Epoch 21100/30000  training loss: 0.09367945790290833 Validation loss 0.09332861006259918\n",
            "Epoch 21110/30000  training loss: 0.09365341812372208 Validation loss 0.09330384433269501\n",
            "Epoch 21120/30000  training loss: 0.09362748265266418 Validation loss 0.09327919036149979\n",
            "Epoch 21130/30000  training loss: 0.09360157698392868 Validation loss 0.09325456619262695\n",
            "Epoch 21140/30000  training loss: 0.09357573091983795 Validation loss 0.0932299941778183\n",
            "Epoch 21150/30000  training loss: 0.09354989975690842 Validation loss 0.09320543706417084\n",
            "Epoch 21160/30000  training loss: 0.09352409839630127 Validation loss 0.09318090975284576\n",
            "Epoch 21170/30000  training loss: 0.0934983342885971 Validation loss 0.09315638244152069\n",
            "Epoch 21180/30000  training loss: 0.09347258508205414 Validation loss 0.0931318998336792\n",
            "Epoch 21190/30000  training loss: 0.09344689548015594 Validation loss 0.09310745447874069\n",
            "Epoch 21200/30000  training loss: 0.09342122077941895 Validation loss 0.09308306127786636\n",
            "Epoch 21210/30000  training loss: 0.09339558333158493 Validation loss 0.09305866062641144\n",
            "Epoch 21220/30000  training loss: 0.0933699905872345 Validation loss 0.09303432703018188\n",
            "Epoch 21230/30000  training loss: 0.09334447979927063 Validation loss 0.0930100679397583\n",
            "Epoch 21240/30000  training loss: 0.09331902116537094 Validation loss 0.0929858461022377\n",
            "Epoch 21250/30000  training loss: 0.09329359978437424 Validation loss 0.09296166151762009\n",
            "Epoch 21260/30000  training loss: 0.09326818585395813 Validation loss 0.09293750673532486\n",
            "Epoch 21270/30000  training loss: 0.09324280917644501 Validation loss 0.09291334450244904\n",
            "Epoch 21280/30000  training loss: 0.09321745485067368 Validation loss 0.0928892269730568\n",
            "Epoch 21290/30000  training loss: 0.09319211542606354 Validation loss 0.09286513924598694\n",
            "Epoch 21300/30000  training loss: 0.09316683560609818 Validation loss 0.09284108877182007\n",
            "Epoch 21310/30000  training loss: 0.0931415855884552 Validation loss 0.09281706064939499\n",
            "Epoch 21320/30000  training loss: 0.09311635047197342 Validation loss 0.0927930399775505\n",
            "Epoch 21330/30000  training loss: 0.0930912122130394 Validation loss 0.09276914596557617\n",
            "Epoch 21340/30000  training loss: 0.09306611865758896 Validation loss 0.09274525940418243\n",
            "Epoch 21350/30000  training loss: 0.0930410698056221 Validation loss 0.09272142499685287\n",
            "Epoch 21360/30000  training loss: 0.09301604330539703 Validation loss 0.0926976203918457\n",
            "Epoch 21370/30000  training loss: 0.09299104660749435 Validation loss 0.09267383813858032\n",
            "Epoch 21380/30000  training loss: 0.09296607226133347 Validation loss 0.09265004843473434\n",
            "Epoch 21390/30000  training loss: 0.09294110536575317 Validation loss 0.09262630343437195\n",
            "Epoch 21400/30000  training loss: 0.09291619807481766 Validation loss 0.09260260313749313\n",
            "Epoch 21410/30000  training loss: 0.09289134293794632 Validation loss 0.09257892519235611\n",
            "Epoch 21420/30000  training loss: 0.09286646544933319 Validation loss 0.09255526959896088\n",
            "Epoch 21430/30000  training loss: 0.09284164011478424 Validation loss 0.09253164380788803\n",
            "Epoch 21440/30000  training loss: 0.09281691908836365 Validation loss 0.09250810742378235\n",
            "Epoch 21450/30000  training loss: 0.09279221296310425 Validation loss 0.09248460829257965\n",
            "Epoch 21460/30000  training loss: 0.09276756644248962 Validation loss 0.09246115386486053\n",
            "Epoch 21470/30000  training loss: 0.09274294227361679 Validation loss 0.09243769943714142\n",
            "Epoch 21480/30000  training loss: 0.09271833300590515 Validation loss 0.09241426736116409\n",
            "Epoch 21490/30000  training loss: 0.0926937535405159 Validation loss 0.09239085763692856\n",
            "Epoch 21500/30000  training loss: 0.09266918152570724 Validation loss 0.09236747026443481\n",
            "Epoch 21510/30000  training loss: 0.09264467656612396 Validation loss 0.09234414249658585\n",
            "Epoch 21520/30000  training loss: 0.09262019395828247 Validation loss 0.09232082962989807\n",
            "Epoch 21530/30000  training loss: 0.09259572625160217 Validation loss 0.0922975242137909\n",
            "Epoch 21540/30000  training loss: 0.09257128834724426 Validation loss 0.0922742635011673\n",
            "Epoch 21550/30000  training loss: 0.09254693984985352 Validation loss 0.09225109219551086\n",
            "Epoch 21560/30000  training loss: 0.09252264350652695 Validation loss 0.09222795814275742\n",
            "Epoch 21570/30000  training loss: 0.09249837696552277 Validation loss 0.09220486134290695\n",
            "Epoch 21580/30000  training loss: 0.09247413277626038 Validation loss 0.09218175709247589\n",
            "Epoch 21590/30000  training loss: 0.09244991838932037 Validation loss 0.09215869009494781\n",
            "Epoch 21600/30000  training loss: 0.09242570400238037 Validation loss 0.09213563054800034\n",
            "Epoch 21610/30000  training loss: 0.09240151941776276 Validation loss 0.09211260825395584\n",
            "Epoch 21620/30000  training loss: 0.09237738698720932 Validation loss 0.09208963066339493\n",
            "Epoch 21630/30000  training loss: 0.09235328435897827 Validation loss 0.09206665307283401\n",
            "Epoch 21640/30000  training loss: 0.09232918918132782 Validation loss 0.09204371273517609\n",
            "Epoch 21650/30000  training loss: 0.09230512380599976 Validation loss 0.09202078729867935\n",
            "Epoch 21660/30000  training loss: 0.09228117018938065 Validation loss 0.09199795871973038\n",
            "Epoch 21670/30000  training loss: 0.09225722402334213 Validation loss 0.09197516739368439\n",
            "Epoch 21680/30000  training loss: 0.09223335236310959 Validation loss 0.09195241332054138\n",
            "Epoch 21690/30000  training loss: 0.09220946580171585 Validation loss 0.09192966669797897\n",
            "Epoch 21700/30000  training loss: 0.0921856090426445 Validation loss 0.09190692752599716\n",
            "Epoch 21710/30000  training loss: 0.09216177463531494 Validation loss 0.09188423305749893\n",
            "Epoch 21720/30000  training loss: 0.09213797003030777 Validation loss 0.0918615385890007\n",
            "Epoch 21730/30000  training loss: 0.09211419522762299 Validation loss 0.09183888882398605\n",
            "Epoch 21740/30000  training loss: 0.09209045767784119 Validation loss 0.0918162614107132\n",
            "Epoch 21750/30000  training loss: 0.09206672757863998 Validation loss 0.09179367870092392\n",
            "Epoch 21760/30000  training loss: 0.09204303473234177 Validation loss 0.09177107363939285\n",
            "Epoch 21770/30000  training loss: 0.09201941639184952 Validation loss 0.09174857288599014\n",
            "Epoch 21780/30000  training loss: 0.09199583530426025 Validation loss 0.09172610938549042\n",
            "Epoch 21790/30000  training loss: 0.09197231382131577 Validation loss 0.09170368313789368\n",
            "Epoch 21800/30000  training loss: 0.09194880723953247 Validation loss 0.09168127179145813\n",
            "Epoch 21810/30000  training loss: 0.09192531555891037 Validation loss 0.09165887534618378\n",
            "Epoch 21820/30000  training loss: 0.09190183877944946 Validation loss 0.09163650870323181\n",
            "Epoch 21830/30000  training loss: 0.09187837690114975 Validation loss 0.09161414206027985\n",
            "Epoch 21840/30000  training loss: 0.09185496717691422 Validation loss 0.09159181267023087\n",
            "Epoch 21850/30000  training loss: 0.09183157980442047 Validation loss 0.09156954288482666\n",
            "Epoch 21860/30000  training loss: 0.09180822223424911 Validation loss 0.09154725819826126\n",
            "Epoch 21870/30000  training loss: 0.09178487211465836 Validation loss 0.09152498096227646\n",
            "Epoch 21880/30000  training loss: 0.09176158159971237 Validation loss 0.09150278568267822\n",
            "Epoch 21890/30000  training loss: 0.09173835068941116 Validation loss 0.09148064255714417\n",
            "Epoch 21900/30000  training loss: 0.09171515703201294 Validation loss 0.09145853668451309\n",
            "Epoch 21910/30000  training loss: 0.0916920006275177 Validation loss 0.09143645316362381\n",
            "Epoch 21920/30000  training loss: 0.09166887402534485 Validation loss 0.09141438454389572\n",
            "Epoch 21930/30000  training loss: 0.0916457325220108 Validation loss 0.09139232337474823\n",
            "Epoch 21940/30000  training loss: 0.09162262827157974 Validation loss 0.09137029945850372\n",
            "Epoch 21950/30000  training loss: 0.09159955382347107 Validation loss 0.09134826809167862\n",
            "Epoch 21960/30000  training loss: 0.09157652407884598 Validation loss 0.09132631868124008\n",
            "Epoch 21970/30000  training loss: 0.09155350178480148 Validation loss 0.09130434691905975\n",
            "Epoch 21980/30000  training loss: 0.09153050184249878 Validation loss 0.09128241240978241\n",
            "Epoch 21990/30000  training loss: 0.09150750935077667 Validation loss 0.09126047790050507\n",
            "Epoch 22000/30000  training loss: 0.09148460626602173 Validation loss 0.09123864769935608\n",
            "Epoch 22010/30000  training loss: 0.09146175533533096 Validation loss 0.09121682494878769\n",
            "Epoch 22020/30000  training loss: 0.09143893420696259 Validation loss 0.09119506925344467\n",
            "Epoch 22030/30000  training loss: 0.091416135430336 Validation loss 0.09117332100868225\n",
            "Epoch 22040/30000  training loss: 0.0913933515548706 Validation loss 0.09115157276391983\n",
            "Epoch 22050/30000  training loss: 0.0913705825805664 Validation loss 0.0911298543214798\n",
            "Epoch 22060/30000  training loss: 0.0913478285074234 Validation loss 0.09110815078020096\n",
            "Epoch 22070/30000  training loss: 0.09132511913776398 Validation loss 0.09108646214008331\n",
            "Epoch 22080/30000  training loss: 0.09130243957042694 Validation loss 0.09106481820344925\n",
            "Epoch 22090/30000  training loss: 0.0912797749042511 Validation loss 0.09104318916797638\n",
            "Epoch 22100/30000  training loss: 0.09125711768865585 Validation loss 0.0910215675830841\n",
            "Epoch 22110/30000  training loss: 0.0912344828248024 Validation loss 0.09099999070167542\n",
            "Epoch 22120/30000  training loss: 0.0912119597196579 Validation loss 0.0909784734249115\n",
            "Epoch 22130/30000  training loss: 0.091189444065094 Validation loss 0.09095699340105057\n",
            "Epoch 22140/30000  training loss: 0.09116698056459427 Validation loss 0.09093555063009262\n",
            "Epoch 22150/30000  training loss: 0.09114453196525574 Validation loss 0.09091411530971527\n",
            "Epoch 22160/30000  training loss: 0.091122105717659 Validation loss 0.09089270234107971\n",
            "Epoch 22170/30000  training loss: 0.09109967947006226 Validation loss 0.09087130427360535\n",
            "Epoch 22180/30000  training loss: 0.0910772755742073 Validation loss 0.09084991365671158\n",
            "Epoch 22190/30000  training loss: 0.09105489403009415 Validation loss 0.0908285453915596\n",
            "Epoch 22200/30000  training loss: 0.09103254973888397 Validation loss 0.09080721437931061\n",
            "Epoch 22210/30000  training loss: 0.09101024270057678 Validation loss 0.09078589826822281\n",
            "Epoch 22220/30000  training loss: 0.09098793566226959 Validation loss 0.0907646045088768\n",
            "Epoch 22230/30000  training loss: 0.09096565842628479 Validation loss 0.09074334055185318\n",
            "Epoch 22240/30000  training loss: 0.09094345569610596 Validation loss 0.09072213619947433\n",
            "Epoch 22250/30000  training loss: 0.09092128276824951 Validation loss 0.09070096909999847\n",
            "Epoch 22260/30000  training loss: 0.09089916199445724 Validation loss 0.090679831802845\n",
            "Epoch 22270/30000  training loss: 0.09087704867124557 Validation loss 0.09065871685743332\n",
            "Epoch 22280/30000  training loss: 0.0908549576997757 Validation loss 0.09063761681318283\n",
            "Epoch 22290/30000  training loss: 0.09083285927772522 Validation loss 0.09061650931835175\n",
            "Epoch 22300/30000  training loss: 0.09081079810857773 Validation loss 0.09059541672468185\n",
            "Epoch 22310/30000  training loss: 0.09078875184059143 Validation loss 0.09057435393333435\n",
            "Epoch 22320/30000  training loss: 0.09076675027608871 Validation loss 0.09055335819721222\n",
            "Epoch 22330/30000  training loss: 0.09074476361274719 Validation loss 0.0905323326587677\n",
            "Epoch 22340/30000  training loss: 0.09072279185056686 Validation loss 0.09051134437322617\n",
            "Epoch 22350/30000  training loss: 0.09070083498954773 Validation loss 0.09049036353826523\n",
            "Epoch 22360/30000  training loss: 0.09067895263433456 Validation loss 0.09046947211027145\n",
            "Epoch 22370/30000  training loss: 0.09065712243318558 Validation loss 0.09044861048460007\n",
            "Epoch 22380/30000  training loss: 0.09063531458377838 Validation loss 0.09042777866125107\n",
            "Epoch 22390/30000  training loss: 0.09061352908611298 Validation loss 0.09040694683790207\n",
            "Epoch 22400/30000  training loss: 0.09059177339076996 Validation loss 0.09038614481687546\n",
            "Epoch 22410/30000  training loss: 0.09057001769542694 Validation loss 0.09036535024642944\n",
            "Epoch 22420/30000  training loss: 0.09054826945066452 Validation loss 0.09034456312656403\n",
            "Epoch 22430/30000  training loss: 0.09052654355764389 Validation loss 0.0903237983584404\n",
            "Epoch 22440/30000  training loss: 0.09050483256578445 Validation loss 0.09030306339263916\n",
            "Epoch 22450/30000  training loss: 0.0904831811785698 Validation loss 0.09028235077857971\n",
            "Epoch 22460/30000  training loss: 0.09046153724193573 Validation loss 0.09026165306568146\n",
            "Epoch 22470/30000  training loss: 0.09043989330530167 Validation loss 0.0902409628033638\n",
            "Epoch 22480/30000  training loss: 0.09041832387447357 Validation loss 0.09022033959627151\n",
            "Epoch 22490/30000  training loss: 0.09039679914712906 Validation loss 0.09019976109266281\n",
            "Epoch 22500/30000  training loss: 0.09037529677152634 Validation loss 0.0901792049407959\n",
            "Epoch 22510/30000  training loss: 0.0903538316488266 Validation loss 0.09015867114067078\n",
            "Epoch 22520/30000  training loss: 0.09033237397670746 Validation loss 0.09013815969228745\n",
            "Epoch 22530/30000  training loss: 0.09031093120574951 Validation loss 0.09011764824390411\n",
            "Epoch 22540/30000  training loss: 0.09028951823711395 Validation loss 0.09009715169668198\n",
            "Epoch 22550/30000  training loss: 0.0902680903673172 Validation loss 0.09007667005062103\n",
            "Epoch 22560/30000  training loss: 0.09024669975042343 Validation loss 0.09005621820688248\n",
            "Epoch 22570/30000  training loss: 0.09022534638643265 Validation loss 0.09003578871488571\n",
            "Epoch 22580/30000  training loss: 0.09020401537418365 Validation loss 0.09001538157463074\n",
            "Epoch 22590/30000  training loss: 0.09018267691135406 Validation loss 0.08999497443437576\n",
            "Epoch 22600/30000  training loss: 0.09016136825084686 Validation loss 0.08997459709644318\n",
            "Epoch 22610/30000  training loss: 0.09014014154672623 Validation loss 0.08995429426431656\n",
            "Epoch 22620/30000  training loss: 0.09011893719434738 Validation loss 0.08993400633335114\n",
            "Epoch 22630/30000  training loss: 0.09009777754545212 Validation loss 0.0899137556552887\n",
            "Epoch 22640/30000  training loss: 0.09007664024829865 Validation loss 0.08989351242780685\n",
            "Epoch 22650/30000  training loss: 0.09005550295114517 Validation loss 0.0898732915520668\n",
            "Epoch 22660/30000  training loss: 0.0900343731045723 Validation loss 0.08985307067632675\n",
            "Epoch 22670/30000  training loss: 0.09001327306032181 Validation loss 0.08983287960290909\n",
            "Epoch 22680/30000  training loss: 0.08999217301607132 Validation loss 0.08981268107891083\n",
            "Epoch 22690/30000  training loss: 0.08997110277414322 Validation loss 0.08979252725839615\n",
            "Epoch 22700/30000  training loss: 0.0899500697851181 Validation loss 0.08977238833904266\n",
            "Epoch 22710/30000  training loss: 0.08992903679609299 Validation loss 0.08975227177143097\n",
            "Epoch 22720/30000  training loss: 0.08990804105997086 Validation loss 0.08973215520381927\n",
            "Epoch 22730/30000  training loss: 0.08988704532384872 Validation loss 0.08971207588911057\n",
            "Epoch 22740/30000  training loss: 0.08986614644527435 Validation loss 0.08969204872846603\n",
            "Epoch 22750/30000  training loss: 0.08984524756669998 Validation loss 0.0896720439195633\n",
            "Epoch 22760/30000  training loss: 0.08982440084218979 Validation loss 0.08965209126472473\n",
            "Epoch 22770/30000  training loss: 0.08980356901884079 Validation loss 0.08963213860988617\n",
            "Epoch 22780/30000  training loss: 0.08978274464607239 Validation loss 0.0896122008562088\n",
            "Epoch 22790/30000  training loss: 0.08976195007562637 Validation loss 0.08959224820137024\n",
            "Epoch 22800/30000  training loss: 0.08974114060401917 Validation loss 0.08957235515117645\n",
            "Epoch 22810/30000  training loss: 0.08972033858299255 Validation loss 0.08955243229866028\n",
            "Epoch 22820/30000  training loss: 0.08969957381486893 Validation loss 0.08953253924846649\n",
            "Epoch 22830/30000  training loss: 0.08967884629964828 Validation loss 0.08951269835233688\n",
            "Epoch 22840/30000  training loss: 0.08965813368558884 Validation loss 0.08949283510446548\n",
            "Epoch 22850/30000  training loss: 0.08963742852210999 Validation loss 0.08947300910949707\n",
            "Epoch 22860/30000  training loss: 0.08961673825979233 Validation loss 0.08945320546627045\n",
            "Epoch 22870/30000  training loss: 0.08959612995386124 Validation loss 0.089433453977108\n",
            "Epoch 22880/30000  training loss: 0.08957555890083313 Validation loss 0.08941373974084854\n",
            "Epoch 22890/30000  training loss: 0.08955500274896622 Validation loss 0.08939404785633087\n",
            "Epoch 22900/30000  training loss: 0.0895344614982605 Validation loss 0.08937437832355499\n",
            "Epoch 22910/30000  training loss: 0.08951395004987717 Validation loss 0.08935469388961792\n",
            "Epoch 22920/30000  training loss: 0.08949343860149384 Validation loss 0.08933505415916443\n",
            "Epoch 22930/30000  training loss: 0.0894729420542717 Validation loss 0.08931539952754974\n",
            "Epoch 22940/30000  training loss: 0.08945243060588837 Validation loss 0.08929575979709625\n",
            "Epoch 22950/30000  training loss: 0.08943195641040802 Validation loss 0.08927613496780396\n",
            "Epoch 22960/30000  training loss: 0.08941152691841125 Validation loss 0.08925653994083405\n",
            "Epoch 22970/30000  training loss: 0.08939111977815628 Validation loss 0.08923697471618652\n",
            "Epoch 22980/30000  training loss: 0.08937070518732071 Validation loss 0.0892174169421196\n",
            "Epoch 22990/30000  training loss: 0.08935029804706573 Validation loss 0.08919784426689148\n",
            "Epoch 23000/30000  training loss: 0.08932998031377792 Validation loss 0.08917838335037231\n",
            "Epoch 23010/30000  training loss: 0.0893096849322319 Validation loss 0.08915890753269196\n",
            "Epoch 23020/30000  training loss: 0.08928939700126648 Validation loss 0.08913947641849518\n",
            "Epoch 23030/30000  training loss: 0.08926916122436523 Validation loss 0.089120052754879\n",
            "Epoch 23040/30000  training loss: 0.08924892544746399 Validation loss 0.0891006588935852\n",
            "Epoch 23050/30000  training loss: 0.08922869712114334 Validation loss 0.08908126503229141\n",
            "Epoch 23060/30000  training loss: 0.08920849859714508 Validation loss 0.08906188607215881\n",
            "Epoch 23070/30000  training loss: 0.08918829262256622 Validation loss 0.08904251456260681\n",
            "Epoch 23080/30000  training loss: 0.08916810154914856 Validation loss 0.08902314305305481\n",
            "Epoch 23090/30000  training loss: 0.08914794027805328 Validation loss 0.08900380879640579\n",
            "Epoch 23100/30000  training loss: 0.08912781625986099 Validation loss 0.08898448199033737\n",
            "Epoch 23110/30000  training loss: 0.08910767734050751 Validation loss 0.08896519243717194\n",
            "Epoch 23120/30000  training loss: 0.08908756822347641 Validation loss 0.0889458879828453\n",
            "Epoch 23130/30000  training loss: 0.0890674889087677 Validation loss 0.08892662823200226\n",
            "Epoch 23140/30000  training loss: 0.08904748409986496 Validation loss 0.08890742808580399\n",
            "Epoch 23150/30000  training loss: 0.08902747184038162 Validation loss 0.08888822793960571\n",
            "Epoch 23160/30000  training loss: 0.08900749683380127 Validation loss 0.08886905759572983\n",
            "Epoch 23170/30000  training loss: 0.0889875516295433 Validation loss 0.08884992450475693\n",
            "Epoch 23180/30000  training loss: 0.08896762132644653 Validation loss 0.08883078396320343\n",
            "Epoch 23190/30000  training loss: 0.08894768357276917 Validation loss 0.08881165087223053\n",
            "Epoch 23200/30000  training loss: 0.0889277532696724 Validation loss 0.08879252523183823\n",
            "Epoch 23210/30000  training loss: 0.08890785276889801 Validation loss 0.08877342194318771\n",
            "Epoch 23220/30000  training loss: 0.08888795226812363 Validation loss 0.0887543335556984\n",
            "Epoch 23230/30000  training loss: 0.08886808156967163 Validation loss 0.08873525261878967\n",
            "Epoch 23240/30000  training loss: 0.08884823322296143 Validation loss 0.08871619403362274\n",
            "Epoch 23250/30000  training loss: 0.08882838487625122 Validation loss 0.088697150349617\n",
            "Epoch 23260/30000  training loss: 0.0888085663318634 Validation loss 0.08867809921503067\n",
            "Epoch 23270/30000  training loss: 0.08878880739212036 Validation loss 0.0886591300368309\n",
            "Epoch 23280/30000  training loss: 0.08876906335353851 Validation loss 0.08864019066095352\n",
            "Epoch 23290/30000  training loss: 0.08874934911727905 Validation loss 0.08862124383449554\n",
            "Epoch 23300/30000  training loss: 0.08872966468334198 Validation loss 0.08860234171152115\n",
            "Epoch 23310/30000  training loss: 0.0887099876999855 Validation loss 0.08858344703912735\n",
            "Epoch 23320/30000  training loss: 0.08869033306837082 Validation loss 0.08856456726789474\n",
            "Epoch 23330/30000  training loss: 0.08867067843675613 Validation loss 0.08854569494724274\n",
            "Epoch 23340/30000  training loss: 0.08865103870630264 Validation loss 0.08852683007717133\n",
            "Epoch 23350/30000  training loss: 0.08863140642642975 Validation loss 0.08850796520709991\n",
            "Epoch 23360/30000  training loss: 0.08861178159713745 Validation loss 0.0884891226887703\n",
            "Epoch 23370/30000  training loss: 0.08859219402074814 Validation loss 0.08847030252218246\n",
            "Epoch 23380/30000  training loss: 0.08857262134552002 Validation loss 0.08845148980617523\n",
            "Epoch 23390/30000  training loss: 0.0885530635714531 Validation loss 0.08843269944190979\n",
            "Epoch 23400/30000  training loss: 0.08853351324796677 Validation loss 0.08841391652822495\n",
            "Epoch 23410/30000  training loss: 0.08851401507854462 Validation loss 0.08839518576860428\n",
            "Epoch 23420/30000  training loss: 0.08849454671144485 Validation loss 0.088376484811306\n",
            "Epoch 23430/30000  training loss: 0.08847511559724808 Validation loss 0.08835779875516891\n",
            "Epoch 23440/30000  training loss: 0.0884556919336319 Validation loss 0.08833912014961243\n",
            "Epoch 23450/30000  training loss: 0.08843629062175751 Validation loss 0.08832048624753952\n",
            "Epoch 23460/30000  training loss: 0.08841690421104431 Validation loss 0.08830184489488602\n",
            "Epoch 23470/30000  training loss: 0.08839752525091171 Validation loss 0.0882832258939743\n",
            "Epoch 23480/30000  training loss: 0.08837815374135971 Validation loss 0.0882645919919014\n",
            "Epoch 23490/30000  training loss: 0.0883588045835495 Validation loss 0.08824599534273148\n",
            "Epoch 23500/30000  training loss: 0.08833944797515869 Validation loss 0.08822737634181976\n",
            "Epoch 23510/30000  training loss: 0.08832009881734848 Validation loss 0.08820878714323044\n",
            "Epoch 23520/30000  training loss: 0.08830080926418304 Validation loss 0.0881902351975441\n",
            "Epoch 23530/30000  training loss: 0.0882815271615982 Validation loss 0.08817168325185776\n",
            "Epoch 23540/30000  training loss: 0.08826223760843277 Validation loss 0.08815313875675201\n",
            "Epoch 23550/30000  training loss: 0.08824297785758972 Validation loss 0.08813462406396866\n",
            "Epoch 23560/30000  training loss: 0.08822378516197205 Validation loss 0.08811616897583008\n",
            "Epoch 23570/30000  training loss: 0.08820460736751556 Validation loss 0.0880977138876915\n",
            "Epoch 23580/30000  training loss: 0.08818544447422028 Validation loss 0.08807927370071411\n",
            "Epoch 23590/30000  training loss: 0.08816629648208618 Validation loss 0.08806087076663971\n",
            "Epoch 23600/30000  training loss: 0.08814717829227448 Validation loss 0.08804246783256531\n",
            "Epoch 23610/30000  training loss: 0.08812807500362396 Validation loss 0.0880240797996521\n",
            "Epoch 23620/30000  training loss: 0.08810895681381226 Validation loss 0.08800569921731949\n",
            "Epoch 23630/30000  training loss: 0.08808986097574234 Validation loss 0.08798731863498688\n",
            "Epoch 23640/30000  training loss: 0.08807077258825302 Validation loss 0.08796895295381546\n",
            "Epoch 23650/30000  training loss: 0.0880516916513443 Validation loss 0.08795058727264404\n",
            "Epoch 23660/30000  training loss: 0.08803263306617737 Validation loss 0.08793225139379501\n",
            "Epoch 23670/30000  training loss: 0.08801360428333282 Validation loss 0.08791393041610718\n",
            "Epoch 23680/30000  training loss: 0.08799458295106888 Validation loss 0.08789562433958054\n",
            "Epoch 23690/30000  training loss: 0.08797556161880493 Validation loss 0.0878773182630539\n",
            "Epoch 23700/30000  training loss: 0.08795658499002457 Validation loss 0.08785907179117203\n",
            "Epoch 23710/30000  training loss: 0.08793766796588898 Validation loss 0.08784086257219315\n",
            "Epoch 23720/30000  training loss: 0.08791874349117279 Validation loss 0.08782263845205307\n",
            "Epoch 23730/30000  training loss: 0.08789984881877899 Validation loss 0.08780443668365479\n",
            "Epoch 23740/30000  training loss: 0.08788096904754639 Validation loss 0.08778627216815948\n",
            "Epoch 23750/30000  training loss: 0.08786211907863617 Validation loss 0.08776810765266418\n",
            "Epoch 23760/30000  training loss: 0.08784326165914536 Validation loss 0.08774995803833008\n",
            "Epoch 23770/30000  training loss: 0.08782441914081573 Validation loss 0.08773180842399597\n",
            "Epoch 23780/30000  training loss: 0.08780558407306671 Validation loss 0.08771367371082306\n",
            "Epoch 23790/30000  training loss: 0.08778675645589828 Validation loss 0.08769553154706955\n",
            "Epoch 23800/30000  training loss: 0.08776793628931046 Validation loss 0.08767741173505783\n",
            "Epoch 23810/30000  training loss: 0.08774913847446442 Validation loss 0.08765929937362671\n",
            "Epoch 23820/30000  training loss: 0.08773036301136017 Validation loss 0.08764121681451797\n",
            "Epoch 23830/30000  training loss: 0.08771159499883652 Validation loss 0.08762314915657043\n",
            "Epoch 23840/30000  training loss: 0.08769284188747406 Validation loss 0.0876050665974617\n",
            "Epoch 23850/30000  training loss: 0.08767412602901459 Validation loss 0.08758705109357834\n",
            "Epoch 23860/30000  training loss: 0.0876554623246193 Validation loss 0.08756905794143677\n",
            "Epoch 23870/30000  training loss: 0.0876368060708046 Validation loss 0.08755107969045639\n",
            "Epoch 23880/30000  training loss: 0.0876181498169899 Validation loss 0.08753310889005661\n",
            "Epoch 23890/30000  training loss: 0.08759952336549759 Validation loss 0.08751516789197922\n",
            "Epoch 23900/30000  training loss: 0.08758091926574707 Validation loss 0.08749723434448242\n",
            "Epoch 23910/30000  training loss: 0.08756233006715775 Validation loss 0.08747930824756622\n",
            "Epoch 23920/30000  training loss: 0.08754372596740723 Validation loss 0.08746139705181122\n",
            "Epoch 23930/30000  training loss: 0.0875251442193985 Validation loss 0.08744347840547562\n",
            "Epoch 23940/30000  training loss: 0.08750656992197037 Validation loss 0.08742557466030121\n",
            "Epoch 23950/30000  training loss: 0.08748800307512283 Validation loss 0.0874076709151268\n",
            "Epoch 23960/30000  training loss: 0.0874694436788559 Validation loss 0.08738977462053299\n",
            "Epoch 23970/30000  training loss: 0.08745090663433075 Validation loss 0.08737190812826157\n",
            "Epoch 23980/30000  training loss: 0.08743239939212799 Validation loss 0.08735404908657074\n",
            "Epoch 23990/30000  training loss: 0.08741389214992523 Validation loss 0.0873362198472023\n",
            "Epoch 24000/30000  training loss: 0.08739539980888367 Validation loss 0.08731839060783386\n",
            "Epoch 24010/30000  training loss: 0.08737697452306747 Validation loss 0.0873006209731102\n",
            "Epoch 24020/30000  training loss: 0.08735857158899307 Validation loss 0.08728286623954773\n",
            "Epoch 24030/30000  training loss: 0.08734016865491867 Validation loss 0.08726511895656586\n",
            "Epoch 24040/30000  training loss: 0.08732176572084427 Validation loss 0.08724736422300339\n",
            "Epoch 24050/30000  training loss: 0.08730341494083405 Validation loss 0.0872296690940857\n",
            "Epoch 24060/30000  training loss: 0.08728505671024323 Validation loss 0.08721195161342621\n",
            "Epoch 24070/30000  training loss: 0.087266705930233 Validation loss 0.08719425648450851\n",
            "Epoch 24080/30000  training loss: 0.08724837005138397 Validation loss 0.08717655390501022\n",
            "Epoch 24090/30000  training loss: 0.08723003417253494 Validation loss 0.08715887367725372\n",
            "Epoch 24100/30000  training loss: 0.0872117131948471 Validation loss 0.08714118599891663\n",
            "Epoch 24110/30000  training loss: 0.08719339966773987 Validation loss 0.08712351322174072\n",
            "Epoch 24120/30000  training loss: 0.08717507869005203 Validation loss 0.08710584789514542\n",
            "Epoch 24130/30000  training loss: 0.08715680241584778 Validation loss 0.0870882123708725\n",
            "Epoch 24140/30000  training loss: 0.08713853359222412 Validation loss 0.08707058429718018\n",
            "Epoch 24150/30000  training loss: 0.08712028712034225 Validation loss 0.08705294877290726\n",
            "Epoch 24160/30000  training loss: 0.08710206300020218 Validation loss 0.08703536540269852\n",
            "Epoch 24170/30000  training loss: 0.08708387613296509 Validation loss 0.08701782673597336\n",
            "Epoch 24180/30000  training loss: 0.08706571906805038 Validation loss 0.0870002955198288\n",
            "Epoch 24190/30000  training loss: 0.08704755455255508 Validation loss 0.08698274940252304\n",
            "Epoch 24200/30000  training loss: 0.08702940493822098 Validation loss 0.08696522563695908\n",
            "Epoch 24210/30000  training loss: 0.08701128512620926 Validation loss 0.0869477391242981\n",
            "Epoch 24220/30000  training loss: 0.08699318021535873 Validation loss 0.08693026006221771\n",
            "Epoch 24230/30000  training loss: 0.08697507530450821 Validation loss 0.08691277354955673\n",
            "Epoch 24240/30000  training loss: 0.08695697784423828 Validation loss 0.08689529448747635\n",
            "Epoch 24250/30000  training loss: 0.08693888783454895 Validation loss 0.08687782287597656\n",
            "Epoch 24260/30000  training loss: 0.08692081272602081 Validation loss 0.08686035871505737\n",
            "Epoch 24270/30000  training loss: 0.08690273016691208 Validation loss 0.08684289455413818\n",
            "Epoch 24280/30000  training loss: 0.08688465505838394 Validation loss 0.08682544529438019\n",
            "Epoch 24290/30000  training loss: 0.08686661720275879 Validation loss 0.08680801838636398\n",
            "Epoch 24300/30000  training loss: 0.08684859424829483 Validation loss 0.08679059892892838\n",
            "Epoch 24310/30000  training loss: 0.08683057129383087 Validation loss 0.08677318692207336\n",
            "Epoch 24320/30000  training loss: 0.0868125855922699 Validation loss 0.08675580471754074\n",
            "Epoch 24330/30000  training loss: 0.0867946520447731 Validation loss 0.0867384746670723\n",
            "Epoch 24340/30000  training loss: 0.08677670359611511 Validation loss 0.08672115951776505\n",
            "Epoch 24350/30000  training loss: 0.08675877749919891 Validation loss 0.0867038294672966\n",
            "Epoch 24360/30000  training loss: 0.08674086630344391 Validation loss 0.08668652921915054\n",
            "Epoch 24370/30000  training loss: 0.0867229774594307 Validation loss 0.08666922152042389\n",
            "Epoch 24380/30000  training loss: 0.08670510351657867 Validation loss 0.08665194362401962\n",
            "Epoch 24390/30000  training loss: 0.08668723702430725 Validation loss 0.08663467317819595\n",
            "Epoch 24400/30000  training loss: 0.08666937798261642 Validation loss 0.08661741763353348\n",
            "Epoch 24410/30000  training loss: 0.0866515189409256 Validation loss 0.0866001546382904\n",
            "Epoch 24420/30000  training loss: 0.08663365989923477 Validation loss 0.08658289164304733\n",
            "Epoch 24430/30000  training loss: 0.08661582320928574 Validation loss 0.08656563609838486\n",
            "Epoch 24440/30000  training loss: 0.0865979790687561 Validation loss 0.08654838800430298\n",
            "Epoch 24450/30000  training loss: 0.08658015727996826 Validation loss 0.0865311473608017\n",
            "Epoch 24460/30000  training loss: 0.08656235784292221 Validation loss 0.086513951420784\n",
            "Epoch 24470/30000  training loss: 0.08654457330703735 Validation loss 0.0864967331290245\n",
            "Epoch 24480/30000  training loss: 0.0865267887711525 Validation loss 0.0864795371890068\n",
            "Epoch 24490/30000  training loss: 0.08650906383991241 Validation loss 0.08646240830421448\n",
            "Epoch 24500/30000  training loss: 0.08649135380983353 Validation loss 0.08644527941942215\n",
            "Epoch 24510/30000  training loss: 0.08647366613149643 Validation loss 0.08642816543579102\n",
            "Epoch 24520/30000  training loss: 0.08645597100257874 Validation loss 0.08641105145215988\n",
            "Epoch 24530/30000  training loss: 0.08643829077482224 Validation loss 0.08639394491910934\n",
            "Epoch 24540/30000  training loss: 0.08642063289880753 Validation loss 0.0863768681883812\n",
            "Epoch 24550/30000  training loss: 0.0864029973745346 Validation loss 0.08635979890823364\n",
            "Epoch 24560/30000  training loss: 0.08638536185026169 Validation loss 0.08634272962808609\n",
            "Epoch 24570/30000  training loss: 0.08636772632598877 Validation loss 0.08632567524909973\n",
            "Epoch 24580/30000  training loss: 0.08635009825229645 Validation loss 0.08630860596895218\n",
            "Epoch 24590/30000  training loss: 0.08633247762918472 Validation loss 0.08629155904054642\n",
            "Epoch 24600/30000  training loss: 0.0863148644566536 Validation loss 0.08627451211214066\n",
            "Epoch 24610/30000  training loss: 0.08629725128412247 Validation loss 0.08625747263431549\n",
            "Epoch 24620/30000  training loss: 0.08627966791391373 Validation loss 0.08624042570590973\n",
            "Epoch 24630/30000  training loss: 0.08626209199428558 Validation loss 0.08622342348098755\n",
            "Epoch 24640/30000  training loss: 0.08624452352523804 Validation loss 0.08620643615722656\n",
            "Epoch 24650/30000  training loss: 0.08622697740793228 Validation loss 0.08618942648172379\n",
            "Epoch 24660/30000  training loss: 0.0862094834446907 Validation loss 0.08617250621318817\n",
            "Epoch 24670/30000  training loss: 0.08619200438261032 Validation loss 0.08615558594465256\n",
            "Epoch 24680/30000  training loss: 0.08617454022169113 Validation loss 0.08613866567611694\n",
            "Epoch 24690/30000  training loss: 0.08615707606077194 Validation loss 0.08612175285816193\n",
            "Epoch 24700/30000  training loss: 0.08613962680101395 Validation loss 0.0861048474907875\n",
            "Epoch 24710/30000  training loss: 0.08612218499183655 Validation loss 0.08608795702457428\n",
            "Epoch 24720/30000  training loss: 0.08610476553440094 Validation loss 0.08607108145952225\n",
            "Epoch 24730/30000  training loss: 0.08608734607696533 Validation loss 0.08605422079563141\n",
            "Epoch 24740/30000  training loss: 0.08606993407011032 Validation loss 0.08603736013174057\n",
            "Epoch 24750/30000  training loss: 0.0860525369644165 Validation loss 0.08602050691843033\n",
            "Epoch 24760/30000  training loss: 0.08603513240814209 Validation loss 0.08600363880395889\n",
            "Epoch 24770/30000  training loss: 0.08601775765419006 Validation loss 0.08598678559064865\n",
            "Epoch 24780/30000  training loss: 0.08600035309791565 Validation loss 0.0859699547290802\n",
            "Epoch 24790/30000  training loss: 0.08598297089338303 Validation loss 0.08595310151576996\n",
            "Epoch 24800/30000  training loss: 0.08596562594175339 Validation loss 0.0859362855553627\n",
            "Epoch 24810/30000  training loss: 0.08594827353954315 Validation loss 0.08591949194669724\n",
            "Epoch 24820/30000  training loss: 0.08593093603849411 Validation loss 0.08590268343687057\n",
            "Epoch 24830/30000  training loss: 0.08591365069150925 Validation loss 0.0858859270811081\n",
            "Epoch 24840/30000  training loss: 0.08589639514684677 Validation loss 0.0858692079782486\n",
            "Epoch 24850/30000  training loss: 0.08587914705276489 Validation loss 0.0858524814248085\n",
            "Epoch 24860/30000  training loss: 0.08586189895868301 Validation loss 0.0858357697725296\n",
            "Epoch 24870/30000  training loss: 0.08584465086460114 Validation loss 0.0858190506696701\n",
            "Epoch 24880/30000  training loss: 0.08582741767168045 Validation loss 0.08580233156681061\n",
            "Epoch 24890/30000  training loss: 0.08581020683050156 Validation loss 0.08578566461801529\n",
            "Epoch 24900/30000  training loss: 0.08579301089048386 Validation loss 0.08576897531747818\n",
            "Epoch 24910/30000  training loss: 0.08577582240104675 Validation loss 0.08575230836868286\n",
            "Epoch 24920/30000  training loss: 0.08575862646102905 Validation loss 0.08573564141988754\n",
            "Epoch 24930/30000  training loss: 0.08574144542217255 Validation loss 0.08571897447109222\n",
            "Epoch 24940/30000  training loss: 0.08572427183389664 Validation loss 0.0857023149728775\n",
            "Epoch 24950/30000  training loss: 0.08570709079504013 Validation loss 0.08568564802408218\n",
            "Epoch 24960/30000  training loss: 0.08568991720676422 Validation loss 0.08566899597644806\n",
            "Epoch 24970/30000  training loss: 0.0856727585196495 Validation loss 0.08565235882997513\n",
            "Epoch 24980/30000  training loss: 0.08565562218427658 Validation loss 0.0856357291340828\n",
            "Epoch 24990/30000  training loss: 0.08563848584890366 Validation loss 0.08561911433935165\n",
            "Epoch 25000/30000  training loss: 0.08562137186527252 Validation loss 0.08560249954462051\n",
            "Epoch 25010/30000  training loss: 0.08560431003570557 Validation loss 0.08558596670627594\n",
            "Epoch 25020/30000  training loss: 0.0855872705578804 Validation loss 0.08556943386793137\n",
            "Epoch 25030/30000  training loss: 0.08557023108005524 Validation loss 0.0855528935790062\n",
            "Epoch 25040/30000  training loss: 0.08555320650339127 Validation loss 0.08553636819124222\n",
            "Epoch 25050/30000  training loss: 0.0855361744761467 Validation loss 0.08551984280347824\n",
            "Epoch 25060/30000  training loss: 0.08551915735006332 Validation loss 0.08550331741571426\n",
            "Epoch 25070/30000  training loss: 0.08550214767456055 Validation loss 0.08548680692911148\n",
            "Epoch 25080/30000  training loss: 0.08548516780138016 Validation loss 0.08547031879425049\n",
            "Epoch 25090/30000  training loss: 0.08546819537878036 Validation loss 0.08545384556055069\n",
            "Epoch 25100/30000  training loss: 0.08545121550559998 Validation loss 0.0854373574256897\n",
            "Epoch 25110/30000  training loss: 0.08543423563241959 Validation loss 0.0854208767414093\n",
            "Epoch 25120/30000  training loss: 0.08541727066040039 Validation loss 0.0854044035077095\n",
            "Epoch 25130/30000  training loss: 0.0854003056883812 Validation loss 0.0853879451751709\n",
            "Epoch 25140/30000  training loss: 0.0853833556175232 Validation loss 0.0853714719414711\n",
            "Epoch 25150/30000  training loss: 0.0853663980960846 Validation loss 0.0853550061583519\n",
            "Epoch 25160/30000  training loss: 0.08534944802522659 Validation loss 0.08533855527639389\n",
            "Epoch 25170/30000  training loss: 0.08533253520727158 Validation loss 0.08532212674617767\n",
            "Epoch 25180/30000  training loss: 0.08531561493873596 Validation loss 0.08530569821596146\n",
            "Epoch 25190/30000  training loss: 0.08529873937368393 Validation loss 0.08528932183980942\n",
            "Epoch 25200/30000  training loss: 0.08528191596269608 Validation loss 0.08527296781539917\n",
            "Epoch 25210/30000  training loss: 0.08526507765054703 Validation loss 0.08525662124156952\n",
            "Epoch 25220/30000  training loss: 0.08524825423955917 Validation loss 0.08524026721715927\n",
            "Epoch 25230/30000  training loss: 0.08523143827915192 Validation loss 0.08522392809391022\n",
            "Epoch 25240/30000  training loss: 0.08521462231874466 Validation loss 0.08520758897066116\n",
            "Epoch 25250/30000  training loss: 0.0851977989077568 Validation loss 0.08519124984741211\n",
            "Epoch 25260/30000  training loss: 0.08518101274967194 Validation loss 0.08517494052648544\n",
            "Epoch 25270/30000  training loss: 0.08516423404216766 Validation loss 0.08515863865613937\n",
            "Epoch 25280/30000  training loss: 0.08514746278524399 Validation loss 0.0851423367857933\n",
            "Epoch 25290/30000  training loss: 0.08513069897890091 Validation loss 0.08512603491544724\n",
            "Epoch 25300/30000  training loss: 0.08511392772197723 Validation loss 0.08510974794626236\n",
            "Epoch 25310/30000  training loss: 0.08509717136621475 Validation loss 0.08509345352649689\n",
            "Epoch 25320/30000  training loss: 0.08508042246103287 Validation loss 0.08507716655731201\n",
            "Epoch 25330/30000  training loss: 0.08506366610527039 Validation loss 0.08506087958812714\n",
            "Epoch 25340/30000  training loss: 0.0850469246506691 Validation loss 0.08504460006952286\n",
            "Epoch 25350/30000  training loss: 0.08503018319606781 Validation loss 0.08502833545207977\n",
            "Epoch 25360/30000  training loss: 0.08501347154378891 Validation loss 0.08501207828521729\n",
            "Epoch 25370/30000  training loss: 0.0849967673420906 Validation loss 0.08499584347009659\n",
            "Epoch 25380/30000  training loss: 0.08498009294271469 Validation loss 0.08497963845729828\n",
            "Epoch 25390/30000  training loss: 0.08496347069740295 Validation loss 0.08496347069740295\n",
            "Epoch 25400/30000  training loss: 0.08494685590267181 Validation loss 0.08494729548692703\n",
            "Epoch 25410/30000  training loss: 0.08493022620677948 Validation loss 0.08493112772703171\n",
            "Epoch 25420/30000  training loss: 0.08491360396146774 Validation loss 0.08491497486829758\n",
            "Epoch 25430/30000  training loss: 0.0848969891667366 Validation loss 0.08489879965782166\n",
            "Epoch 25440/30000  training loss: 0.08488038182258606 Validation loss 0.08488264679908752\n",
            "Epoch 25450/30000  training loss: 0.08486378937959671 Validation loss 0.08486650884151459\n",
            "Epoch 25460/30000  training loss: 0.08484721183776855 Validation loss 0.08485037833452225\n",
            "Epoch 25470/30000  training loss: 0.084830641746521 Validation loss 0.0848342627286911\n",
            "Epoch 25480/30000  training loss: 0.08481407910585403 Validation loss 0.08481814712285995\n",
            "Epoch 25490/30000  training loss: 0.08479751646518707 Validation loss 0.08480201661586761\n",
            "Epoch 25500/30000  training loss: 0.08478094637393951 Validation loss 0.08478590846061707\n",
            "Epoch 25510/30000  training loss: 0.08476439863443375 Validation loss 0.08476979285478592\n",
            "Epoch 25520/30000  training loss: 0.08474784344434738 Validation loss 0.08475369215011597\n",
            "Epoch 25530/30000  training loss: 0.08473130315542221 Validation loss 0.08473759144544601\n",
            "Epoch 25540/30000  training loss: 0.08471474796533585 Validation loss 0.08472148329019547\n",
            "Epoch 25550/30000  training loss: 0.08469821512699127 Validation loss 0.08470539003610611\n",
            "Epoch 25560/30000  training loss: 0.08468171954154968 Validation loss 0.08468932658433914\n",
            "Epoch 25570/30000  training loss: 0.0846652165055275 Validation loss 0.08467328548431396\n",
            "Epoch 25580/30000  training loss: 0.08464879542589188 Validation loss 0.08465728163719177\n",
            "Epoch 25590/30000  training loss: 0.08463235944509506 Validation loss 0.08464129269123077\n",
            "Epoch 25600/30000  training loss: 0.08461593091487885 Validation loss 0.08462528139352798\n",
            "Epoch 25610/30000  training loss: 0.08459950983524323 Validation loss 0.08460929989814758\n",
            "Epoch 25620/30000  training loss: 0.0845830962061882 Validation loss 0.08459331095218658\n",
            "Epoch 25630/30000  training loss: 0.08456668257713318 Validation loss 0.08457732200622559\n",
            "Epoch 25640/30000  training loss: 0.08455026149749756 Validation loss 0.08456133306026459\n",
            "Epoch 25650/30000  training loss: 0.08453387022018433 Validation loss 0.08454538136720657\n",
            "Epoch 25660/30000  training loss: 0.08451749384403229 Validation loss 0.08452942967414856\n",
            "Epoch 25670/30000  training loss: 0.08450112491846085 Validation loss 0.08451347053050995\n",
            "Epoch 25680/30000  training loss: 0.08448474854230881 Validation loss 0.08449751883745193\n",
            "Epoch 25690/30000  training loss: 0.08446837961673737 Validation loss 0.08448158949613571\n",
            "Epoch 25700/30000  training loss: 0.08445202559232712 Validation loss 0.0844656378030777\n",
            "Epoch 25710/30000  training loss: 0.08443566411733627 Validation loss 0.08444970101118088\n",
            "Epoch 25720/30000  training loss: 0.08441931009292603 Validation loss 0.08443377912044525\n",
            "Epoch 25730/30000  training loss: 0.08440294861793518 Validation loss 0.08441781997680664\n",
            "Epoch 25740/30000  training loss: 0.08438660204410553 Validation loss 0.08440190553665161\n",
            "Epoch 25750/30000  training loss: 0.08437027037143707 Validation loss 0.08438597619533539\n",
            "Epoch 25760/30000  training loss: 0.08435395359992981 Validation loss 0.08437006920576096\n",
            "Epoch 25770/30000  training loss: 0.08433764427900314 Validation loss 0.08435419946908951\n",
            "Epoch 25780/30000  training loss: 0.08432141691446304 Validation loss 0.08433837443590164\n",
            "Epoch 25790/30000  training loss: 0.08430517464876175 Validation loss 0.08432255685329437\n",
            "Epoch 25800/30000  training loss: 0.08428894728422165 Validation loss 0.08430671691894531\n",
            "Epoch 25810/30000  training loss: 0.08427271246910095 Validation loss 0.08429089188575745\n",
            "Epoch 25820/30000  training loss: 0.08425649255514145 Validation loss 0.08427508175373077\n",
            "Epoch 25830/30000  training loss: 0.08424025774002075 Validation loss 0.08425925672054291\n",
            "Epoch 25840/30000  training loss: 0.08422404527664185 Validation loss 0.08424343913793564\n",
            "Epoch 25850/30000  training loss: 0.08420782536268234 Validation loss 0.08422762900590897\n",
            "Epoch 25860/30000  training loss: 0.08419163525104523 Validation loss 0.08421184867620468\n",
            "Epoch 25870/30000  training loss: 0.0841754600405693 Validation loss 0.0841960683465004\n",
            "Epoch 25880/30000  training loss: 0.08415927737951279 Validation loss 0.08418028056621552\n",
            "Epoch 25890/30000  training loss: 0.08414310216903687 Validation loss 0.08416450023651123\n",
            "Epoch 25900/30000  training loss: 0.08412692695856094 Validation loss 0.08414873480796814\n",
            "Epoch 25910/30000  training loss: 0.08411075174808502 Validation loss 0.08413294702768326\n",
            "Epoch 25920/30000  training loss: 0.0840945914387703 Validation loss 0.08411718159914017\n",
            "Epoch 25930/30000  training loss: 0.08407842367887497 Validation loss 0.08410141617059708\n",
            "Epoch 25940/30000  training loss: 0.08406225591897964 Validation loss 0.08408565074205399\n",
            "Epoch 25950/30000  training loss: 0.08404608815908432 Validation loss 0.0840698853135109\n",
            "Epoch 25960/30000  training loss: 0.08402995765209198 Validation loss 0.0840541422367096\n",
            "Epoch 25970/30000  training loss: 0.08401382714509964 Validation loss 0.0840383917093277\n",
            "Epoch 25980/30000  training loss: 0.08399774134159088 Validation loss 0.08402272313833237\n",
            "Epoch 25990/30000  training loss: 0.08398169279098511 Validation loss 0.08400703966617584\n",
            "Epoch 26000/30000  training loss: 0.08396564424037933 Validation loss 0.0839914008975029\n",
            "Epoch 26010/30000  training loss: 0.08394959568977356 Validation loss 0.08397572487592697\n",
            "Epoch 26020/30000  training loss: 0.08393355458974838 Validation loss 0.08396007865667343\n",
            "Epoch 26030/30000  training loss: 0.0839175209403038 Validation loss 0.0839444100856781\n",
            "Epoch 26040/30000  training loss: 0.08390147984027863 Validation loss 0.08392875641584396\n",
            "Epoch 26050/30000  training loss: 0.08388544619083405 Validation loss 0.08391308784484863\n",
            "Epoch 26060/30000  training loss: 0.08386941999197006 Validation loss 0.08389744907617569\n",
            "Epoch 26070/30000  training loss: 0.08385340124368668 Validation loss 0.08388182520866394\n",
            "Epoch 26080/30000  training loss: 0.08383741229772568 Validation loss 0.08386620134115219\n",
            "Epoch 26090/30000  training loss: 0.08382142335176468 Validation loss 0.08385059982538223\n",
            "Epoch 26100/30000  training loss: 0.08380542695522308 Validation loss 0.08383498340845108\n",
            "Epoch 26110/30000  training loss: 0.08378943800926208 Validation loss 0.08381935954093933\n",
            "Epoch 26120/30000  training loss: 0.08377344906330109 Validation loss 0.08380375057458878\n",
            "Epoch 26130/30000  training loss: 0.08375746756792068 Validation loss 0.08378813415765762\n",
            "Epoch 26140/30000  training loss: 0.08374148607254028 Validation loss 0.08377253264188766\n",
            "Epoch 26150/30000  training loss: 0.08372550457715988 Validation loss 0.0837569385766983\n",
            "Epoch 26160/30000  training loss: 0.08370954543352127 Validation loss 0.08374133706092834\n",
            "Epoch 26170/30000  training loss: 0.08369355648756027 Validation loss 0.08372572809457779\n",
            "Epoch 26180/30000  training loss: 0.08367761969566345 Validation loss 0.08371014147996902\n",
            "Epoch 26190/30000  training loss: 0.08366168290376663 Validation loss 0.08369459211826324\n",
            "Epoch 26200/30000  training loss: 0.08364582061767578 Validation loss 0.08367908746004105\n",
            "Epoch 26210/30000  training loss: 0.08362995833158493 Validation loss 0.08366359025239944\n",
            "Epoch 26220/30000  training loss: 0.08361409604549408 Validation loss 0.08364809304475784\n",
            "Epoch 26230/30000  training loss: 0.08359823375940323 Validation loss 0.08363259583711624\n",
            "Epoch 26240/30000  training loss: 0.08358237892389297 Validation loss 0.08361709117889404\n",
            "Epoch 26250/30000  training loss: 0.08356653153896332 Validation loss 0.08360160142183304\n",
            "Epoch 26260/30000  training loss: 0.08355066925287247 Validation loss 0.08358608931303024\n",
            "Epoch 26270/30000  training loss: 0.08353482186794281 Validation loss 0.08357059210538864\n",
            "Epoch 26280/30000  training loss: 0.08351897448301315 Validation loss 0.08355510979890823\n",
            "Epoch 26290/30000  training loss: 0.08350314944982529 Validation loss 0.08353965729475021\n",
            "Epoch 26300/30000  training loss: 0.08348732441663742 Validation loss 0.0835241973400116\n",
            "Epoch 26310/30000  training loss: 0.08347152918577194 Validation loss 0.08350874483585358\n",
            "Epoch 26320/30000  training loss: 0.08345571160316467 Validation loss 0.08349326997995377\n",
            "Epoch 26330/30000  training loss: 0.0834399089217186 Validation loss 0.08347782492637634\n",
            "Epoch 26340/30000  training loss: 0.08342409878969193 Validation loss 0.08346237242221832\n",
            "Epoch 26350/30000  training loss: 0.08340830355882645 Validation loss 0.0834469124674797\n",
            "Epoch 26360/30000  training loss: 0.08339250087738037 Validation loss 0.08343148231506348\n",
            "Epoch 26370/30000  training loss: 0.08337670564651489 Validation loss 0.08341602236032486\n",
            "Epoch 26380/30000  training loss: 0.08336091041564941 Validation loss 0.08340056985616684\n",
            "Epoch 26390/30000  training loss: 0.08334511518478394 Validation loss 0.08338510990142822\n",
            "Epoch 26400/30000  training loss: 0.08332933485507965 Validation loss 0.08336970210075378\n",
            "Epoch 26410/30000  training loss: 0.08331359922885895 Validation loss 0.08335430920124054\n",
            "Epoch 26420/30000  training loss: 0.08329790085554123 Validation loss 0.08333895355463028\n",
            "Epoch 26430/30000  training loss: 0.0832822173833847 Validation loss 0.08332360535860062\n",
            "Epoch 26440/30000  training loss: 0.08326653391122818 Validation loss 0.08330825716257095\n",
            "Epoch 26450/30000  training loss: 0.08325085788965225 Validation loss 0.08329292386770248\n",
            "Epoch 26460/30000  training loss: 0.08323517441749573 Validation loss 0.08327758312225342\n",
            "Epoch 26470/30000  training loss: 0.0832194983959198 Validation loss 0.08326222747564316\n",
            "Epoch 26480/30000  training loss: 0.08320382237434387 Validation loss 0.08324690163135529\n",
            "Epoch 26490/30000  training loss: 0.08318814635276794 Validation loss 0.08323156088590622\n",
            "Epoch 26500/30000  training loss: 0.08317247778177261 Validation loss 0.08321621268987656\n",
            "Epoch 26510/30000  training loss: 0.08315679430961609 Validation loss 0.08320089429616928\n",
            "Epoch 26520/30000  training loss: 0.08314116299152374 Validation loss 0.083185575902462\n",
            "Epoch 26530/30000  training loss: 0.0831255242228508 Validation loss 0.08317027240991592\n",
            "Epoch 26540/30000  training loss: 0.08310990035533905 Validation loss 0.08315496891736984\n",
            "Epoch 26550/30000  training loss: 0.08309425413608551 Validation loss 0.08313966542482376\n",
            "Epoch 26560/30000  training loss: 0.08307863026857376 Validation loss 0.08312436938285828\n",
            "Epoch 26570/30000  training loss: 0.08306299895048141 Validation loss 0.0831090658903122\n",
            "Epoch 26580/30000  training loss: 0.08304738253355026 Validation loss 0.08309376984834671\n",
            "Epoch 26590/30000  training loss: 0.08303175866603851 Validation loss 0.08307848125696182\n",
            "Epoch 26600/30000  training loss: 0.08301613479852676 Validation loss 0.08306318521499634\n",
            "Epoch 26610/30000  training loss: 0.08300051838159561 Validation loss 0.08304788917303085\n",
            "Epoch 26620/30000  training loss: 0.08298489451408386 Validation loss 0.08303260058164597\n",
            "Epoch 26630/30000  training loss: 0.0829693004488945 Validation loss 0.08301732689142227\n",
            "Epoch 26640/30000  training loss: 0.08295373618602753 Validation loss 0.08300209045410156\n",
            "Epoch 26650/30000  training loss: 0.08293822407722473 Validation loss 0.08298689126968384\n",
            "Epoch 26660/30000  training loss: 0.08292271196842194 Validation loss 0.08297169953584671\n",
            "Epoch 26670/30000  training loss: 0.08290719985961914 Validation loss 0.08295649290084839\n",
            "Epoch 26680/30000  training loss: 0.08289169520139694 Validation loss 0.08294130116701126\n",
            "Epoch 26690/30000  training loss: 0.08287619054317474 Validation loss 0.08292612433433533\n",
            "Epoch 26700/30000  training loss: 0.08286067843437195 Validation loss 0.0829109251499176\n",
            "Epoch 26710/30000  training loss: 0.08284517377614975 Validation loss 0.08289574086666107\n",
            "Epoch 26720/30000  training loss: 0.08282968401908875 Validation loss 0.08288054913282394\n",
            "Epoch 26730/30000  training loss: 0.08281417191028595 Validation loss 0.08286535739898682\n",
            "Epoch 26740/30000  training loss: 0.08279868960380554 Validation loss 0.08285017311573029\n",
            "Epoch 26750/30000  training loss: 0.08278319984674454 Validation loss 0.08283501863479614\n",
            "Epoch 26760/30000  training loss: 0.08276773989200592 Validation loss 0.0828198567032814\n",
            "Epoch 26770/30000  training loss: 0.08275226503610611 Validation loss 0.08280470222234726\n",
            "Epoch 26780/30000  training loss: 0.08273681253194809 Validation loss 0.08278954029083252\n",
            "Epoch 26790/30000  training loss: 0.08272136002779007 Validation loss 0.08277439326047897\n",
            "Epoch 26800/30000  training loss: 0.08270589262247086 Validation loss 0.08275925368070602\n",
            "Epoch 26810/30000  training loss: 0.08269044011831284 Validation loss 0.08274409174919128\n",
            "Epoch 26820/30000  training loss: 0.08267498761415482 Validation loss 0.08272894471883774\n",
            "Epoch 26830/30000  training loss: 0.08265954256057739 Validation loss 0.08271379768848419\n",
            "Epoch 26840/30000  training loss: 0.08264409750699997 Validation loss 0.08269865810871124\n",
            "Epoch 26850/30000  training loss: 0.08262864500284195 Validation loss 0.0826835036277771\n",
            "Epoch 26860/30000  training loss: 0.08261319249868393 Validation loss 0.08266835659742355\n",
            "Epoch 26870/30000  training loss: 0.0825977697968483 Validation loss 0.0826532393693924\n",
            "Epoch 26880/30000  training loss: 0.08258239924907684 Validation loss 0.08263818919658661\n",
            "Epoch 26890/30000  training loss: 0.08256706595420837 Validation loss 0.08262313902378082\n",
            "Epoch 26900/30000  training loss: 0.08255171775817871 Validation loss 0.08260808885097504\n",
            "Epoch 26910/30000  training loss: 0.08253638446331024 Validation loss 0.08259303122758865\n",
            "Epoch 26920/30000  training loss: 0.08252104371786118 Validation loss 0.08257798850536346\n",
            "Epoch 26930/30000  training loss: 0.0825057104229927 Validation loss 0.08256295323371887\n",
            "Epoch 26940/30000  training loss: 0.08249037712812424 Validation loss 0.08254789561033249\n",
            "Epoch 26950/30000  training loss: 0.08247503638267517 Validation loss 0.0825328603386879\n",
            "Epoch 26960/30000  training loss: 0.0824597030878067 Validation loss 0.0825178250670433\n",
            "Epoch 26970/30000  training loss: 0.08244436979293823 Validation loss 0.08250278979539871\n",
            "Epoch 26980/30000  training loss: 0.08242904394865036 Validation loss 0.08248773962259293\n",
            "Epoch 26990/30000  training loss: 0.08241371810436249 Validation loss 0.08247270435094833\n",
            "Epoch 27000/30000  training loss: 0.0823984146118164 Validation loss 0.08245769143104553\n",
            "Epoch 27010/30000  training loss: 0.08238312602043152 Validation loss 0.08244268596172333\n",
            "Epoch 27020/30000  training loss: 0.08236783742904663 Validation loss 0.08242768049240112\n",
            "Epoch 27030/30000  training loss: 0.08235254883766174 Validation loss 0.08241268247365952\n",
            "Epoch 27040/30000  training loss: 0.08233725279569626 Validation loss 0.08239767700433731\n",
            "Epoch 27050/30000  training loss: 0.08232196420431137 Validation loss 0.08238266408443451\n",
            "Epoch 27060/30000  training loss: 0.08230668306350708 Validation loss 0.0823676660656929\n",
            "Epoch 27070/30000  training loss: 0.08229140192270279 Validation loss 0.0823526605963707\n",
            "Epoch 27080/30000  training loss: 0.0822761207818985 Validation loss 0.08233766257762909\n",
            "Epoch 27090/30000  training loss: 0.08226083219051361 Validation loss 0.08232266455888748\n",
            "Epoch 27100/30000  training loss: 0.08224555104970932 Validation loss 0.08230765908956528\n",
            "Epoch 27110/30000  training loss: 0.08223028481006622 Validation loss 0.08229266852140427\n",
            "Epoch 27120/30000  training loss: 0.08221503347158432 Validation loss 0.08227770775556564\n",
            "Epoch 27130/30000  training loss: 0.08219984918832779 Validation loss 0.0822627991437912\n",
            "Epoch 27140/30000  training loss: 0.08218467980623245 Validation loss 0.08224789798259735\n",
            "Epoch 27150/30000  training loss: 0.08216949552297592 Validation loss 0.0822330042719841\n",
            "Epoch 27160/30000  training loss: 0.08215431869029999 Validation loss 0.08221809566020966\n",
            "Epoch 27170/30000  training loss: 0.08213914930820465 Validation loss 0.08220318704843521\n",
            "Epoch 27180/30000  training loss: 0.08212397992610931 Validation loss 0.08218827843666077\n",
            "Epoch 27190/30000  training loss: 0.08210880309343338 Validation loss 0.08217337727546692\n",
            "Epoch 27200/30000  training loss: 0.08209363371133804 Validation loss 0.08215848356485367\n",
            "Epoch 27210/30000  training loss: 0.0820784717798233 Validation loss 0.08214359730482101\n",
            "Epoch 27220/30000  training loss: 0.08206330239772797 Validation loss 0.08212869614362717\n",
            "Epoch 27230/30000  training loss: 0.08204814791679382 Validation loss 0.08211380243301392\n",
            "Epoch 27240/30000  training loss: 0.08203298598527908 Validation loss 0.08209890127182007\n",
            "Epoch 27250/30000  training loss: 0.08201783895492554 Validation loss 0.0820840373635292\n",
            "Epoch 27260/30000  training loss: 0.08200270682573318 Validation loss 0.08206916600465775\n",
            "Epoch 27270/30000  training loss: 0.08198758959770203 Validation loss 0.08205428719520569\n",
            "Epoch 27280/30000  training loss: 0.08197245746850967 Validation loss 0.08203942328691483\n",
            "Epoch 27290/30000  training loss: 0.08195732533931732 Validation loss 0.08202455937862396\n",
            "Epoch 27300/30000  training loss: 0.08194219321012497 Validation loss 0.0820097029209137\n",
            "Epoch 27310/30000  training loss: 0.08192708343267441 Validation loss 0.08199483901262283\n",
            "Epoch 27320/30000  training loss: 0.08191195130348206 Validation loss 0.08197998255491257\n",
            "Epoch 27330/30000  training loss: 0.0818968415260315 Validation loss 0.0819651186466217\n",
            "Epoch 27340/30000  training loss: 0.08188171684741974 Validation loss 0.08195026218891144\n",
            "Epoch 27350/30000  training loss: 0.08186660706996918 Validation loss 0.08193541318178177\n",
            "Epoch 27360/30000  training loss: 0.08185149729251862 Validation loss 0.0819205567240715\n",
            "Epoch 27370/30000  training loss: 0.08183636516332626 Validation loss 0.08190569281578064\n",
            "Epoch 27380/30000  training loss: 0.08182132989168167 Validation loss 0.08189091831445694\n",
            "Epoch 27390/30000  training loss: 0.08180630952119827 Validation loss 0.08187615126371384\n",
            "Epoch 27400/30000  training loss: 0.08179129660129547 Validation loss 0.08186137676239014\n",
            "Epoch 27410/30000  training loss: 0.08177627623081207 Validation loss 0.08184661716222763\n",
            "Epoch 27420/30000  training loss: 0.08176127821207047 Validation loss 0.08183185011148453\n",
            "Epoch 27430/30000  training loss: 0.08174626529216766 Validation loss 0.08181709796190262\n",
            "Epoch 27440/30000  training loss: 0.08173125982284546 Validation loss 0.08180233836174011\n",
            "Epoch 27450/30000  training loss: 0.08171625435352325 Validation loss 0.08178757131099701\n",
            "Epoch 27460/30000  training loss: 0.08170124143362045 Validation loss 0.0817728117108345\n",
            "Epoch 27470/30000  training loss: 0.08168622851371765 Validation loss 0.081758052110672\n",
            "Epoch 27480/30000  training loss: 0.08167122304439545 Validation loss 0.08174329251050949\n",
            "Epoch 27490/30000  training loss: 0.08165622502565384 Validation loss 0.08172854036092758\n",
            "Epoch 27500/30000  training loss: 0.08164122700691223 Validation loss 0.08171378076076508\n",
            "Epoch 27510/30000  training loss: 0.08162623643875122 Validation loss 0.08169903606176376\n",
            "Epoch 27520/30000  training loss: 0.0816112533211708 Validation loss 0.08168431371450424\n",
            "Epoch 27530/30000  training loss: 0.08159629255533218 Validation loss 0.08166959881782532\n",
            "Epoch 27540/30000  training loss: 0.08158131688833237 Validation loss 0.0816548690199852\n",
            "Epoch 27550/30000  training loss: 0.08156635612249374 Validation loss 0.08164013177156448\n",
            "Epoch 27560/30000  training loss: 0.08155139535665512 Validation loss 0.08162541687488556\n",
            "Epoch 27570/30000  training loss: 0.0815364271402359 Validation loss 0.08161068707704544\n",
            "Epoch 27580/30000  training loss: 0.08152146637439728 Validation loss 0.08159595727920532\n",
            "Epoch 27590/30000  training loss: 0.08150649815797806 Validation loss 0.0815812349319458\n",
            "Epoch 27600/30000  training loss: 0.08149153739213943 Validation loss 0.08156652003526688\n",
            "Epoch 27610/30000  training loss: 0.08147658407688141 Validation loss 0.08155179768800735\n",
            "Epoch 27620/30000  training loss: 0.08146163076162338 Validation loss 0.08153708279132843\n",
            "Epoch 27630/30000  training loss: 0.08144666999578476 Validation loss 0.0815223678946495\n",
            "Epoch 27640/30000  training loss: 0.08143173903226852 Validation loss 0.08150767534971237\n",
            "Epoch 27650/30000  training loss: 0.08141686022281647 Validation loss 0.08149304986000061\n",
            "Epoch 27660/30000  training loss: 0.0814020112156868 Validation loss 0.08147839456796646\n",
            "Epoch 27670/30000  training loss: 0.08138715475797653 Validation loss 0.0814637765288353\n",
            "Epoch 27680/30000  training loss: 0.08137229830026627 Validation loss 0.08144913613796234\n",
            "Epoch 27690/30000  training loss: 0.081357441842556 Validation loss 0.08143452554941177\n",
            "Epoch 27700/30000  training loss: 0.08134258538484573 Validation loss 0.08141989260911942\n",
            "Epoch 27710/30000  training loss: 0.08132773637771606 Validation loss 0.08140527456998825\n",
            "Epoch 27720/30000  training loss: 0.0813128873705864 Validation loss 0.08139064162969589\n",
            "Epoch 27730/30000  training loss: 0.08129803836345673 Validation loss 0.08137603104114532\n",
            "Epoch 27740/30000  training loss: 0.08128318935632706 Validation loss 0.08136140555143356\n",
            "Epoch 27750/30000  training loss: 0.08126834034919739 Validation loss 0.0813467875123024\n",
            "Epoch 27760/30000  training loss: 0.08125349879264832 Validation loss 0.08133216202259064\n",
            "Epoch 27770/30000  training loss: 0.08123865723609924 Validation loss 0.08131754398345947\n",
            "Epoch 27780/30000  training loss: 0.08122380077838898 Validation loss 0.08130292594432831\n",
            "Epoch 27790/30000  training loss: 0.08120898902416229 Validation loss 0.08128833025693893\n",
            "Epoch 27800/30000  training loss: 0.08119416236877441 Validation loss 0.08127373456954956\n",
            "Epoch 27810/30000  training loss: 0.08117936551570892 Validation loss 0.08125913888216019\n",
            "Epoch 27820/30000  training loss: 0.08116455376148224 Validation loss 0.08124454319477081\n",
            "Epoch 27830/30000  training loss: 0.08114974945783615 Validation loss 0.08122996985912323\n",
            "Epoch 27840/30000  training loss: 0.08113493770360947 Validation loss 0.08121538162231445\n",
            "Epoch 27850/30000  training loss: 0.08112014085054398 Validation loss 0.08120079338550568\n",
            "Epoch 27860/30000  training loss: 0.08110533654689789 Validation loss 0.0811862051486969\n",
            "Epoch 27870/30000  training loss: 0.0810905247926712 Validation loss 0.08117160946130753\n",
            "Epoch 27880/30000  training loss: 0.08107572048902512 Validation loss 0.08115702867507935\n",
            "Epoch 27890/30000  training loss: 0.08106091618537903 Validation loss 0.08114244043827057\n",
            "Epoch 27900/30000  training loss: 0.08104611188173294 Validation loss 0.08112786710262299\n",
            "Epoch 27910/30000  training loss: 0.08103132247924805 Validation loss 0.0811132863163948\n",
            "Epoch 27920/30000  training loss: 0.08101656287908554 Validation loss 0.08109874278306961\n",
            "Epoch 27930/30000  training loss: 0.08100184798240662 Validation loss 0.0810842514038086\n",
            "Epoch 27940/30000  training loss: 0.08098714798688889 Validation loss 0.08106975257396698\n",
            "Epoch 27950/30000  training loss: 0.08097244799137115 Validation loss 0.08105525374412537\n",
            "Epoch 27960/30000  training loss: 0.08095774054527283 Validation loss 0.08104076236486435\n",
            "Epoch 27970/30000  training loss: 0.0809430405497551 Validation loss 0.08102626353502274\n",
            "Epoch 27980/30000  training loss: 0.08092834055423737 Validation loss 0.08101176470518112\n",
            "Epoch 27990/30000  training loss: 0.08091364055871964 Validation loss 0.0809972807765007\n",
            "Epoch 28000/30000  training loss: 0.0808989405632019 Validation loss 0.08098278939723969\n",
            "Epoch 28010/30000  training loss: 0.08088424801826477 Validation loss 0.08096829801797867\n",
            "Epoch 28020/30000  training loss: 0.08086954802274704 Validation loss 0.08095381408929825\n",
            "Epoch 28030/30000  training loss: 0.0808548703789711 Validation loss 0.08093932271003723\n",
            "Epoch 28040/30000  training loss: 0.08084017783403397 Validation loss 0.08092484623193741\n",
            "Epoch 28050/30000  training loss: 0.08082548528909683 Validation loss 0.0809103474020958\n",
            "Epoch 28060/30000  training loss: 0.0808108001947403 Validation loss 0.08089587092399597\n",
            "Epoch 28070/30000  training loss: 0.08079610764980316 Validation loss 0.08088138699531555\n",
            "Epoch 28080/30000  training loss: 0.08078145235776901 Validation loss 0.08086692541837692\n",
            "Epoch 28090/30000  training loss: 0.08076679706573486 Validation loss 0.0808524638414383\n",
            "Epoch 28100/30000  training loss: 0.08075212687253952 Validation loss 0.08083800226449966\n",
            "Epoch 28110/30000  training loss: 0.08073747158050537 Validation loss 0.08082354068756104\n",
            "Epoch 28120/30000  training loss: 0.08072281628847122 Validation loss 0.0808090791106224\n",
            "Epoch 28130/30000  training loss: 0.08070816844701767 Validation loss 0.08079463243484497\n",
            "Epoch 28140/30000  training loss: 0.08069351315498352 Validation loss 0.08078019320964813\n",
            "Epoch 28150/30000  training loss: 0.08067887276411057 Validation loss 0.0807657465338707\n",
            "Epoch 28160/30000  training loss: 0.08066421747207642 Validation loss 0.08075129240751266\n",
            "Epoch 28170/30000  training loss: 0.08064956218004227 Validation loss 0.08073683828115463\n",
            "Epoch 28180/30000  training loss: 0.08063492178916931 Validation loss 0.080722376704216\n",
            "Epoch 28190/30000  training loss: 0.08062026649713516 Validation loss 0.08070792257785797\n",
            "Epoch 28200/30000  training loss: 0.08060561865568161 Validation loss 0.08069347590208054\n",
            "Epoch 28210/30000  training loss: 0.08059103041887283 Validation loss 0.08067907392978668\n",
            "Epoch 28220/30000  training loss: 0.08057644218206406 Validation loss 0.08066468685865402\n",
            "Epoch 28230/30000  training loss: 0.08056190609931946 Validation loss 0.08065032213926315\n",
            "Epoch 28240/30000  training loss: 0.08054734766483307 Validation loss 0.08063595741987228\n",
            "Epoch 28250/30000  training loss: 0.08053281158208847 Validation loss 0.0806216150522232\n",
            "Epoch 28260/30000  training loss: 0.08051826059818268 Validation loss 0.08060725033283234\n",
            "Epoch 28270/30000  training loss: 0.08050372451543808 Validation loss 0.08059289306402206\n",
            "Epoch 28280/30000  training loss: 0.08048916608095169 Validation loss 0.08057854324579239\n",
            "Epoch 28290/30000  training loss: 0.0804746225476265 Validation loss 0.08056416362524033\n",
            "Epoch 28300/30000  training loss: 0.0804600790143013 Validation loss 0.08054982125759125\n",
            "Epoch 28310/30000  training loss: 0.0804455354809761 Validation loss 0.08053545653820038\n",
            "Epoch 28320/30000  training loss: 0.08043099194765091 Validation loss 0.08052109181880951\n",
            "Epoch 28330/30000  training loss: 0.08041644841432571 Validation loss 0.08050672709941864\n",
            "Epoch 28340/30000  training loss: 0.08040190488100052 Validation loss 0.08049237728118896\n",
            "Epoch 28350/30000  training loss: 0.08038736134767532 Validation loss 0.08047802001237869\n",
            "Epoch 28360/30000  training loss: 0.08037282526493073 Validation loss 0.08046367019414902\n",
            "Epoch 28370/30000  training loss: 0.08035830408334732 Validation loss 0.08044932037591934\n",
            "Epoch 28380/30000  training loss: 0.08034379780292511 Validation loss 0.08043499290943146\n",
            "Epoch 28390/30000  training loss: 0.0803292915225029 Validation loss 0.08042066544294357\n",
            "Epoch 28400/30000  training loss: 0.08031478524208069 Validation loss 0.08040634542703629\n",
            "Epoch 28410/30000  training loss: 0.08030027151107788 Validation loss 0.080392025411129\n",
            "Epoch 28420/30000  training loss: 0.08028577268123627 Validation loss 0.08037769049406052\n",
            "Epoch 28430/30000  training loss: 0.08027126640081406 Validation loss 0.08036336302757263\n",
            "Epoch 28440/30000  training loss: 0.08025676012039185 Validation loss 0.08034903556108475\n",
            "Epoch 28450/30000  training loss: 0.08024226129055023 Validation loss 0.08033470809459686\n",
            "Epoch 28460/30000  training loss: 0.08022775501012802 Validation loss 0.08032038062810898\n",
            "Epoch 28470/30000  training loss: 0.080213263630867 Validation loss 0.0803060531616211\n",
            "Epoch 28480/30000  training loss: 0.0801987573504448 Validation loss 0.0802917331457138\n",
            "Epoch 28490/30000  training loss: 0.08018426597118378 Validation loss 0.08027741312980652\n",
            "Epoch 28500/30000  training loss: 0.08016977459192276 Validation loss 0.08026310056447983\n",
            "Epoch 28510/30000  training loss: 0.08015532046556473 Validation loss 0.08024883270263672\n",
            "Epoch 28520/30000  training loss: 0.08014089614152908 Validation loss 0.0802345797419548\n",
            "Epoch 28530/30000  training loss: 0.08012647926807404 Validation loss 0.08022034168243408\n",
            "Epoch 28540/30000  training loss: 0.08011207729578018 Validation loss 0.08020609617233276\n",
            "Epoch 28550/30000  training loss: 0.08009767532348633 Validation loss 0.08019185811281204\n",
            "Epoch 28560/30000  training loss: 0.08008327335119247 Validation loss 0.08017762750387192\n",
            "Epoch 28570/30000  training loss: 0.08006887137889862 Validation loss 0.0801633968949318\n",
            "Epoch 28580/30000  training loss: 0.08005447685718536 Validation loss 0.08014915883541107\n",
            "Epoch 28590/30000  training loss: 0.08004007488489151 Validation loss 0.08013492077589035\n",
            "Epoch 28600/30000  training loss: 0.08002568781375885 Validation loss 0.08012070506811142\n",
            "Epoch 28610/30000  training loss: 0.0800112783908844 Validation loss 0.0801064744591713\n",
            "Epoch 28620/30000  training loss: 0.07999689877033234 Validation loss 0.08009224385023117\n",
            "Epoch 28630/30000  training loss: 0.07998251169919968 Validation loss 0.08007802814245224\n",
            "Epoch 28640/30000  training loss: 0.07996811717748642 Validation loss 0.08006379008293152\n",
            "Epoch 28650/30000  training loss: 0.07995373010635376 Validation loss 0.0800495520234108\n",
            "Epoch 28660/30000  training loss: 0.0799393281340599 Validation loss 0.08003532886505127\n",
            "Epoch 28670/30000  training loss: 0.07992494851350784 Validation loss 0.08002111315727234\n",
            "Epoch 28680/30000  training loss: 0.07991054654121399 Validation loss 0.08000687509775162\n",
            "Epoch 28690/30000  training loss: 0.07989618182182312 Validation loss 0.07999265938997269\n",
            "Epoch 28700/30000  training loss: 0.07988182455301285 Validation loss 0.07997847348451614\n",
            "Epoch 28710/30000  training loss: 0.07986745983362198 Validation loss 0.07996427267789841\n",
            "Epoch 28720/30000  training loss: 0.0798531100153923 Validation loss 0.07995007187128067\n",
            "Epoch 28730/30000  training loss: 0.07983875274658203 Validation loss 0.07993587851524353\n",
            "Epoch 28740/30000  training loss: 0.07982439547777176 Validation loss 0.07992168515920639\n",
            "Epoch 28750/30000  training loss: 0.07981004565954208 Validation loss 0.07990749925374985\n",
            "Epoch 28760/30000  training loss: 0.07979568839073181 Validation loss 0.07989329099655151\n",
            "Epoch 28770/30000  training loss: 0.07978133857250214 Validation loss 0.07987909764051437\n",
            "Epoch 28780/30000  training loss: 0.07976698875427246 Validation loss 0.07986490428447723\n",
            "Epoch 28790/30000  training loss: 0.07975263148546219 Validation loss 0.0798507109284401\n",
            "Epoch 28800/30000  training loss: 0.07973828166723251 Validation loss 0.07983651757240295\n",
            "Epoch 28810/30000  training loss: 0.07972393184900284 Validation loss 0.07982232421636581\n",
            "Epoch 28820/30000  training loss: 0.07970959693193436 Validation loss 0.07980813086032867\n",
            "Epoch 28830/30000  training loss: 0.07969530671834946 Validation loss 0.0797940045595169\n",
            "Epoch 28840/30000  training loss: 0.07968101650476456 Validation loss 0.07977987080812454\n",
            "Epoch 28850/30000  training loss: 0.07966675609350204 Validation loss 0.07976575195789337\n",
            "Epoch 28860/30000  training loss: 0.07965250313282013 Validation loss 0.07975165545940399\n",
            "Epoch 28870/30000  training loss: 0.07963825017213821 Validation loss 0.07973754405975342\n",
            "Epoch 28880/30000  training loss: 0.0796239897608757 Validation loss 0.07972344011068344\n",
            "Epoch 28890/30000  training loss: 0.07960975170135498 Validation loss 0.07970932871103287\n",
            "Epoch 28900/30000  training loss: 0.07959548383951187 Validation loss 0.07969523966312408\n",
            "Epoch 28910/30000  training loss: 0.07958123832941055 Validation loss 0.07968112826347351\n",
            "Epoch 28920/30000  training loss: 0.07956699281930923 Validation loss 0.07966702431440353\n",
            "Epoch 28930/30000  training loss: 0.07955274730920792 Validation loss 0.07965292036533356\n",
            "Epoch 28940/30000  training loss: 0.079538494348526 Validation loss 0.07963882386684418\n",
            "Epoch 28950/30000  training loss: 0.07952424883842468 Validation loss 0.0796247124671936\n",
            "Epoch 28960/30000  training loss: 0.07950999587774277 Validation loss 0.07961061596870422\n",
            "Epoch 28970/30000  training loss: 0.07949575781822205 Validation loss 0.07959651947021484\n",
            "Epoch 28980/30000  training loss: 0.07948151230812073 Validation loss 0.07958241552114487\n",
            "Epoch 28990/30000  training loss: 0.07946726679801941 Validation loss 0.07956831157207489\n",
            "Epoch 29000/30000  training loss: 0.07945302873849869 Validation loss 0.07955421507358551\n",
            "Epoch 29010/30000  training loss: 0.07943879067897797 Validation loss 0.07954011857509613\n",
            "Epoch 29020/30000  training loss: 0.07942457497119904 Validation loss 0.07952603697776794\n",
            "Epoch 29030/30000  training loss: 0.0794103592634201 Validation loss 0.07951196283102036\n",
            "Epoch 29040/30000  training loss: 0.07939615100622177 Validation loss 0.07949790358543396\n",
            "Epoch 29050/30000  training loss: 0.07938192784786224 Validation loss 0.07948382943868637\n",
            "Epoch 29060/30000  training loss: 0.0793677270412445 Validation loss 0.07946974784135818\n",
            "Epoch 29070/30000  training loss: 0.07935351878404617 Validation loss 0.07945568859577179\n",
            "Epoch 29080/30000  training loss: 0.07933931052684784 Validation loss 0.0794416144490242\n",
            "Epoch 29090/30000  training loss: 0.07932509481906891 Validation loss 0.07942754030227661\n",
            "Epoch 29100/30000  training loss: 0.07931088656187057 Validation loss 0.07941347360610962\n",
            "Epoch 29110/30000  training loss: 0.07929668575525284 Validation loss 0.07939940690994263\n",
            "Epoch 29120/30000  training loss: 0.0792824774980545 Validation loss 0.07938534021377563\n",
            "Epoch 29130/30000  training loss: 0.07926828414201736 Validation loss 0.07937128096818924\n",
            "Epoch 29140/30000  training loss: 0.07925407588481903 Validation loss 0.07935721427202225\n",
            "Epoch 29150/30000  training loss: 0.0792398676276207 Validation loss 0.07934314757585526\n",
            "Epoch 29160/30000  training loss: 0.07922573387622833 Validation loss 0.07932914048433304\n",
            "Epoch 29170/30000  training loss: 0.07921159267425537 Validation loss 0.07931511849164963\n",
            "Epoch 29180/30000  training loss: 0.07919745147228241 Validation loss 0.0793011337518692\n",
            "Epoch 29190/30000  training loss: 0.07918333262205124 Validation loss 0.07928713411092758\n",
            "Epoch 29200/30000  training loss: 0.07916922867298126 Validation loss 0.07927317172288895\n",
            "Epoch 29210/30000  training loss: 0.07915511727333069 Validation loss 0.07925918698310852\n",
            "Epoch 29220/30000  training loss: 0.07914100587368011 Validation loss 0.0792451947927475\n",
            "Epoch 29230/30000  training loss: 0.07912690937519073 Validation loss 0.07923123240470886\n",
            "Epoch 29240/30000  training loss: 0.07911279797554016 Validation loss 0.07921724766492844\n",
            "Epoch 29250/30000  training loss: 0.07909869402647018 Validation loss 0.07920327037572861\n",
            "Epoch 29260/30000  training loss: 0.07908459007740021 Validation loss 0.07918928563594818\n",
            "Epoch 29270/30000  training loss: 0.07907049357891083 Validation loss 0.07917532324790955\n",
            "Epoch 29280/30000  training loss: 0.07905638962984085 Validation loss 0.07916134595870972\n",
            "Epoch 29290/30000  training loss: 0.07904228568077087 Validation loss 0.07914736121892929\n",
            "Epoch 29300/30000  training loss: 0.0790281891822815 Validation loss 0.07913338392972946\n",
            "Epoch 29310/30000  training loss: 0.07901408523321152 Validation loss 0.07911940664052963\n",
            "Epoch 29320/30000  training loss: 0.07899998128414154 Validation loss 0.0791054368019104\n",
            "Epoch 29330/30000  training loss: 0.07898588478565216 Validation loss 0.07909145206212997\n",
            "Epoch 29340/30000  training loss: 0.07897178083658218 Validation loss 0.07907748967409134\n",
            "Epoch 29350/30000  training loss: 0.0789576917886734 Validation loss 0.07906350493431091\n",
            "Epoch 29360/30000  training loss: 0.07894361019134521 Validation loss 0.07904954999685287\n",
            "Epoch 29370/30000  training loss: 0.07892953604459763 Validation loss 0.07903560996055603\n",
            "Epoch 29380/30000  training loss: 0.07891546934843063 Validation loss 0.07902165502309799\n",
            "Epoch 29390/30000  training loss: 0.07890139520168304 Validation loss 0.07900771498680115\n",
            "Epoch 29400/30000  training loss: 0.07888733595609665 Validation loss 0.0789937749505043\n",
            "Epoch 29410/30000  training loss: 0.07887326925992966 Validation loss 0.07897983491420746\n",
            "Epoch 29420/30000  training loss: 0.07885920256376266 Validation loss 0.07896587997674942\n",
            "Epoch 29430/30000  training loss: 0.07884514331817627 Validation loss 0.07895195484161377\n",
            "Epoch 29440/30000  training loss: 0.07883108407258987 Validation loss 0.07893799245357513\n",
            "Epoch 29450/30000  training loss: 0.07881701737642288 Validation loss 0.07892405241727829\n",
            "Epoch 29460/30000  training loss: 0.07880295068025589 Validation loss 0.07891010493040085\n",
            "Epoch 29470/30000  training loss: 0.0787888914346695 Validation loss 0.07889615744352341\n",
            "Epoch 29480/30000  training loss: 0.0787748247385025 Validation loss 0.07888221740722656\n",
            "Epoch 29490/30000  training loss: 0.07876076549291611 Validation loss 0.07886826246976852\n",
            "Epoch 29500/30000  training loss: 0.0787467211484909 Validation loss 0.07885435223579407\n",
            "Epoch 29510/30000  training loss: 0.07873272895812988 Validation loss 0.0788404643535614\n",
            "Epoch 29520/30000  training loss: 0.07871874421834946 Validation loss 0.07882659137248993\n",
            "Epoch 29530/30000  training loss: 0.07870474457740784 Validation loss 0.07881271094083786\n",
            "Epoch 29540/30000  training loss: 0.07869076728820801 Validation loss 0.07879885286092758\n",
            "Epoch 29550/30000  training loss: 0.07867680490016937 Validation loss 0.0787849947810173\n",
            "Epoch 29560/30000  training loss: 0.07866283506155014 Validation loss 0.07877114415168762\n",
            "Epoch 29570/30000  training loss: 0.0786488726735115 Validation loss 0.07875728607177734\n",
            "Epoch 29580/30000  training loss: 0.07863491028547287 Validation loss 0.07874344289302826\n",
            "Epoch 29590/30000  training loss: 0.07862094789743423 Validation loss 0.07872958481311798\n",
            "Epoch 29600/30000  training loss: 0.0786069855093956 Validation loss 0.0787157267332077\n",
            "Epoch 29610/30000  training loss: 0.07859300822019577 Validation loss 0.07870186120271683\n",
            "Epoch 29620/30000  training loss: 0.07857904583215714 Validation loss 0.07868800312280655\n",
            "Epoch 29630/30000  training loss: 0.0785650908946991 Validation loss 0.07867416739463806\n",
            "Epoch 29640/30000  training loss: 0.07855112850666046 Validation loss 0.07866030186414719\n",
            "Epoch 29650/30000  training loss: 0.07853716611862183 Validation loss 0.0786464586853981\n",
            "Epoch 29660/30000  training loss: 0.07852322608232498 Validation loss 0.07863260060548782\n",
            "Epoch 29670/30000  training loss: 0.07850926369428635 Validation loss 0.07861874997615814\n",
            "Epoch 29680/30000  training loss: 0.07849530130624771 Validation loss 0.07860489934682846\n",
            "Epoch 29690/30000  training loss: 0.07848134636878967 Validation loss 0.07859105616807938\n",
            "Epoch 29700/30000  training loss: 0.07846740633249283 Validation loss 0.07857722043991089\n",
            "Epoch 29710/30000  training loss: 0.07845345884561539 Validation loss 0.07856336981058121\n",
            "Epoch 29720/30000  training loss: 0.07843950390815735 Validation loss 0.07854953408241272\n",
            "Epoch 29730/30000  training loss: 0.0784255787730217 Validation loss 0.07853571325540543\n",
            "Epoch 29740/30000  training loss: 0.07841165363788605 Validation loss 0.07852189242839813\n",
            "Epoch 29750/30000  training loss: 0.0783977210521698 Validation loss 0.07850807160139084\n",
            "Epoch 29760/30000  training loss: 0.07838380336761475 Validation loss 0.07849424332380295\n",
            "Epoch 29770/30000  training loss: 0.0783698782324791 Validation loss 0.07848041504621506\n",
            "Epoch 29780/30000  training loss: 0.07835594564676285 Validation loss 0.07846658676862717\n",
            "Epoch 29790/30000  training loss: 0.0783420130610466 Validation loss 0.07845275849103928\n",
            "Epoch 29800/30000  training loss: 0.07832810282707214 Validation loss 0.07843894511461258\n",
            "Epoch 29810/30000  training loss: 0.07831417769193649 Validation loss 0.07842513918876648\n",
            "Epoch 29820/30000  training loss: 0.07830026000738144 Validation loss 0.07841130346059799\n",
            "Epoch 29830/30000  training loss: 0.07828634232282639 Validation loss 0.0783974900841713\n",
            "Epoch 29840/30000  training loss: 0.07827243208885193 Validation loss 0.0783836841583252\n",
            "Epoch 29850/30000  training loss: 0.07825852185487747 Validation loss 0.0783698707818985\n",
            "Epoch 29860/30000  training loss: 0.07824461162090302 Validation loss 0.0783560574054718\n",
            "Epoch 29870/30000  training loss: 0.07823072373867035 Validation loss 0.07834227383136749\n",
            "Epoch 29880/30000  training loss: 0.07821688055992126 Validation loss 0.07832852005958557\n",
            "Epoch 29890/30000  training loss: 0.07820302993059158 Validation loss 0.07831478118896484\n",
            "Epoch 29900/30000  training loss: 0.0781891793012619 Validation loss 0.07830101996660233\n",
            "Epoch 29910/30000  training loss: 0.07817532122135162 Validation loss 0.0782872661948204\n",
            "Epoch 29920/30000  training loss: 0.07816150784492493 Validation loss 0.07827351987361908\n",
            "Epoch 29930/30000  training loss: 0.07814767211675644 Validation loss 0.07825979590415955\n",
            "Epoch 29940/30000  training loss: 0.07813384383916855 Validation loss 0.07824606448411942\n",
            "Epoch 29950/30000  training loss: 0.07812002301216125 Validation loss 0.07823231816291809\n",
            "Epoch 29960/30000  training loss: 0.07810620218515396 Validation loss 0.07821858674287796\n",
            "Epoch 29970/30000  training loss: 0.07809238135814667 Validation loss 0.07820485532283783\n",
            "Epoch 29980/30000  training loss: 0.07807855308055878 Validation loss 0.07819113880395889\n",
            "Epoch 29990/30000  training loss: 0.07806473970413208 Validation loss 0.07817739248275757\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Mean Square Error')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsHUlEQVR4nO3dd3gU5d7G8e/upvdAQhIgEAKh9yogWECKShMVEaWIoIhYEEVUmg1E9KCiqCggFqqAHOkgTYrUQOgtEEpCT0J62Xn/yHHfk0MxgSSbcn+ua65rd+bZ2d8OSfZm5pnnMRmGYSAiIiJSgpjtXYCIiIhIQVMAEhERkRJHAUhERERKHAUgERERKXEUgERERKTEUQASERGREkcBSEREREocB3sXUBhZrVbOnTuHp6cnJpPJ3uWIiIhIDhiGwbVr1yhbtixm863P8SgA3cC5c+cIDg62dxkiIiJyG06fPk358uVv2UYB6AY8PT2BrAPo5eVl52pEREQkJ+Lj4wkODrZ9j9+KAtAN/H3Zy8vLSwFIRESkiMlJ9xV1ghYREZESRwFIREREShwFIBERESlx1AdIRETynNVqJS0tzd5lSDHj6OiIxWLJk30pAImISJ5KS0sjMjISq9Vq71KkGPLx8SEwMPCOx+lTABIRkTxjGAbR0dFYLBaCg4P/cTA6kZwyDIOkpCQuXLgAQFBQ0B3tTwFIRETyTEZGBklJSZQtWxY3Nzd7lyPFjKurKwAXLlygTJkyd3Q5TNFcRETyTGZmJgBOTk52rkSKq7+DdXp6+h3tRwFIRETynOZRlPySVz9bCkAiIiJS4igAiYiISImjACQiInIHQkJCmDRpUo7br1u3DpPJRGxsbL7VJP9MAagAWa0Gaw9dwDAMe5ciIiL/YTKZbrmMGTPmlq/fvn07AwcOzPH7tWjRgujoaLy9ve+w8ltT0Lo13QZfgObuOM2bCyJoWaU0YzvXpkoZD3uXJCJS4kVHR9sez5kzh1GjRnH48GHbOg+PW/+t9vf3z9X7OTk5ERgYmLsiJc/pDFABSkzLxNnBzKZjl+n42QbGLztEUlqGvcsSEck3hmGQlJZhlyWnZ9sDAwNti7e3NyaTyfY8MTGRXr16ERAQgIeHB02aNGH16tXZXv+/l8BMJhPfffcd3bp1w83NjbCwMBYvXmzb/r9nZmbMmIGPjw8rVqygRo0aeHh40KFDh2zBLCMjg5deegkfHx9Kly7N8OHD6dOnD127dr3tf5urV6/Su3dvfH19cXNzo2PHjhw9etS2/dSpU3Tq1AlfX1/c3d2pVasWS5cutb22V69e+Pv74+rqSlhYGNOnT7/tWuxBZ4AKUP+7K/FAjQDG/ns/aw5d4Ov1x1kcfpaRD9ekQ+07H9ZbRKSwSU7PpOaoFXZ57wPvtsfN6c6+5hISEnjwwQf54IMPcHZ2ZubMmXTq1InDhw9ToUKFm75u7NixTJgwgY8//pgvvviCXr16cerUKUqVKnXD9klJSUycOJEff/wRs9nMU089xbBhw/j5558B+Oijj/j555+ZPn06NWrU4LPPPmPRokXcd999t/3Z+vbty9GjR1m8eDFeXl4MHz6cBx98kAMHDuDo6MjgwYNJS0tjw4YNuLu7c+DAAdvZsJEjR3LgwAGWLVuGn58fx44dIzk5+bZrsQcFoAJWobQb3/dtwuoD5xnz7/2cuZrMoJ930bqqP2M61STUX5fFREQKi3r16lGvXj3b8/fee4+FCxeyePFiXnzxxZu+rm/fvvTs2ROADz/8kM8//5xt27bRoUOHG7ZPT0/n66+/pnLlygC8+OKLvPvuu7btX3zxBSNGjKBbt24ATJ482XY25nb8HXw2bdpEixYtAPj5558JDg5m0aJFPPbYY0RFRdG9e3fq1KkDQGhoqO31UVFRNGjQgMaNGwNZZ8GKGgUgO2lbM4CWVfyYsu4YX68/wYYjF+kwaSND7q/Cc/dUxslBVydFpOhzdbRw4N32dnvvO5WQkMCYMWNYsmQJ0dHRZGRkkJycTFRU1C1fV7duXdtjd3d3vLy8bHNY3Yibm5st/EDWPFd/t4+Li+P8+fM0bdrUtt1isdCoUaPbnnD24MGDODg40KxZM9u60qVLU61aNQ4ePAjASy+9xKBBg1i5ciVt27ale/futs81aNAgunfvzq5du2jXrh1du3a1BamiQt+yBe3aedtDVycLQ9tVY+WrrWld1Z+0TCufrDpCpy/+ZFfUVTsWKSKSN0wmE25ODnZZ8qJbwbBhw1i4cCEffvghGzduJDw8nDp16pCWlnbL1zk6Ol53HG4VVm7U3t53DD/77LOcOHGCp59+moiICBo3bswXX3wBQMeOHTl16hSvvvoq586do02bNgwbNsyu9eaWAlBBOrEOJtWGNe9BWpJtdYifOz/0a8JnT9SntLsTh89fo/uUzYxZvJ+EVHWSFhGxl02bNtG3b1+6detGnTp1CAwM5OTJkwVag7e3NwEBAWzfvt22LjMzk127dt32PmvUqEFGRgZ//fWXbd3ly5c5fPgwNWvWtK0LDg7m+eefZ8GCBbz22mtMnTrVts3f358+ffrw008/MWnSJL799tvbrscedAmsIB1aAplpsHEiRMyFjhOgWkcgK+13qV+O1mH+vL/kIL/uOsOMzSdZuT+G97vV5v7qAXYuXkSk5AkLC2PBggV06tQJk8nEyJEjb/uy050YMmQI48aNo0qVKlSvXp0vvviCq1ev5ugsV0REBJ6enrbnJpOJevXq0aVLFwYMGMA333yDp6cnb775JuXKlaNLly4AvPLKK3Ts2JGqVaty9epV1q5dS40aNQAYNWoUjRo1olatWqSmpvL777/bthUVCkAFqeMEqNQalr0JsVEw6wmo9iB0GA++FQHwdXfik8fr0bVBWd5aGMHpK8k8M2MHXeuXZWzn2ni7Of7Dm4iISF759NNPeeaZZ2jRogV+fn4MHz6c+Pj4Aq9j+PDhxMTE0Lt3bywWCwMHDqR9+/ZYLP/cz6l169bZnlssFjIyMpg+fTovv/wyDz/8MGlpabRu3ZqlS5faLsdlZmYyePBgzpw5g5eXFx06dOBf//oXkDWW0YgRIzh58iSurq60atWK2bNn5/0Hz0cmw94XGQuh+Ph4vL29iYuLw8vLK+/fIC0R1k+ALZPBmgEOrtD6NWjxEjg425olpWUwafVRvtt4AqsBZTyd+ah7Xe6rXibvaxIRyQMpKSlERkZSqVIlXFxc7F1OsWW1WqlRowaPP/447733nr3LKVC3+hnLzfe3+gDZg5M7PDAWnt8EIa0gIxn+eB+mtMjqJ/Qfbk4OvPVgDX4d1IJQf3cuXEul34ztvDF/D/Ep6farX0RECtSpU6eYOnUqR44cISIigkGDBhEZGcmTTz5p79KKLAUgeypTHfr8Gx75DtzLwOVjMLMLLBwESVdszRpU8GXpS6149u5KmEwwd8cZOvxrA38evWTH4kVEpKCYzWZmzJhBkyZNaNmyJREREaxevbrI9bspTHQJ7Aby/RLYjaTEZZ0F2jYVMMDdHx78GGp2hf/q5LYt8grD5u0h6krWXWRP3VWBtx6sccejnYqI5AVdApP8pktgxY2Ld1bg6b8S/KpB4kWY1xdm94L4/58PpmmlUix7uRVP35XVafqnrVE8/MWf7DsbZ6fCRUREih4FoMImuCk8vxHuGQ5mRzi8BL5sCjumw39uvXR3duC9rrX5qX8zArycOXExkW5fbWLqhhNYrTqhJyIi8k8UgAojB2e47y14bgOUawSp8fD7K/BTN4g9bWt2d5gfy19uTbuaAaRnGnyw9CB9pm/jQnyK/WoXEREpAhSACrOAmtB/FbQfl3Wr/Il1WXeK7f4Z/tN1y9fdiW+ebsSH3erg4mhm49FLdPhsI6sPnL/1vkVEREowBaDCzmyB5i/AoE1QvmnW2aDfXoBZPW3ziplMJp5sVoHfh7SiVlkvriSm8ezMHYz+bR+pGZl2/gAiIiKFjwJQUVG6MjyzHNqOBYsTHFkGXzWDfQtsTaqU8WDBCy0Y0KoSAD9sOcVjX2/h9JWkm+1VRETyyL333ssrr7xiex4SEsKkSZNu+RqTycSiRYvu+L3zaj8liQJQUWK2wN2vwMD1EFgXkq/C/H6w8HlIvQaAs4OFtx+qyfS+TfBxc2TvmTge+nwjK/fH2Ld2EZFCqlOnTnTo0OGG2zZu3IjJZGLv3r253u/27dsZOHDgnZaXzZgxY6hfv/5166Ojo+nYsWOevtf/mjFjBj4+Pvn6HgVJAagoCqgJA/6A1m+AyQx7ZsHXd8OZHbYm91Uvw5KXWtGggg/xKRkM/HEn7/9+gPTMgp/ET0SkMOvfvz+rVq3izJkz122bPn06jRs3pm7durner7+/P25ubnlR4j8KDAzE2dn5nxuKjQJQUWVxhPvfhn7LwLsCXD0J37eDDR+DNavfTzkfV+YMbM6zd2ddEvvuz0ge/2YL52KT7Vi4iEjh8vDDD+Pv78+MGTOyrU9ISGDevHn079+fy5cv07NnT8qVK4ebmxt16tRh1qxZt9zv/14CO3r0KK1bt8bFxYWaNWuyatWq614zfPhwqlatipubG6GhoYwcOZL09Kypj2bMmMHYsWPZs2cPJpMJk8lkq/l/L4FFRERw//334+rqSunSpRk4cCAJCQm27X379qVr165MnDiRoKAgSpcuzeDBg23vdTuioqLo0qULHh4eeHl58fjjj3P+/P/fkLNnzx7uu+8+PD098fLyolGjRuzYkfUf91OnTtGpUyd8fX1xd3enVq1aLF269LZryQkNH1zUVbgra9ygJUNh369Zo0kfXwvdvgGfYJwczLzzcE2aVCrFsHl72B0VS6cv/uSrXg1pFlra3tWLSHFnGJBup36Ijm7ZRtK/GQcHB3r37s2MGTN4++23Mf3nNfPmzSMzM5OePXuSkJBAo0aNGD58OF5eXixZsoSnn36aypUr07Rp0398D6vVyiOPPEJAQAB//fUXcXFx2foL/c3T05MZM2ZQtmxZIiIiGDBgAJ6enrzxxhv06NGDffv2sXz5clavXg2At7f3dftITEykffv2NG/enO3bt3PhwgWeffZZXnzxxWwhb+3atQQFBbF27VqOHTtGjx49qF+/PgMGDPjHz3Ojz/d3+Fm/fj0ZGRkMHjyYHj16sG7dOgB69epFgwYNmDJlChaLhfDwcNvM84MHDyYtLY0NGzbg7u7OgQMH8PDwyHUduaEAVBy4+kD376HKA7B0GJzaBF+3hC5fQo1OALSvFUiNQC+e/2knB6Lj6fXdX4x8uCa9m1e0/bKLiOS59CT4sKx93vutc1mTT+fAM888w8cff8z69eu59957gazLX927d8fb2xtvb2+GDRtmaz9kyBBWrFjB3LlzcxSAVq9ezaFDh1ixYgVly2Ydjw8//PC6fjvvvPOO7XFISAjDhg1j9uzZvPHGG7i6uuLh4YGDgwOBgYE3fa9ffvmFlJQUZs6cibt71uefPHkynTp14qOPPiIgIAAAX19fJk+ejMVioXr16jz00EOsWbPmtgLQmjVriIiIIDIykuDgYABmzpxJrVq12L59O02aNCEqKorXX3+d6tWrAxAWFmZ7fVRUFN27d6dOnToAhIaG5rqG3NIlsOLCZIL6PbPOBpVrnDW32JynYPlbkJEGQIXSbvw6qAWd65Ulw2owevF+3pi/l5R03SovIiVb9erVadGiBdOmTQPg2LFjbNy4kf79+wOQmZnJe++9R506dShVqhQeHh6sWLGCqKioHO3/4MGDBAcH28IPQPPmza9rN2fOHFq2bElgYCAeHh688847OX6P/36vevXq2cIPQMuWLbFarRw+fNi2rlatWlgsFtvzoKAgLly4kKv3+u/3DA4OtoUfgJo1a+Lj48PBgwcBGDp0KM8++yxt27Zl/PjxHD9+3Nb2pZde4v3336dly5aMHj36tjqd55bOABU3pUKzbpdfPQa2TIatX8KZbfDodPAJxtXJwmdP1KdOOW/GLTvIvJ1nOHIhga+fakiQt6u9qxeR4sbRLetMjL3eOxf69+/PkCFD+PLLL5k+fTqVK1fmnnvuAeDjjz/ms88+Y9KkSdSpUwd3d3deeeUV0tLS8qzcLVu20KtXL8aOHUv79u3x9vZm9uzZfPLJJ3n2Hv/t78tPfzOZTFit+XejzJgxY3jyySdZsmQJy5YtY/To0cyePZtu3brx7LPP0r59e5YsWcLKlSsZN24cn3zyCUOGDMm3enQGqDiyOEL7D+CJX7ImWT2zHb5pBUezOtyZTCYGtA7lh2ea4uPmyJ7TsXT6YhPbT16xc+EiUuyYTFmXoeyx5PLy/uOPP47ZbOaXX35h5syZPPPMM7YuAps2baJLly489dRT1KtXj9DQUI4cOZLjfdeoUYPTp08THf3/k1tv3bo1W5vNmzdTsWJF3n77bRo3bkxYWBinTp3K1sbJyYnMzFufta9RowZ79uwhMTHRtm7Tpk2YzWaqVauW45pz4+/Pd/r0/0/XdODAAWJjY6lZs6ZtXdWqVXn11VdZuXIljzzyCNOnT7dtCw4O5vnnn2fBggW89tprTJ06NV9q/ZsCUHFW/aGs+cSC6meNGfTzo7DmXcjMAKBVmD+LB99N9UBPLiWk0vPbrfy09dSt9ykiUkx5eHjQo0cPRowYQXR0NH379rVtCwsLY9WqVWzevJmDBw/y3HPPZbvD6Z+0bduWqlWr0qdPH/bs2cPGjRt5++23s7UJCwsjKiqK2bNnc/z4cT7//HMWLlyYrU1ISAiRkZGEh4dz6dIlUlNTr3uvXr164eLiQp8+fdi3bx9r165lyJAhPP3007b+P7crMzOT8PDwbMvBgwdp27YtderUoVevXuzatYtt27bRu3dv7rnnHho3bkxycjIvvvgi69at49SpU2zatInt27dTo0YNAF555RVWrFhBZGQku3btYu3atbZt+UUBqLjzDYH+K6HJfzq1bfwEZnaxTaNRobQbC15owcN1g8iwGryzaB9jFu8nQ+MFiUgJ1L9/f65evUr79u2z9dd55513aNiwIe3bt+fee+8lMDCQrl275ni/ZrOZhQsXkpycTNOmTXn22Wf54IMPsrXp3Lkzr776Ki+++CL169dn8+bNjBw5Mlub7t2706FDB+677z78/f1veCu+m5sbK1as4MqVKzRp0oRHH32UNm3aMHny5NwdjBtISEigQYMG2ZZOnTphMpn47bff8PX1pXXr1rRt25bQ0FDmzJkDgMVi4fLly/Tu3ZuqVavy+OOP07FjR8aOHQtkBavBgwdTo0YNOnToQNWqVfnqq6/uuN5bMRnGf2bVFJv4+Hi8vb2Ji4vDy8vL3uXknX2/wuKXIC0BPMtCj5+gfCMADMNgyvrjTFie1UHuvmr+fPFkQzyc1U1MRHIuJSWFyMhIKlWqhIuLi73LkWLoVj9jufn+1hmgkqR2dxi4DvyqwbVzML1j1szyZPULeuHeKnz9VENcHM2sPXyRR6ds5qwGTRQRkWJIAaik8QuDZ1dDtYcgMzVrZvmlb0Bm1uifHWoHMWdgc/w9nTkUc40ukzex53SsfWsWERHJYwpAJZGLV9blr3tHZD3f9g3M7AoJFwGoF+zDosEtbZ2je3y7heX7om++PxERkSJGAaikMpvh3jezbpV38oRTf8K390L0HiBrHrH5g1pwXzV/UtKtPP/TLr7beMK+NYuIiOQRBaCSrvpDMGANlK4C8WdgWgc4+DsAHs4OTO3dmD7NKwLw/pKDfLj0IFar+s2LyK3p/hrJL3n1s6UAJOBfDZ5dA5Xvz5q3Z85TsOkzMAwcLGbGdK7Fmx2z5m75dsMJhs4NJy1Dt8mLyPX+nlohL0dIFvlvSUlZk+v+70jWuaV7nCWLqw88OQ+WvQE7vodVo+DSEXjoX5gcnHj+nsqU8XTmjfl7WRR+jksJaXz9dCPdJi8i2Tg4OODm5sbFixdxdHTEbNb/syVvGIZBUlISFy5cwMfHJ9s8ZrdD4wDdQLEdBygnDAO2fQvL3wTDCiGt4PGZ4FYKgPVHLjLop50kpWVSu5wX0/o2oYynxvoQkf+XlpZGZGRkvs4rJSWXj48PgYGBtmlK/ltuvr8VgG6gRAegvx1ZCfP7ZQ2aWKoy9JoHpSsDsPdMLP2mb+dyYhrBpVz5qX8zKpZ2/4cdikhJYrVadRlM8pyjo+Mtz/woAN0hBaD/OL8ffukBcafBrXTWJbL/jBx98lIivadtI+pKEmU8nfnp2WZUDfC0c8EiIlKSaSRoyRsBtbI6RwfVg6TL8MPDcGQFACF+7swf1JzqgZ5cuJbK499sYe+ZWPvWKyIikkMKQHJrngHQdwlUbpN1h9isnrDzBwDKeLowe+Bd1Av2ITYpnSen/sVfJy7buWAREZF/pgAk/8zZE56cA/WeBCMT/v0SrBsPhoGPmxM/P9uMu0JLkZCaQe9p21h3+IK9KxYREbklBSDJGYsjdP0KWg3Ler5uXFYQyszAw9mBGf2acn/1MqRmWBkwcwdLIzR1hoiIFF4KQJJzJhO0GQkPfQomM+yaCfP6QEYqLo4Wvnm6EQ/XDSI90+DFX3axYNcZe1csIiJyQwpAkntN+mdNpmpxhkO/wy+PQ2oCjhYznz3RgCeaBGM14LV5e5i/UyFIREQKHwUguT3VH4Kn5oOTB5xYBz92heSrWMwmPuxWh6fuqoBhwOvz9zBvx2l7VysiIpKNApDcvkqtofdv4OIDZ7bDjIch4QJms4n3utTm6bsqYhjwxq97mbtdIUhERAoPBSC5M+UbQ79l4BEA5/dlzSYfG4XJZOLdLrXo0/z/Q9Cc7VH2rlZERARQAJK8EFAzKwR5V4Arx2FaR7h0FJPJxJjOtejbIgSA4b9GMGubQpCIiNifApDkjdKV4Znl4FcV4s9knQk6vx+TycToTjXp1zIEgBELInQmSERE7E4BSPKOd7msM0FB9SDpEvzQCWIiMJlMjHq4Js+0rATAmwsi+C38rJ2LFRGRkkwBSPKWu19Wx+iyDf8zf1gniN6LyWRi5MM1bHeHDZ27h+X7YuxdrYiIlFAKQJL3XH3h6YVQrhEkX80KQefCszpGd65N94blybQaDJm1i7WaNkNEROxAAUjyh6tPVggq3wRSYmFmZzi3G7PZxIRH6/LQf0aMfv7HnWw+dsne1YqISAmjACT5x8UbnloAwc0gJQ5+6AJnd2Ixm5jUoz5tawSQmmHl2Zk72Hnqir2rFRGREkQBSPKXixc89SsE3wWpcTCzK5zdhaPFzOQnG9AqzI+ktEz6TtvOvrNx9q5WRERKCAUgyX/OnlkhqEILSI2HH7tBzD5cHC18+3RjmlYqxbXUDPpO38bJS4n2rlZEREqAQhGAvvzyS0JCQnBxcaFZs2Zs27btpm2nTp1Kq1at8PX1xdfXl7Zt217Xvm/fvphMpmxLhw4d8vtjyK04e0CvuVCucVafoB+7wsUjuDpZ+K5PY2oGeXEpIY3e07ZxIT7F3tWKiEgxZ/cANGfOHIYOHcro0aPZtWsX9erVo3379ly4cOO7g9atW0fPnj1Zu3YtW7ZsITg4mHbt2nH2bPZxZTp06EB0dLRtmTVrVkF8HLmVv88EBdaFxItZHaOvnMDLxZEfnmlKxdJuRF1Jos/07cQlp9u7WhERKcZMhmEY9iygWbNmNGnShMmTJwNgtVoJDg5myJAhvPnmm//4+szMTHx9fZk8eTK9e/cGss4AxcbGsmjRotuqKT4+Hm9vb+Li4vDy8rqtfcgtJF6GGQ/BxYNZ02f0Wwo+wURdTqL715u5eC2VpiGlmNm/KS6OFntXKyIiRURuvr/tegYoLS2NnTt30rZtW9s6s9lM27Zt2bJlS472kZSURHp6OqVKlcq2ft26dZQpU4Zq1aoxaNAgLl++fNN9pKamEh8fn22RfOReOmuwxFKVIS4q60zQtRgqlHbjh35N8XR2YNvJK7z4y24yMq32rlZERIohuwagS5cukZmZSUBAQLb1AQEBxMTkbJTg4cOHU7Zs2WwhqkOHDsycOZM1a9bw0UcfsX79ejp27EhmZuYN9zFu3Di8vb1tS3Bw8O1/KMkZzwDosxh8KsCVEzCzCyReomZZL77r0xgnBzOrD55nxIII7HySUkREiiG79wG6E+PHj2f27NksXLgQFxcX2/onnniCzp07U6dOHbp27crvv//O9u3bWbdu3Q33M2LECOLi4mzL6dOnC+gTlHDe5aH3YvAsCxcPwc+PQuo1moWWZnLPBphNMG/nGSasOGzvSkVEpJixawDy8/PDYrFw/vz5bOvPnz9PYGDgLV87ceJExo8fz8qVK6lbt+4t24aGhuLn58exY8duuN3Z2RkvL69sixSQUpWyzgS5lYZzu2F2L8hIpV2tQMY/kvXvOmXdcX7+65SdCxURkeLErgHIycmJRo0asWbNGts6q9XKmjVraN68+U1fN2HCBN577z2WL19O48aN//F9zpw5w+XLlwkKCsqTuiWP+YVBr/ng5AGR62HBALBm8niTYF5pGwbAyEX7WHtI84aJiEjesPslsKFDhzJ16lR++OEHDh48yKBBg0hMTKRfv34A9O7dmxEjRtjaf/TRR4wcOZJp06YREhJCTEwMMTExJCQkAJCQkMDrr7/O1q1bOXnyJGvWrKFLly5UqVKF9u3b2+UzSg6Uawg9fgKLExz4DZYOA8Pg5TZhPNqoPFYDBv+yS6NFi4hInrB7AOrRowcTJ05k1KhR1K9fn/DwcJYvX27rGB0VFUV0dLSt/ZQpU0hLS+PRRx8lKCjItkycOBEAi8XC3r176dy5M1WrVqV///40atSIjRs34uzsbJfPKDlU+T545FvABDumwbpxmEwmxj1Sh7urZE2Z0W/Gds7GJtu7UhERKeLsPg5QYaRxgOxs+3ew5LWsxx0/hmYDiU9J5/Gvt3Ao5hpVAzyY93wLvF0d7VuniIgUKkVmHCCRG2ryLNz7VtbjZW9AxHy8XByZ1rcJAV7OHDmfwKCfdpKWoTGCRETk9igASeF0zxvQdCBgwKJBcPJPyvq4Mq1vE9ydLGw+flljBImIyG1TAJLCyWSCDh9Bjc6QmQazn4SLh6lV1psvezXEYjbx664zfL3+hL0rFRGRIkgBSAovszmrU3T5ppASlzVQ4rXz3FutDKM71QRgwopDrDpw/h92JCIikp0CkBRujq7QczaUCoXYKPjlcUhLpHfzEJ66qwKGAS/P3s3BaM3fJiIiOacAJIWfe+msgRLdSkN0OMx/BjIzGN2pFi2rlCYpLZNnf9jBpYRUe1cqIiJFhAKQFA2lK2edCXJwgSPLYdkbOJpNfPlkQ0JKu3E2Npnnf9xJasaNJ7wVERH5bwpAUnQEN4VHppI1UOL3sOkzfNyc+K5PEzxdHNhx6ipvL9ynO8NEROQfKQBJ0VKzM7T/MOvx6tFwYDFVynjw5ZNZd4bN33mGqRt1Z5iIiNyaApAUPc1f+M8YQcDC5yB6D62r+jPyoRoAjFt2iD8O6c4wERG5OQUgKZraj4PK90N6EszqCddi6NMihCeb/efOsFnhnLiYYO8qRUSkkFIAkqLJ4gCPTofSYRB/Fmb3wpSRwphOtWhc0ZdrqRk89+NOElIz7F2piIgUQgpAUnS5+sCTc8DFB87ugN9exMli4qunGhLg5czRCwkMm7tHnaJFROQ6CkBStJWuDD1+BLMD7JsPGyZSxtOFKU81wtFiYvn+GL5ad9zeVYqISCGjACRFX6XW8NAnWY/Xvg/7F9Gwgi9jO9cGYOLKw6w7fMGOBYqISGGjACTFQ6O+cNcLWY8XPg/nwnmyWQV6Ng3GMOClWbs5dTnRriWKiEjhoQAkxccD70GVByAjGeY8BYmXGNO5FvWDfYhPyeoUnZSmTtEiIqIAJMWJxQG6fwelKkPcaZjXF2eTwddPNcLPw5lDMdd4Y/5edYoWEREFIClmXH3giV/AyQNOboRVIwn0dmHKUw1xMJv4fW80M7ecsneVIiJiZwpAUvyUqQ7dvs56vPUr2DObJiGleLNjdQDeX3KA8NOx9qtPRETsTgFIiqcanaD161mP//0ynAun/92V6FArkPRMg8E/7+JqYpp9axQREbtRAJLi694RENYOMlJgzlOYki4z4bG6hJR242xsMkPnhmO1qj+QiEhJpAAkxZfZAo9MzdYp2svRxJe9GuLsYGbt4YtMWa9BEkVESiIFICnebtApulZZb97tUguAT1YeZvPxS/atUURECpwCkBR/ZapD1ylZj7d+Bft+5fHGwTzaqDxWA16aFc6F+BT71igiIgVKAUhKhpqd4e6hWY8Xv4Tp0lHe61Kb6oGeXEpI5cVZu8nItNq3RhERKTAKQFJy3Pc2hLSCtASY2xtXUviyV0PcnSxsi7zCp6uO2LtCEREpIApAUnJYHKD79+ARCBcPwr9fobKfOx89WheAKeuPs/HoRTsXKSIiBUEBSEoWzwB4dBqYLBAxF3ZO5+G6ZXmyWQUMA16ds4eL11LtXaWIiOQzBSApeUJaQtvRWY+XDYdzuxn1cE2qBWT1B9L4QCIixZ8CkJRMLV6Cag9BZhrM7Y1LehxfPNkAF0czG49eYurGE/auUERE8pECkJRMJhN0/Qp8QyA2ChYNoqq/O6M7ZY0P9PGKw+yOumrfGkVEJN8oAEnJ5eoDj88EizMcWQ6b/sUTTYJ5qG4QGVaDIbN2E5ecbu8qRUQkHygASckWVA8e/Djr8R/vY4rawrhH6lDe15UzV5N5a0EEhqH+QCIixY0CkEjD3lD3CTCs8OuzeFmv8UXPBjiYTSyJiGb29tP2rlBERPKYApCIyQQPfQKlq0D8WVg0iAbBPgxrXw2AMYv3c+T8NTsXKSIieUkBSATA2QMenf7//YG2TmFgq1BahfmRmmFlyC+7SUnPtHeVIiKSR3IVgNLT03nmmWeIjIzMr3pE7CeoLrT/IOvxqlGYo3fz6eP18fNw4vD5a0xYfti+9YmISJ7JVQBydHTk119/za9aROyvybNQ/WGwpsP8Z/B3TOWj7llTZUzbFMmGI5oqQ0SkOMj1JbCuXbuyaNGifChFpBAwmaDLZPCuAFcj4fdXaFO9DE/dVQGAYfP2cDUxzc5FiojInXLI7QvCwsJ499132bRpE40aNcLd3T3b9pdeeinPihOxC1dfePR7mNYB9v0Kle7h7QefYsvxyxy/mMiIBRFMeaohJpPJ3pWKiMhtMhm5HOSkUqVKN9+ZycSJE0V/CoH4+Hi8vb2Ji4vDy8vL3uWIvfz5L1g9BhxcYeBa9qWXpdtXm0jPNJjQvS6PNwm2d4UiIvJfcvP9nesAVBIoAAkAViv8/CgcXwP+NWDgWqZsOsdHyw/h5mRh6UutCPFz/+f9iIhIgcjN9/cd3QZvGIZGyZXiy2yGbt+Aexm4eBBWjWZg61CaVSpFUlomr8wJJz3Tau8qRUTkNtxWAJo5cyZ16tTB1dUVV1dX6taty48//pjXtYnYn4c/dJ2S9XjbN1iOreLTHvXxdHEg/HQsX/xxzL71iYjIbcl1APr0008ZNGgQDz74IHPnzmXu3Ll06NCB559/nn/961/5UaOIfYW1hWbPZz3+7QXKOVzjg251AJj8x1F2nrpix+JEROR23FYn6LFjx9K7d+9s63/44QfGjBlTLAZJVB8guU56Cky9Hy7sh7B28ORcXp27h4W7zxJcypWlL7XC08XR3lWKiJRo+doHKDo6mhYtWly3vkWLFkRHR+d2dyJFg6MLdP8ua6qMoyth27eM7VKLcj6unL6SzLv/PmDvCkVEJBdyHYCqVKnC3Llzr1s/Z84cwsLC8qQokUIpoCa0ey/r8cqReMUd5V896mMywbydZ1h94Lx96xMRkRzL9UCIY8eOpUePHmzYsIGWLVsCsGnTJtasWXPDYCRSrDQdCMdWZ50F+rU/TQesZUCrUL7dcII3F0SwsqIvpdyd7F2liIj8g1yfAerevTvbtm3Dz8+PRYsWsWjRIvz8/Ni2bRvdunXLjxpFCg+TCbp8Ce7+cOEArB7N0AeqUjXAg0sJqbyzKEJDQ4iIFAG56gSdnp7Oc889x8iRI285InRRp07Q8o+OrIRfHst63Gs++9ya0vXLTWRYDT57oj5d6pezb30iIiVQvnWC1mzwIv9RtR00fS7r8aIXqO2TzpD7s/rAjVy0j5i4FDsWJyIi/0SzwYvcrgfezZoiI/EC/P4KL9wbSt3y3sSnZDD81726FCYiUohpNniR2+XoAo98A1PbwMF/47hvLp8+/jAPfv4n649c5JdtUfRqVtHeVYqIyA1oNvgbUB8gyZUNE+GP98DZCwZt5ruIdN5fchA3JwvLXm5FxdKaMFVEpCDk22zwhmEQFRVFmTJlcHV1veNCCysFIMmVzAyY3hHObIOQVlif/o2e323jr8grNAnxZfbA5ljMJntXKSJS7OVbJ2jDMAgLC+PMmTN3VKBIsWJxgG5fg6M7nNyIedvXTHysHu5OFrafvMr3fxb9s6IiIsVNrgKQ2WwmLCyMy5cv51c9IkVT6crQ/v2sx6vHEpxxipEP1wRg4oojHDl/zY7FiYjI/8r1XWDjx4/n9ddfZ9++fflRj0jR1ahf1kSpmamwYCA9GgZwf/UypGVaeW3uHjIyrfauUERE/iPXnaB9fX1JSkoiIyMDJyen6/oCXblyJU8LtAf1AZLbdi0GvmoOyVeg1TDON3mdBz5dT3xKBq+3r8bg+6rYu0IRkWIrN9/fub4NftKkSbdbl0jx5xkID/8L5vWBPz8loGp7RneqxWvz9vDZ6qM8UDOAqgGe9q5SRKTEy/UZoJJAZ4Dkji0YCHvnQKlQjOc28uysg6w5dIG65b1ZMKgFDpZcX30WEZF/kC93gc2dO5e0tDTb8zNnzmC1/n+fhqSkJCZMmHAb5YoUQx0ngFc5uHIC06qRfPhIHbxcHNh7Jo5vN+quMBERe8txAOrZsyexsbG25zVr1uTkyZO259euXWPEiBF5WZtI0eXqA12/ynq8YxoB5zcyqlMtACatOspR3RUmImJXOQ5A/3ulTFfORP5B6L3QbFDW48VD6F7D3XZX2LD5e3VXmIiIHakjgkh+ajMKSleBa9GYVozgw2518HRxYM/pWKZujLR3dSIiJZYCkEh+cnKDrlPAZIY9swiM/oNR/xkg8V+rjuhSmIiIneTqNvgVK1bg7e0NgNVqZc2aNbYBEf+7f5CI/JfgptD8Rdj8Ofz+Co8O2sLSav6sPXyRYfP38uvzzXVXmIhIAcvVX90+ffrQtWtXunbtSnJyMs8995zted++fW+7iC+//JKQkBBcXFxo1qwZ27Ztu2nbqVOn0qpVK3x9ffH19aVt27bXtTcMg1GjRhEUFISrqytt27bl6NGjt12fyB27723wqwYJ5zEte4Nxj9S1XQr7/k9dChMRKWg5DkBWq/Ufl8zMzFwXMGfOHIYOHcro0aPZtWsX9erVo3379ly4cOGG7detW0fPnj1Zu3YtW7ZsITg4mHbt2nH27FlbmwkTJvD555/z9ddf89dff+Hu7k779u1JSUnJdX0iecLRBbpNAZMF9s0n8OwK21xhn6w6wrELuhQmIlKQ7D4QYrNmzWjSpAmTJ08GsoJWcHAwQ4YM4c033/zH12dmZuLr68vkyZPp3bs3hmFQtmxZXnvtNYYNGwZAXFwcAQEBzJgxgyeeeOIf96mBECXfrHkXNn4Cbn4YL2yl37xI1h2+SP1gH34d1AKL2WTvCkVEiqx8GQgxP6SlpbFz507atm1rW2c2m2nbti1btmzJ0T6SkpJIT0+nVKlSAERGRhITE5Ntn97e3jRr1uym+0xNTSU+Pj7bIpIv7hkOZWpB0iVMS4cx7pE6eDo7EH46lu//1ACJIiIFxa4B6NKlS2RmZhIQEJBtfUBAADExMTnax/Dhwylbtqwt8Pz9utzsc9y4cXh7e9uW4ODg3H4UkZxxcM4aINHsAAcWEXR6me1S2MSVRzh+McHOBYqIlAxF+taT8ePHM3v2bBYuXIiLi8tt72fEiBHExcXZltOnT+dhlSL/o2x9aJV1eZYlr/FYdSfuqepPWoaV4fP3YrVqkFERkfxm1wDk5+eHxWLh/Pnz2dafP3+ewMDAW7524sSJjB8/npUrV1K3bl3b+r9fl5t9Ojs74+XllW0RyVetXoPAOpB8BdPvr/Jht9q4O1nYceoqM7ectHd1IiLF3m0FoNjYWL777jtGjBjBlStXANi1a1e2O7FywsnJiUaNGrFmzRrbur/HF2revPlNXzdhwgTee+89li9fTuPGjbNtq1SpEoGBgdn2GR8fz19//XXLfYoUKAenrAESzY5weAnlTv/Omw/WAGDCisOcvpJk5wJFRIq3XAegvXv3UrVqVT766CMmTpxoGwBxwYIFtzUZ6tChQ5k6dSo//PADBw8eZNCgQSQmJtKvXz8AevfunW2/H330ESNHjmTatGmEhIQQExNDTEwMCQlZfSdMJhOvvPIK77//PosXLyYiIoLevXtTtmxZunbtmuv6RPJNYJ2sTtEAS1+nVw1HmlYqRVJaJiMWRGi+PRGRfJTrADR06FD69u3L0aNHs/W7efDBB9mwYUOuC+jRowcTJ05k1KhR1K9fn/DwcJYvX27rxBwVFUV0dLSt/ZQpU0hLS+PRRx8lKCjItkycONHW5o033mDIkCEMHDiQJk2akJCQwPLly++on5BIvrj7VQiqDymxmJe8ykeP1MHZwcyfxy4xb8cZe1cnIlJs5XocIG9vb3bt2kXlypXx9PRkz549hIaGcurUKapVq1YsBhvUOEBSoC4chG9aQ2YadPmKb6/dxYdLD+Hp4sDqofcQ4KXgLiKSE/k6DpCzs/MNx8k5cuQI/v7+ud2diJSpAfe9lfV4+Qj613WmXrAP11IyeHvhPl0KExHJB7kOQJ07d+bdd98lPT0dyOpzExUVxfDhw+nevXueFyhSIjQfAuUaQ2oclt9f5uPudXC0mFh98Dz/3hv9z68XEZFcyXUA+uSTT0hISKBMmTIkJydzzz33UKVKFTw9Pfnggw/yo0aR4s/ikHVXmMUZjq2m6rnfePG+MADGLN7P5YRUOxcoIlK83PZcYJs2bWLPnj0kJCTQsGHDbFNPFHXqAyR2s+kzWDUKnL1IG7iJzj+e5FDMNTrXK8vnPRvYuzoRkUItN9/fuQpA6enpuLq6Eh4eTu3ate+40MJKAUjsxpoJ09rDme1QuQ177/2erl9txmrA1N6NeaBmwD/vQ0SkhMq3TtCOjo5UqFCBzMzMOypQRG7CbMm6FObgAsfXUPfCYga0DgXg7YURxCWn27lAEZHiIdd9gN5++23eeust2wjQIpLH/MLg/neyHq94m1ebuFHJz50L11L5cMlB+9YmIlJM5LoPUIMGDTh27Bjp6elUrFgRd3f3bNt37dqVpwXagy6Bid1ZM2F6Rzj9F4Tex/ZW3/P4t1sxDPipfzPuDvOzd4UiIoVObr6/HXK7c00nIVIAzBbo8hV83RJOrKVJzcX0vqsJP2w5xZsL9rLilda4O+f611dERP7jtu8CK850BkgKjS1fwoq3wMmDpGf/5IFpkZyNTaZvixDGdK5l7+pERAqVfB0JWkQKULPnIfguSEvAbdnLjOuWdfflD1tOsuOk+uGJiNyuXAegzMxMJk6cSNOmTQkMDKRUqVLZFhHJQ2YLdP0KHFwhcj2t4//NY43KYxjwxq97SUnXHZkiIrcj1wFo7NixfPrpp/To0YO4uDiGDh3KI488gtlsZsyYMflQokgJV7oytB2d9XjVKEbe7YG/pzMnLiby2Zqj9q1NRKSIynUA+vnnn5k6dSqvvfYaDg4O9OzZk++++45Ro0axdevW/KhRRJo+BxVaQFoCXitf4YMuNQH4dsMJIs7E2bk4EZGiJ9cBKCYmhjp16gDg4eFBXFzWH9+HH36YJUuW5G11IpLFbIYuk8HRDSI30C5pCQ/XDSLTavD6/D2kZVjtXaGISJGS6wBUvnx5oqOzZqeuXLkyK1euBGD79u04OzvnbXUi8v9KV4a2Y7IerxrNe609KOXuxKGYa3y17phdSxMRKWpyHYC6devGmjVrABgyZAgjR44kLCyM3r1788wzz+R5gSLyX5oMgIp3Q3oivquHMrZTDQAm/3GMg9Hxdi5ORKTouONxgLZs2cKWLVsICwujU6dOeVWXXWkcICnUrkTClJaQnojRcQLPHW7EygPnqVPOm4UvtMDBotEtRKRkyrfZ4EsKBSAp9LZNhaXDwNGNy0+v5f7pUcQlp/NGh2q8cG8Ve1cnImIX+RqAZs6cecvtvXv3zs3uCiUFICn0rFaY2RlOboSKLfm1zte8Nj8CJwczS1+6myplPO1doYhIgcvXAOTr65vteXp6OklJSTg5OeHm5lYsZolXAJIi4epJ+KpF1qWwDuPpd7AR6w5fpEEFH+Y/3wKL2WTvCkVEClS+ToVx9erVbEtCQgKHDx/m7rvvZtasWbddtIjkkm8ItHsXANPqsUy43wNPZwd2R8UyfVOkfWsTESnk8qS3ZFhYGOPHj+fll1/Oi92JSE41egYqtYaMZMqseY23HqwGwMSVhzl5KdHOxYmIFF55druIg4MD586dy6vdiUhOmM3QeTI4eUDUZp6wLqVlldKkpFt549e9WK26x0FE5EYccvuCxYsXZ3tuGAbR0dFMnjyZli1b5llhIpJDvhWh3Xvw+6uY1rzLxCdX0yYqlm2RV/j5r1M83TzE3hWKiBQ6ue4EbTZnP2lkMpnw9/fn/vvv55NPPiEoKChPC7QHdYKWIscw4MeucGIdBN/FzOpfMerfh3BzsrDildYEl3Kzd4UiIvkuXztBW63WbEtmZiYxMTH88ssvxSL8iBRJJhN0/gKcPOH0Vp4yLaNpSCmS0jJ5a2EEGu5LRCQ7DRkrUlz4VID27wNg/uM9Pm3jhrODmY1HLzF3x2k7FyciUrjk+hLY0KFDc9z2008/zXVBhYEugUmRZRjw0yNw/A8o35Tvwr7i/WVH8HR2YOXQ1gR5u9q7QhGRfJOb7+9cd4LevXs3u3fvJj09nWrVsm65PXLkCBaLhYYNG9ramUwahE2kwJlM0Olz+Ko5nNnGMzWW8XtwI8JPx/LWggim9W2i300REW7jElinTp1o3bo1Z86cYdeuXezatYvTp09z33338fDDD7N27VrWrl3LH3/8kR/1isg/8QmGDh8CYP7jfSa1ccPJYmbt4YssCj9r5+JERAqHXF8CK1euHCtXrqRWrVrZ1u/bt4927doVi7GAdAlMijzDgJ8fhWOroXwTvgr9kgkrj+Ht6siqoa0p4+li7wpFRPJcvt4FFh8fz8WLF69bf/HiRa5du5bb3YlIfvj7UpizF5zZznOOy6hdzou45HRGLtqnu8JEpMTLdQDq1q0b/fr1Y8GCBZw5c4YzZ87w66+/0r9/fx555JH8qFFEbod3OegwDgDLug+ZdL8bDmYTK/afZ2lEjH1rExGxs1wHoK+//pqOHTvy5JNPUrFiRSpWrMiTTz5Jhw4d+Oqrr/KjRhG5XfV7QVg7yEylyubXefHeEABG/baPywmp9q1NRMSOct0H6G+JiYkcP34cgMqVK+Pu7p6nhdmT+gBJsRJ/Dr68C1LjyLh/NA/tbMzh89foXK8sn/dsYO/qRETyTL72Afqbu7s7devWxdvbm1OnTmG1Wm93VyKSn7zKQsfxADisH8fnbV2wmE0s3nOOlft1KUxESqYcB6Bp06ZdN7DhwIEDCQ0NpU6dOtSuXZvTpzXarEihVK8nhLWHzDSqbXmD51pVAODtRfuIS0q3c3EiIgUvxwHo22+/xdfX1/Z8+fLlTJ8+nZkzZ7J9+3Z8fHwYO3ZsvhQpInfIZIJOn4GLN5zbzauuy6ns787Fa6m8t+SAvasTESlwOQ5AR48epXHjxrbnv/32G126dKFXr140bNiQDz/8kDVr1uRLkSKSB7yCoOMEABw3fsTnbVwwmWD+zjOsPXzBzsWJiBSsHAeg5OTkbB2KNm/eTOvWrW3PQ0NDiYlRfwKRQq1uD6jaETLTqPXXcJ5tHgzAiF8jiEvWpTARKTlyHIAqVqzIzp07Abh06RL79++nZcuWtu0xMTF4e3vnfYUikndMJug0CVx8IDqcNzyXU8nPnZj4FMb+e7+9qxMRKTA5DkB9+vRh8ODBvPfeezz22GNUr16dRo0a2bZv3ryZ2rVr50uRIpKHPAPhwY8BcNw4gcltnDCbYMGus6zQXWEiUkLkOAC98cYbDBgwgAULFuDi4sK8efOybd+0aRM9e/bM8wJFJB/UeQyqPQTWdGr9NZzn764IwNsLIzRAooiUCLc9EGJxpoEQpUS4dh6+agbJV8m4ZwQPhbfg8PlrdKwdyFe9GmIymexdoYhIrhTIQIgiUsR5BkDHrEthDhsn8mUbRxzMJpbti2HxnnN2Lk5EJH8pAImUZHUeheoPgzWdKptf55X7KgEwctE+zsen2Lk4EZH8owAkUpKZTPDwv8C1FMREMMiyiLrlvYlPyWD4r3vRFXIRKa4UgERKOo8ytrvCLH9OZPL9Djg5mFl3+CKzt2t6GxEpnhSARARqd4cancGaQYX1r/HmA6EAvP/7AU5fSbJzcSIiec8hty/IzMxkxowZrFmzhgsXLlw3C/wff/yRZ8WJSAExmeChT+HUJji/j77V5rM85H62nbzCsHl7mDXgLsxm3RUmIsVHrs8Avfzyy7z88stkZmZSu3Zt6tWrl20RkSLKwx8enAiAeeMnfN7KipuThb8irzB980n71iYiksdyPQ6Qn58fM2fO5MEHH8yvmuxO4wBJiTb/Gdj3K5QKZU6jXxj+7xM4OZj5fcjdVA3wtHd1IiI3la/jADk5OVGlSpXbLk5ECrmHPgWv8nDlBI9f+pJ7q/mTlmHl5dnhpGZk2rs6EZE8kesA9Nprr/HZZ5/p9liR4srVBx75BjBh2v0jk+qdoZS7Ewej4/lk5RF7VycikidyfQmsW7durF27llKlSlGrVi0cHR2zbV+wYEGeFmgPugQmAqwaDZsmgasvG9oupve805hM8HP/ZrSo4mfv6kRErpOvl8B8fHzo1q0b99xzD35+fnh7e2dbRKSYuO9tCKoHyVdpvX8UTzYpj2HAa/P2EJeUbu/qRETuiCZDvQGdARL5j4tH4JvWkJFMWtsPaL+1NpGXEnmobhCTezbQhKkiUqhoMlQRyRv+VaH9+wA4rR3L1w84YzGbWLI3moW7z9q5OBGR25frgRAB5s+fz9y5c4mKiiItLS3btl27duVJYSJSSDTuD0dXwZHlVNs0lNfum8qENacY9dt+moSUIriUm70rFBHJtVyfAfr888/p168fAQEB7N69m6ZNm1K6dGlOnDhBx44d86NGEbEnkwk6TwZ3f7hwgOczfqRxRV8SUjN4dU44mVZdRReRoifXAeirr77i22+/5YsvvsDJyYk33niDVatW8dJLLxEXF5cfNYqIvXn4Q5evADD/9TVT7rqKh7MDO05dZcq6Y3YuTkQk93IdgKKiomjRogUArq6uXLt2DYCnn36aWbNm5W11IlJ4VG0HTQYA4L/6Zca3KwPApNVH2R111Z6ViYjkWq4DUGBgIFeuXAGgQoUKbN26FYDIyEgNjihS3LV7D8rUgsSLPHR8DJ3qBJBhNRgyazdxybo1XkSKjlwHoPvvv5/FixcD0K9fP1599VUeeOABevToQbdu3fK8QBEpRBxd4bHp4OiG6cQ6Pi67lvK+rpy5msxbCyL0nyARKTJyPQ6Q1WrFarXi4JB1A9ns2bPZvHkzYWFhPPfcczg5OeVLoQVJ4wCJ/INdP8LiF8Fk4ciDc3lwYRoZVoNxj9ShZ9MK9q5OREqo3Hx/ayDEG1AAEvkHhgELBkDEPPAOZkadmYxZHY2zg5l/a9Z4EbGTfB8IcePGjTz11FM0b96cs2ezBkP78ccf+fPPP29ndyJS1JhMWbPG+1aCuNP0ufQJrcP8SM2w8uIvu0hO06zxIlK45ToA/frrr7Rv3x5XV1d2795NamoqAHFxcXz44Yd5XqCIFFIuXln9gcyOmA79zldVd+Lv6cyR8wm8+/sBe1cnInJLuQ5A77//Pl9//TVTp07NNhN8y5Ytb2sU6C+//JKQkBBcXFxo1qwZ27Ztu2nb/fv30717d0JCQjCZTEyaNOm6NmPGjMFkMmVbqlevnuu6RCQHyjaAB94FwGPdaL5t64TJBLO2RfH73nN2Lk5E5OZyHYAOHz5M69atr1vv7e1NbGxsrvY1Z84chg4dyujRo9m1axf16tWjffv2XLhw4Ybtk5KSCA0NZfz48QQGBt50v7Vq1SI6Otq26NKcSD66axBU7QCZaTTY9iovt8r63RzxawSnryTZuTgRkRu7rXGAjh27fuTXP//8k9DQ0Fzt69NPP2XAgAH069ePmjVr8vXXX+Pm5sa0adNu2L5JkyZ8/PHHPPHEEzg7O990vw4ODgQGBtoWPz+/XNUlIrlgMmWNEu1ZFi4f46WEL2hUwYdrqRm8+MsuUjPUH0hECp9cB6ABAwbw8ssv89dff2EymTh37hw///wzw4YNY9CgQTneT1paGjt37qRt27b/X4zZTNu2bdmyZUtuy8rm6NGjlC1bltDQUHr16kVUVNQt26emphIfH59tEZFccC8Nj80AswPmAwv5rsYuvF0d2XMmjg+WHLR3dSIi18l1AHrzzTd58sknadOmDQkJCbRu3Zpnn32W5557jiFDhuR4P5cuXSIzM5OAgIBs6wMCAoiJicltWTbNmjVjxowZLF++nClTphAZGUmrVq1sU3bcyLhx4/D29rYtwcHBt/3+IiVWhWbwwHsA+G4cy/dtskbYmLnlFL+Fn7VnZSIi18l1ADKZTLz99ttcuXKFffv2sXXrVi5evMh7772XH/XlWseOHXnssceoW7cu7du3Z+nSpcTGxjJ37tybvmbEiBHExcXZltOnTxdgxSLFyF2DoGYXsKbTeNtQXr876/LziAURHD1/8/+EiIgUtNsaBwjAycmJmjVr0rRpUzw8PHL9ej8/PywWC+fPn8+2/vz587fs4JxbPj4+VK1a9Yb9lv7m7OyMl5dXtkVEboPJBJ0nQ+kqEH+GF66Mp2WoD0lpmQz6eReJqRn2rlBEBACHnDZ85plnctTuZh2Y/5eTkxONGjVizZo1dO3aFciaZmPNmjW8+OKLOS3rHyUkJHD8+HGefvrpPNuniNyCixc8PhOmtsF04g++bdGI+y/dxbELCYxYEMFnT9THZDLZu0oRKeFyfAZoxowZrF27ltjYWK5evXrTJTeGDh3K1KlT+eGHHzh48CCDBg0iMTGRfv36AdC7d29GjBhha5+WlkZ4eDjh4eGkpaVx9uxZwsPDs53dGTZsGOvXr+fkyZNs3ryZbt26YbFY6NmzZ65qE5E7EFALOk0CwH3zRGbecw2L2cTiPef4aesp+9YmIkIuzgANGjSIWbNmERkZSb9+/XjqqacoVarUHb15jx49uHjxIqNGjSImJob69euzfPlyW8foqKgozOb/z2jnzp2jQYMGtucTJ05k4sSJ3HPPPaxbtw6AM2fO0LNnTy5fvoy/vz933303W7duxd/f/45qFZFcqvcERG2BnTOo9udQPrj3R978I453fz9AnfI+1A/2sXeFIlKC5Woy1NTUVBYsWMC0adPYvHkzDz30EP3796ddu3bF6pS2JkMVySPpKTCtPUSHYwTV4yWX8fz74FWCvF1Y/OLd+HvefDwvEZHcyrfJUJ2dnenZsyerVq3iwIED1KpVixdeeIGQkBASEhLuqGgRKYYcXaDHj+BWGlP0Hj5xm0aonxvRcSm88PNO0jKs9q5QREqo274LzGw2YzKZMAyDzEyN9CoiN+FTAR77AUwWnPbPY2693Xg6O7D95FXe06SpImInuQpAqampzJo1iwceeICqVasSERHB5MmTiYqKuq1b4UWkhKjUCtp/CIDf5vf44d5ETCb4cespZm+79UjtIiL5IccB6IUXXiAoKIjx48fz8MMPc/r0aebNm8eDDz6YraOyiMgNNXsO6j0JhpWG24Yy5m53AEb+to+dp3J3B6mIyJ3KcSdos9lMhQoVaNCgwS07PC9YsCDPirMXdYIWySfpKTC9I5zbhRFQm5fdPmLxwTj8PZ35fcjdBHi52LtCESnCcvP9nePb4Hv37l2s7vQSETtwdIEeP8G392A6v49PakzlUJl+HLmQyHM/7mTOc3fh7GCxd5UiUgLk6jb4kkJngETy2akt8MPDYM0gtslQWm+/i/iUDB5tVJ6PH62r/2yJyG3Jt9vgRUTyRMXm8PC/APDZ/ilzWpzBbIL5O88wZf1xOxcnIiWBApCI2EfD3tDiJQBq/PUmk1ulAzBh+WGWRUTbszIRKQEUgETEftqOheoPQ2YaD+57jZcbOgLw6txw9p6JtW9tIlKsKQCJiP2YzfDItxBYF5Iu8cqFkXQMcyMl3Ur/H3ZwLjbZ3hWKSDGlACQi9uXkDk/OAc8gTJcO8YXDZ9Qo48bFa6n0/2EHiakZ9q5QRIohBSARsT+vstBzNji64RC5lnkVF+Ln7sjB6HhemrWbTKtuVhWRvKUAJCKFQ9n68MhUwIRHxA/8Vn87Tg5m1hy6wJjF+9GIHSKSlxSARKTwqPGwbc6wcjsnMO+uk7Y5w75ce8zOxYlIcaIAJCKFS/MXoMUQAOrteodv78qaJ2ziyiPM3XHanpWJSDGiACQihU/bd6HO42DN4IF9bzCmURoAIxZEsPbQBTsXJyLFgQKQiBQ+ZjN0+RJC74X0RPqcfJ0BtSHTavDCz7sIPx1r7wpFpIhTABKRwsnBCR7/EQLrYEq8yFuX3+bhyg4kp2fyzIztnLiYYO8KRaQIUwASkcLLxQt6/Qo+FTBdjeSz9He5K8jMlcQ0nv5+mwZKFJHbpgAkIoWbZwA8vQjcy2C5sI8fXT6mZmkTZ2OTeeq7v7h4LdXeFYpIEaQAJCKFX+nK0HsRuPjgGL2Thb6TqeRt4cSlRJ7+/i9ik9LsXaGIFDEKQCJSNATUgqcWgJMHzmc2sSRoKoEeFg7FXKPP9O0kaMoMEckFBSARKTrKN8qaN8zBBbeTq1lZ8WdKu5rZczqWZ2ZsJzkt094VikgRoQAkIkVLyN3Q4ycwO+J1fDGrwhbi5WxmW+QVnv9pJ6kZCkEi8s8UgESk6Al7ALpPBZOZUkfmsKbqAtwcTaw/cpFBP+1SCBKRf6QAJCJFU61u0O1bMJnxPzqXtVV/xcUB/jh0ged+3ElKukKQiNycApCIFF11H8uaQd5kJuD4fDZU/RU3R1h3+CIDFYJE5BYUgESkaKvzKHT/DkwWypz4lXVhv+LuaGLDkYsMmLlDHaNF5IYUgESk6Kvd/X9C0Fw8nWDj0Uv0/0F3h4nI9RSARKR4qP0IPPo9mCz4n1jI+koz8XGysvn4ZfpM38a1lHR7VygihYgCkIgUH7W6weMzweJEqVPL2VD+a8o4Z7At8go9p27lcoKmzRCRLApAIlK81HgYes0DR3e8zv3JuoBJhLilse9sPI99s4WzmkBVRFAAEpHiKPRe6LMYXHxwu7CLFb4fUcc7hRMXE3lsymaOX0ywd4UiYmcKQCJSPJVvDP2WgUcgzpcPstDlXe4ufY1zcSk89vUWIs7E2btCEbEjBSARKb4CasIzy8CnIg5xJ5lpvMMjAee5kpjGE99uYf2Ri/auUETsRAFIRIq3UqHwzAoIrIM56SKfJL7FkHJHSEzL5JkZ25m9LcreFYqIHSgAiUjx5xWUdTmsSltMGckMvfIun4RsI9Nq8OaCCD5ecQir1bB3lSJSgBSARKRkcPaEnrOhYW9MhpXuMZOYH7oUE1a+XHucV+aEaxJVkRJEAUhESg6LI3T6HO4fCUDjcz+xMeQHvMypLN5zjqe/26axgkRKCAUgESlZTCZoPSxrElWLE+VjVrGlzEdUc77CtpNX6Dx5EwfOxdu7ShHJZwpAIlIy1X0c+vwO7mVwjz3EUtdRdPE5wdnYZLpP2czSiGh7Vygi+UgBSERKrgrNYOBaCKqPJeUKk9LGMCZoC8npmbzw8y4+WXlYnaNFiikFIBEp2bzLwzPLofajmKwZ9L36BQuC5+BMGl/8cYyBP+4kXhOpihQ7CkAiIo6u0P07aDsGMNHw4m9sDZhAZYeLrD54ns5f/Mn+cxo5WqQ4UQASEYGsztF3vwpPzQfXUvjGHWCl20h6eO7l5OUkun21mVnbojAMXRITKQ4UgERE/luVtvD8RijfFEtaPB+lj2dKmUVYM9IYsSCC1+buISktw95VisgdUgASEflf3uWh7xK46wUAOsbPZWPApwSbL7Fg91m6TN7EoRjdKi9SlCkAiYjciIMTdBgHj88EZy+C4sJZ6/4WT7tv4+iFBDpP3sS0PyN1l5hIEaUAJCJyKzW7wHMboHxTHNITeC9zErNKf4dzRgLv/n6APtO3cSE+xd5VikguKQCJiPyTUpWyJlO9dwSYzDRP/IMtPqNo6XiYjUcv0X7SBlbsj7F3lSKSCwpAIiI5YXGAe9+EZ1aAT0U8Us7xk8N7fOozj6SkRJ77cSdvzN9DXLLGDBIpChSARERyI7gpPP8n1HsSk2HlkZSFbPEZSRPzIebuOEO7f61nzcHz9q5SRP6BApCISG65eEG3KdBzDngGUSrlNHOd3uMTz1+Ij4+j/w87eGX2bq4mptm7UhG5CQUgEZHbVa0DvLAVGjyNCYPu6b+z2fsdWpkjWBR+jgf+tZ6lEdEaPFGkEFIAEhG5E64+0GUyPLUAvIPxTT3Hj07jmOb5NaaE87zw8y6embGdqMtJ9q5URP6LApCISF6o0gZe2ALNngeTmfvTN/Cn+xv0d1zBhsMxPPCv9Xy+5igp6Zn2rlREAJOhc7PXiY+Px9vbm7i4OLy8vOxdjogUNefCYclQOLsTgJOOlRma8DS7jKqElHbj3S61aV3V3741ihRDufn+VgC6AQUgEbljVivs+gFWj4GUWACWm1rxbvJjnMOP9rUCGNGxBiF+7nYtU6Q4UQC6QwpAIpJnEi/BqtEQ/jNgkG5y4puMh/gqvRPpFlf6NA9hyP1heLs52rtSkSJPAegOKQCJSJ47Fw4r3oJTmwCItZTig5RH+TWzNV5uzrzSJoxed1XE0aKumSK3SwHoDikAiUi+MAw49DusHAlXIwE4aa7AuJTurLA2JtTPg6HtqvJg7SDMZpOdixUpehSA7pACkIjkq4xU2PYtbPgYUuIA2EcVPkp7lI3WOtQI8ub19lW5r1oZTCYFIZGcUgC6QwpAIlIgkmNh8xewdQqkJwKw3ajJ+LTH2GlUo2EFH15vX53mlUvbt06RIkIB6A4pAIlIgUq4CH9+Ctu/g8ys6TO2GrX4PL0Lm621aFHZjxfvq0LzyqV1RkjkFhSA7pACkIjYRdwZWD8h644xawYA4dYqfJHRhTXWhjSo4MuL91Xh/uq6NCZyIwpAd0gBSETsKvY0bP4cds2EjBQADhkV+DK9M0utzQgL9GHwfVV4sE4QFnWWFrFRALpDCkAiUigkXIAtX2ZdGktLAOCc4cf0jHbMybyP0n5leKZlCI80LI+7s4OdixWxPwWgO6QAJCKFSvJV+OvbrDvHki4BkIQzczLuZXpmB2Kdy9GzaQV6twihnI+rnYsVsZ/cfH/bfcStL7/8kpCQEFxcXGjWrBnbtm27adv9+/fTvXt3QkJCMJlMTJo06Y73KSJS6Ln6wr3D4dX90PkL8K+BG6n0c1jBOuehTMz8iAN/LuKeCWsY/Msudp66iv5vK3Jrdg1Ac+bMYejQoYwePZpdu3ZRr1492rdvz4ULF27YPikpidDQUMaPH09gYGCe7FNEpMhwdIGGvbNmnX96IVR5ADMG7Sw7+dFpPGscXiV4/zcMnLKMTpP/5Je/okhIzbB31SKFkl0vgTVr1owmTZowefJkAKxWK8HBwQwZMoQ333zzlq8NCQnhlVde4ZVXXsmzff5Nl8BEpMi4eAR2fA/hsyA1a1DFNMPCcmtTfs5oyz7HWnRpUJ4nm1agdjlvOxcrkr+KxCWwtLQ0du7cSdu2bf+/GLOZtm3bsmXLlgLdZ2pqKvHx8dkWEZEiwb8qdPwIXjsEXb6Eco1wMmXS2bKFOc7v8TsvU3rHJJ77YiGdJ//J7G06KyQCdgxAly5dIjMzk4CAgGzrAwICiImJKdB9jhs3Dm9vb9sSHBx8W+8vImI3Tm7Q4CkY8Ac8twEa9cNwdKeS+TyvOc5nk8vLjDj/Ojt+m8w97y/mldm72XDkIplW9RWSksnunaALgxEjRhAXF2dbTp8+be+SRERuX1A96DQJ07Aj0PVrqNQaAxPNLQeY6PgNG83P03r/O3w3Yyqtxq1g3LKDHDl/zd5VixQouw0c4efnh8Vi4fz589nWnz9//qYdnPNrn87Ozjg7O9/We4qIFFrOHlC/J9TviSn2NOydjRE+C7crx3nE8iePWP7kSpoHyzc1YczG5iQENuPh+sE8WCeI8r5u9q5eJF/Z7QyQk5MTjRo1Ys2aNbZ1VquVNWvW0Lx580KzTxGRYsEnGFq/jmnITui/Ghr3x3Dzo5QpgScd1vKL04d8f/lpXFYO57UJX/HI5A18t/EEZ2OT7V25SL6w69ChQ4cOpU+fPjRu3JimTZsyadIkEhMT6devHwC9e/emXLlyjBs3Dsjq5HzgwAHb47NnzxIeHo6HhwdVqlTJ0T5FREo0kwmCm0BwE0wdJ8DJjbB/AdYD/8Y/5Sq9HVbR22EVFy96s3pFQ0Yua0RSuZY8UK8SHWsHUlYDLUoxYfeRoCdPnszHH39MTEwM9evX5/PPP6dZs2YA3HvvvYSEhDBjxgwATp48SaVKla7bxz333MO6detytM+c0G3wIlLiZKbDifVZYejgvzGn/v/dsEmGM+utdVmV2YiYgHu4q3YYD9QMoHqgpyZllUJFU2HcIQUgESnRMtKyzgwdXkrmwSVYEqL/f5NhZodRjdWZDTns0ZQqtRrzQK1AmoaUwsGi+2rEvhSA7pACkIjIfxgGnNsNh5eSceB3HC4dzLb5jOHHhsy6bHNoiCX0XprVDOHeqv6U8XKxU8FSkikA3SEFIBGRm7gSCYeXkXlkJZzahMWaZtuUbljYYa3GemtdzpS6i+CazbinWgCNKvriqLNDUgAUgO6QApCISA6kJcHJP7EeW0XawZW4XDuZbfNVw4Ot1hrsNNclvUIrwmo2oEUVPyr5uavvkOQLBaA7pAAkInIbLh+HY2tIO7wKU9QmHDMSs22OMXzZbK3Ffuf6GJVaU7N6LVpULq07yyTPKADdIQUgEZE7lJkO58KxnlhP4qE1uMbswMFIy9YkyurPdqM6J9zq4hDSgrCaDWhe2Y/SHhqYVm6PAtAdUgASEclj6Slw+i/Sj60n+cgfeFzai5nMbE0uGV7ssFYj0q0u1gp3Ua56UxqHlqGcj6sumUmOKADdIQUgEZF8lnoNTv9F6onNJB3diMelcBz/5wxRouHMLmsYh51qkh7YiFJVm1OvWiWqlvHEbFYgkuspAN0hBSARkQKWkQrnwkk8uoHEY3/ieWEnrpnXT9B63BrEfnMYsaXq41KpKRWqN6FOBT/cne06sYEUEgpAd0gBSETEzqxWuHCAtMhNxB7ZjGP0LnxToq5rlmw4EWGEcsq1JqmBDfGu0pxqVatRxd9DZ4lKIAWgO6QAJCJSCCVeJvP0di4d3kz6qW2Uit2LmzXxumYXDB8OEsplrxpQtj5+Ve+iRtVq+Huqc3VxpwB0hxSARESKAKsVLh8l7tgW4o5uxjlmF/5JxzFjva7pBcOHo5bKxPnUwlS2AaXDmlE1LAwfNyc7FC75RQHoDikAiYgUUWlJZETv5eLhbSSf2o7rpQjKpJ7CcotQFOtdC1PZupSq3JiqVWtSSrfhF1kKQHdIAUhEpBhJSyLxdDgxh7aSfnoXnlf2EZh241AUb7hx3BzCFc9qGAG18Q5tSEj1Rvj7etuhcMktBaA7pAAkIlLMpSWREBXOhcNZocjt6kECU0/iSMZ1TTMMM6fM5bjgGkaKX01cytcnsGpjgoMr4qA5zgoVBaA7pAAkIlICZaSReO4A0Yd3kHx6N06XDhCYfBRv4/rb8SHrEtoZp1ASvKpgCqiJT0g9KlRtiLePT8HWLTYKQHdIAUhERAAwDBIvn+bcoW0knArH4cI+SiUcISjjHGbT9V+fVsPEOXMAF11DSfGthnPZ2vhXrk+5ynUxO6rDdX5TALpDCkAiInIr1pQEYo7t4tKJcNKj9+N69TCBKScoRdwN26cZFs45lOeqexXSS1fDtXwdylRuQJkKVTGZLQVcffGlAHSHFIBEROR2xF88x9mju4g7tRfThYN4XTtCcPopPEzJN2yfZDhz1rEicZ5VsPpVx6N8TcpUrkfpspUVjG6DAtAdUgASEZG8kpGRydmoY1w8vpuUs/twvHyI0onHCM48g7Mp/YavScaZGIfyxHtWxupXFfdytQisXA+vslXB4ljAn6DoUAC6QwpAIiKS39LS0jhz4gBXToSTdi4Cp6tHKZV8knKZZ3E2XX83GkA6Dpx3KEe8RyUySlfDtWwNSofUwTe4JiYntwL+BIWPAtAdUgASERF7SUlNJerYAS6fjCA1+kBWMEqKJNh6BndT6g1fY8XEBUsAV9xCSfcNwymwOr4V6+JfqRYWN98C/gT2owB0hxSARESksElISePkiSNcjtxLWsxBnK4exTfpJBUyo/AxXT8n2t+umny47FKBZK9KWPyr4lW+Bv4htXH2Dy12l9MUgO6QApCIiBQVKWkZnIo6xaWTEaSc24/lylG8E05QNj2KANPVm74uAwsXHcpyzSMEa6kquJatTumKtfAoWwPcSoPJVICfIm8oAN0hBSARESnqMq0GZ2IucD5yH9fOHsR68Qiu8SfwS42iohGNqyntpq9NMHlwxbUiyV6VsZQJw6NcDUpXrIWjX2VwKLxzpSkA3SEFIBERKa4Mw+DitWSiTh4j9tR+Us8fxuHqcXySTlHOepbypks3fW0mZi47BBLvHkKGb2WcA6vhW6EW3uVrYPIMtPtZIwWgO6QAJCIiJdG1lHRORF/i0qn9JJ87hHH5GO7XIvFPjaIS5/Awpdz0tYkmNy47B5PkGYrJrwruZavjV7EmLgFh4OxZIPUrAN0hBSAREZH/Z7UaxMQlc/Z0JFej9pN2/giOscfxSjxJ2cwzlOcilhtMDfK3q+ZSxLpWIM2nEg7+YXiXq45v5UZYSoXkaZ0KQHdIAUhERCRnUtIzOX3xKucjD5Bw7hDWi0dxiY+kVEoU5Y1o/EzxN3zdZr9HafHi93laS26+vx3y9J1FRESkRHFxtBBW1o+wsq2B1tm2XU1MI/zsOS6dOkDK+SNw+TjuCZH4pZ0lw7+mfQr+DwUgERERyRe+7k74Vg2BqiHAg7b1mVaD9EyrvcoCFIBERESkgFnMJix2nuzVbNd3FxEREbEDBSAREREpcRSAREREpMRRABIREZESRwFIREREShwFIBERESlxFIBERESkxFEAEhERkRJHAUhERERKHAUgERERKXEUgERERKTEUQASERGREkcBSEREREoczQZ/A4ZhABAfH2/nSkRERCSn/v7e/vt7/FYUgG7g2rVrAAQHB9u5EhEREcmta9eu4e3tfcs2JiMnMamEsVqtnDt3Dk9PT0wmU57uOz4+nuDgYE6fPo2Xl1ee7ru40bHKOR2rnNOxyjkdq5zTscq5/DxWhmFw7do1ypYti9l8614+OgN0A2azmfLly+fre3h5eemXJId0rHJOxyrndKxyTscq53Ssci6/jtU/nfn5mzpBi4iISImjACQiIiIljgJQAXN2dmb06NE4Ozvbu5RCT8cq53Ssck7HKud0rHJOxyrnCsuxUidoERERKXF0BkhERERKHAUgERERKXEUgERERKTEUQASERGREkcBqAB9+eWXhISE4OLiQrNmzdi2bZu9S8pXY8aMwWQyZVuqV69u256SksLgwYMpXbo0Hh4edO/enfPnz2fbR1RUFA899BBubm6UKVOG119/nYyMjGxt1q1bR8OGDXF2dqZKlSrMmDGjID7eHduwYQOdOnWibNmymEwmFi1alG27YRiMGjWKoKAgXF1dadu2LUePHs3W5sqVK/Tq1QsvLy98fHzo378/CQkJ2drs3buXVq1a4eLiQnBwMBMmTLiulnnz5lG9enVcXFyoU6cOS5cuzfPPeyf+6Vj17dv3up+1Dh06ZGtTEo7VuHHjaNKkCZ6enpQpU4auXbty+PDhbG0K8veuMP/Ny8mxuvfee6/7uXr++eeztSkJxwpgypQp1K1b1zZ4YfPmzVm2bJlte5H8uTKkQMyePdtwcnIypk2bZuzfv98YMGCA4ePjY5w/f97epeWb0aNHG7Vq1TKio6Nty8WLF23bn3/+eSM4ONhYs2aNsWPHDuOuu+4yWrRoYduekZFh1K5d22jbtq2xe/duY+nSpYafn58xYsQIW5sTJ04Ybm5uxtChQ40DBw4YX3zxhWGxWIzly5cX6Ge9HUuXLjXefvttY8GCBQZgLFy4MNv28ePHG97e3saiRYuMPXv2GJ07dzYqVapkJCcn29p06NDBqFevnrF161Zj48aNRpUqVYyePXvatsfFxRkBAQFGr169jH379hmzZs0yXF1djW+++cbWZtOmTYbFYjEmTJhgHDhwwHjnnXcMR0dHIyIiIt+PQU7907Hq06eP0aFDh2w/a1euXMnWpiQcq/bt2xvTp0839u3bZ4SHhxsPPvigUaFCBSMhIcHWpqB+7wr737ycHKt77rnHGDBgQLafq7i4ONv2knKsDMMwFi9ebCxZssQ4cuSIcfjwYeOtt94yHB0djX379hmGUTR/rhSACkjTpk2NwYMH255nZmYaZcuWNcaNG2fHqvLX6NGjjXr16t1wW2xsrOHo6GjMmzfPtu7gwYMGYGzZssUwjKwvPbPZbMTExNjaTJkyxfDy8jJSU1MNwzCMN954w6hVq1a2fffo0cNo3759Hn+a/PW/X+pWq9UIDAw0Pv74Y9u62NhYw9nZ2Zg1a5ZhGIZx4MABAzC2b99ua7Ns2TLDZDIZZ8+eNQzDML766ivD19fXdrwMwzCGDx9uVKtWzfb88ccfNx566KFs9TRr1sx47rnn8vQz5pWbBaAuXbrc9DUl9VhduHDBAIz169cbhlGwv3dF7W/e/x4rw8gKQC+//PJNX1NSj9XffH19je+++67I/lzpElgBSEtLY+fOnbRt29a2zmw207ZtW7Zs2WLHyvLf0aNHKVu2LKGhofTq1YuoqCgAdu7cSXp6erZjUr16dSpUqGA7Jlu2bKFOnToEBATY2rRv3574+Hj2799va/Pf+/i7TVE/rpGRkcTExGT7bN7e3jRr1izb8fHx8aFx48a2Nm3btsVsNvPXX3/Z2rRu3RonJydbm/bt23P48GGuXr1qa1McjuG6desoU6YM1apVY9CgQVy+fNm2raQeq7i4OABKlSoFFNzvXVH8m/e/x+pvP//8M35+ftSuXZsRI0aQlJRk21ZSj1VmZiazZ88mMTGR5s2bF9mfK02GWgAuXbpEZmZmtn94gICAAA4dOmSnqvJfs2bNmDFjBtWqVSM6OpqxY8fSqlUr9u3bR0xMDE5OTvj4+GR7TUBAADExMQDExMTc8Jj9ve1WbeLj40lOTsbV1TWfPl3++vvz3eiz/fdnL1OmTLbtDg4OlCpVKlubSpUqXbePv7f5+vre9Bj+vY+ioEOHDjzyyCNUqlSJ48eP89Zbb9GxY0e2bNmCxWIpkcfKarXyyiuv0LJlS2rXrg1QYL93V69eLVJ/8250rACefPJJKlasSNmyZdm7dy/Dhw/n8OHDLFiwACh5xyoiIoLmzZuTkpKCh4cHCxcupGbNmoSHhxfJnysFIMk3HTt2tD2uW7cuzZo1o2LFisydO7fIBhMpnJ544gnb4zp16lC3bl0qV67MunXraNOmjR0rs5/Bgwezb98+/vzzT3uXUujd7FgNHDjQ9rhOnToEBQXRpk0bjh8/TuXKlQu6TLurVq0a4eHhxMXFMX/+fPr06cP69evtXdZt0yWwAuDn54fFYrmuR/z58+cJDAy0U1UFz8fHh6pVq3Ls2DECAwNJS0sjNjY2W5v/PiaBgYE3PGZ/b7tVGy8vryIdsv7+fLf6mQkMDOTChQvZtmdkZHDlypU8OYZF+WczNDQUPz8/jh07BpS8Y/Xiiy/y+++/s3btWsqXL29bX1C/d0Xpb97NjtWNNGvWDCDbz1VJOlZOTk5UqVKFRo0aMW7cOOrVq8dnn31WZH+uFIAKgJOTE40aNWLNmjW2dVarlTVr1tC8eXM7VlawEhISOH78OEFBQTRq1AhHR8dsx+Tw4cNERUXZjknz5s2JiIjI9sW1atUqvLy8qFmzpq3Nf+/j7zZF/bhWqlSJwMDAbJ8tPj6ev/76K9vxiY2NZefOnbY2f/zxB1ar1faHunnz5mzYsIH09HRbm1WrVlGtWjV8fX1tbYrbMTxz5gyXL18mKCgIKDnHyjAMXnzxRRYuXMgff/xx3SW9gvq9Kwp/8/7pWN1IeHg4QLafq5JwrG7GarWSmppadH+uct1tWm7L7NmzDWdnZ2PGjBnGgQMHjIEDBxo+Pj7ZesQXN6+99pqxbt06IzIy0ti0aZPRtm1bw8/Pz7hw4YJhGFm3TVaoUMH4448/jB07dhjNmzc3mjdvbnv937dNtmvXzggPDzeWL19u+Pv73/C2yddff904ePCg8eWXXxaZ2+CvXbtm7N6929i9e7cBGJ9++qmxe/du49SpU4ZhZN0G7+PjY/z222/G3r17jS5dutzwNvgGDRoYf/31l/Hnn38aYWFh2W7tjo2NNQICAoynn37a2LdvnzF79mzDzc3tulu7HRwcjIkTJxoHDx40Ro8eXahu7TaMWx+ra9euGcOGDTO2bNliREZGGqtXrzYaNmxohIWFGSkpKbZ9lIRjNWjQIMPb29tYt25dtlu3k5KSbG0K6veusP/N+6djdezYMePdd981duzYYURGRhq//fabERoaarRu3dq2j5JyrAzDMN58801j/fr1RmRkpLF3717jzTffNEwmk7Fy5UrDMIrmz5UCUAH64osvjAoVKhhOTk5G06ZNja1bt9q7pHzVo0cPIygoyHBycjLKlStn9OjRwzh27Jhte3JysvHCCy8Yvr6+hpubm9GtWzcjOjo62z5OnjxpdOzY0XB1dTX8/PyM1157zUhPT8/WZu3atUb9+vUNJycnIzQ01Jg+fXpBfLw7tnbtWgO4bunTp49hGFm3wo8cOdIICAgwnJ2djTZt2hiHDx/Oto/Lly8bPXv2NDw8PAwvLy+jX79+xrVr17K12bNnj3H33Xcbzs7ORrly5Yzx48dfV8vcuXONqlWrGk5OTkatWrWMJUuW5Nvnvh23OlZJSUlGu3btDH9/f8PR0dGoWLGiMWDAgOv+IJaEY3WjYwRk+50oyN+7wvw375+OVVRUlNG6dWujVKlShrOzs1GlShXj9ddfzzYOkGGUjGNlGIbxzDPPGBUrVjScnJwMf39/o02bNrbwYxhF8+fKZBiGkfvzRiIiIiJFl/oAiYiISImjACQiIiIljgKQiIiIlDgKQCIiIlLiKACJiIhIiaMAJCIiIiWOApCIiIiUOApAIiIiUuIoAImI3ITJZGLRokX2LkNE8oECkIgUSn379sVkMl23dOjQwd6liUgx4GDvAkREbqZDhw5Mnz492zpnZ2c7VSMixYnOAIlIoeXs7ExgYGC2xdfXF8i6PDVlyhQ6duyIq6sroaGhzJ8/P9vrIyIiuP/++3F1daV06dIMHDiQhISEbG2mTZtGrVq1cHZ2JigoiBdffDHb9kuXLtGtWzfc3NwICwtj8eLFtm1Xr16lV69e+Pv74+rqSlhY2HWBTUQKJwUgESmyRo4cSffu3dmzZw+9evXiiSee4ODBgwAkJibSvn17fH192b59O/PmzWP16tXZAs6UKVMYPHgwAwcOJCIigsWLF1OlSpVs7zF27Fgef/xx9u7dy4MPPkivXr24cuWK7f0PHDjAsmXLOHjwIFOmTMHPz6/gDoCI3L7bmkNeRCSf9enTx7BYLIa7u3u25YMPPjAMwzAA4/nnn8/2mmbNmhmDBg0yDMMwvv32W8PX19dISEiwbV+yZIlhNpuNmJgYwzAMo2zZssbbb7990xoA45133rE9T0hIMABj2bJlhmEYRqdOnYx+/frlzQcWkQKlPkAiUmjdd999TJkyJdu6UqVK2R43b94827bmzZsTHh4OwMGDB6lXrx7u7u627S1btsRqtXL48GFMJhPnzp2jTZs2t6yhbt26tsfu7u54eXlx4cIFAAYNGkT37t3ZtWsX7dq1o2vXrrRo0eK2PquIFCwFIBEptNzd3a+7JJVXXF1dc9TO0dEx23OTyYTVagWgY8eOnDp1iqVLl7Jq1SratGnD4MGDmThxYp7XKyJ5S32ARKTI2rp163XPa9SoAUCNGjXYs2cPiYmJtu2bNm3CbDZTrVo1PD09CQkJYc2aNXdUg7+/P3369OGnn35i0qRJfPvtt3e0PxEpGDoDJCKFVmpqKjExMdnWOTg42Doaz5s3j8aNG3P33Xfz888/s23bNr7//nsAevXqxejRo+nTpw9jxozh4sWLDBkyhKeffpqAgAAAxowZw/PPP0+ZMmXo2LEj165dY9OmTQwZMiRH9Y0aNYpGjRpRq1YtUlNT+f33320BTEQKNwUgESm0li9fTlBQULZ11apV49ChQ0DWHVqzZ8/mhRdeICgoiFmzZlGzZk0A3NzcWLFiBS+//DJNmjTBzc2N7t278+mnn9r21adPH1JSUvjXv/7FsGHD8PPz49FHH81xfU5OTowYMYKTJ0/i6upKq1atmD17dh58chHJbybDMAx7FyEiklsmk4mFCxfStWtXe5ciIkWQ+gCJiIhIiaMAJCIiIiWO+gCJSJGkq/cicid0BkhERERKHAUgERERKXEUgERERKTEUQASERGREkcBSEREREocBSAREREpcRSAREREpMRRABIREZES5/8A3sJIOzM1+tEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Energy output prediction using MLPs in TensorFlow - Regression - Code Sample**"
      ],
      "metadata": {
        "id": "DYlKymaHqQzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requires libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Du7IoRfkqW2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.elu, tf.sigmoid], learning_rate= 0.001):\n",
        "        self.n_input = n_input # Number of inputs to the neuron\n",
        "        self.act_fn = act_func\n",
        "        seed = 123\n",
        "\n",
        "        # Placeholder for input data\n",
        "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n",
        "        # Placeholder for output data\n",
        "        self.y = tf.placeholder(tf.float32, name='Y')\n",
        "\n",
        "        # Build the graph for a single neuron\n",
        "        # Hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev=2, seed = seed), name = \"weights\")\n",
        "        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed), name=\"bias\")\n",
        "        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n",
        "        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n",
        "\n",
        "\n",
        "        # Output Layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output], stddev=2, seed = 0), name = \"weights\")\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed), name=\"bias\")\n",
        "        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n",
        "        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n",
        "\n",
        "\n",
        "        activity = tf.matmul(self.X, self.W1) + self.b1 #\n",
        "        h1 = self.act_fn[0](activity)\n",
        "\n",
        "        activity = tf.matmul(h1, self.W2) + self.b2\n",
        "        self.y_hat = self.act_fn[1](activity)\n",
        "\n",
        "\n",
        "        error = self.y - self.y_hat\n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.square(error)) + 0.6*tf.nn.l2_loss(self.W1) #+ 0.6*tf.nn.l2_loss(self.W2)\n",
        "        self.opt =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
        "\n",
        "\n",
        "        tf.summary.scalar(\"loss\",self.loss)\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        self.merge = tf.summary.merge_all()\n",
        "        self.writer = tf.summary.FileWriter(\"logs/\", graph=tf.get_default_graph())\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
        "        epoch = 0\n",
        "        X, Y = shuffle(X,Y)\n",
        "        loss = []\n",
        "        loss_val = []\n",
        "        while epoch < epochs:\n",
        "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)\n",
        "            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
        "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
        "\n",
        "            loss.append(l)\n",
        "            loss_val.append(l_val)\n",
        "            self.writer.add_summary(merge, epoch)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
        "                      format(epoch,epochs,l, l_val ))\n",
        "\n",
        "\n",
        "            epoch += 1\n",
        "        return loss, loss_val\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
      ],
      "metadata": {
        "id": "eyZ_qkHktiei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Folds5x2_pp.xlsx'\n",
        "df = pd.read_excel(filename, sheet_name='Sheet1')\n",
        "X, Y = df[['AT', 'V','AP','RH']], df['PE']\n",
        "scaler = MinMaxScaler()\n",
        "X_new = scaler.fit_transform(X)\n",
        "target_scaler = MinMaxScaler()\n",
        "Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\n",
        "X_train, X_val, Y_train, y_val = \\\n",
        "  train_test_split(X_new, Y_new, test_size=0.4, random_state=333)"
      ],
      "metadata": {
        "id": "RPSNob9VuCBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, d = X_train.shape\n",
        "_, n = Y_train.shape\n",
        "\n",
        "# import tensorflow 1.0\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # disable tensorflow v2 - wasn't compatible with the code\n",
        "\n",
        "model = MLP(n_input=d, n_hidden=15, n_output=n)\n",
        "\n",
        "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 6000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7r451OLuEU1",
        "outputId": "13e5bdc6-2c78-4498-9845-b5aba5951b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/6000  training loss: 108.84590911865234 Validation loss 108.71925354003906\n",
            "Epoch 10/6000  training loss: 107.54991912841797 Validation loss 107.42478942871094\n",
            "Epoch 20/6000  training loss: 106.2693862915039 Validation loss 106.14578247070312\n",
            "Epoch 30/6000  training loss: 105.00411224365234 Validation loss 104.88204193115234\n",
            "Epoch 40/6000  training loss: 103.75394439697266 Validation loss 103.63336181640625\n",
            "Epoch 50/6000  training loss: 102.5186996459961 Validation loss 102.39956665039062\n",
            "Epoch 60/6000  training loss: 101.29817199707031 Validation loss 101.18050384521484\n",
            "Epoch 70/6000  training loss: 100.09220886230469 Validation loss 99.9759750366211\n",
            "Epoch 80/6000  training loss: 98.90061950683594 Validation loss 98.78582763671875\n",
            "Epoch 90/6000  training loss: 97.72327423095703 Validation loss 97.60985565185547\n",
            "Epoch 100/6000  training loss: 96.5599365234375 Validation loss 96.44792175292969\n",
            "Epoch 110/6000  training loss: 95.41047668457031 Validation loss 95.2998275756836\n",
            "Epoch 120/6000  training loss: 94.27474975585938 Validation loss 94.16545867919922\n",
            "Epoch 130/6000  training loss: 93.1525650024414 Validation loss 93.04459381103516\n",
            "Epoch 140/6000  training loss: 92.04374694824219 Validation loss 91.93711853027344\n",
            "Epoch 150/6000  training loss: 90.94816589355469 Validation loss 90.84282684326172\n",
            "Epoch 160/6000  training loss: 89.86566162109375 Validation loss 89.7615966796875\n",
            "Epoch 170/6000  training loss: 88.79605865478516 Validation loss 88.6932601928711\n",
            "Epoch 180/6000  training loss: 87.73921203613281 Validation loss 87.63766479492188\n",
            "Epoch 190/6000  training loss: 86.6949691772461 Validation loss 86.59466552734375\n",
            "Epoch 200/6000  training loss: 85.6631851196289 Validation loss 85.56410217285156\n",
            "Epoch 210/6000  training loss: 84.64368438720703 Validation loss 84.54582214355469\n",
            "Epoch 220/6000  training loss: 83.63636779785156 Validation loss 83.53968811035156\n",
            "Epoch 230/6000  training loss: 82.64104461669922 Validation loss 82.54554748535156\n",
            "Epoch 240/6000  training loss: 81.6576156616211 Validation loss 81.56327056884766\n",
            "Epoch 250/6000  training loss: 80.68589782714844 Validation loss 80.59271240234375\n",
            "Epoch 260/6000  training loss: 79.72577667236328 Validation loss 79.63371276855469\n",
            "Epoch 270/6000  training loss: 78.77708435058594 Validation loss 78.68614959716797\n",
            "Epoch 280/6000  training loss: 77.8397216796875 Validation loss 77.74988555908203\n",
            "Epoch 290/6000  training loss: 76.91352844238281 Validation loss 76.82479095458984\n",
            "Epoch 300/6000  training loss: 75.99838256835938 Validation loss 75.91072082519531\n",
            "Epoch 310/6000  training loss: 75.09415435791016 Validation loss 75.00756072998047\n",
            "Epoch 320/6000  training loss: 74.2007064819336 Validation loss 74.11516571044922\n",
            "Epoch 330/6000  training loss: 73.31791687011719 Validation loss 73.23341369628906\n",
            "Epoch 340/6000  training loss: 72.44564819335938 Validation loss 72.36217498779297\n",
            "Epoch 350/6000  training loss: 71.58378601074219 Validation loss 71.50131225585938\n",
            "Epoch 360/6000  training loss: 70.73219299316406 Validation loss 70.65072631835938\n",
            "Epoch 370/6000  training loss: 69.89075469970703 Validation loss 69.81027221679688\n",
            "Epoch 380/6000  training loss: 69.0593490600586 Validation loss 68.9798355102539\n",
            "Epoch 390/6000  training loss: 68.23784637451172 Validation loss 68.15930938720703\n",
            "Epoch 400/6000  training loss: 67.42615509033203 Validation loss 67.34855651855469\n",
            "Epoch 410/6000  training loss: 66.62413024902344 Validation loss 66.54747772216797\n",
            "Epoch 420/6000  training loss: 65.83165740966797 Validation loss 65.75593566894531\n",
            "Epoch 430/6000  training loss: 65.04865264892578 Validation loss 64.97383880615234\n",
            "Epoch 440/6000  training loss: 64.27497100830078 Validation loss 64.20106506347656\n",
            "Epoch 450/6000  training loss: 63.51051330566406 Validation loss 63.437496185302734\n",
            "Epoch 460/6000  training loss: 62.75516891479492 Validation loss 62.68302917480469\n",
            "Epoch 470/6000  training loss: 62.00881576538086 Validation loss 61.93755340576172\n",
            "Epoch 480/6000  training loss: 61.2713623046875 Validation loss 61.200965881347656\n",
            "Epoch 490/6000  training loss: 60.5427131652832 Validation loss 60.4731559753418\n",
            "Epoch 500/6000  training loss: 59.82273864746094 Validation loss 59.7540283203125\n",
            "Epoch 510/6000  training loss: 59.11135482788086 Validation loss 59.043460845947266\n",
            "Epoch 520/6000  training loss: 58.408443450927734 Validation loss 58.34136199951172\n",
            "Epoch 530/6000  training loss: 57.71390151977539 Validation loss 57.64762878417969\n",
            "Epoch 540/6000  training loss: 57.02764129638672 Validation loss 56.96216583251953\n",
            "Epoch 550/6000  training loss: 56.34955596923828 Validation loss 56.284873962402344\n",
            "Epoch 560/6000  training loss: 55.6795539855957 Validation loss 55.61564636230469\n",
            "Epoch 570/6000  training loss: 55.01753616333008 Validation loss 54.95439147949219\n",
            "Epoch 580/6000  training loss: 54.3634033203125 Validation loss 54.3010139465332\n",
            "Epoch 590/6000  training loss: 53.71707534790039 Validation loss 53.65542984008789\n",
            "Epoch 600/6000  training loss: 53.07843780517578 Validation loss 53.01752471923828\n",
            "Epoch 610/6000  training loss: 52.44740676879883 Validation loss 52.387229919433594\n",
            "Epoch 620/6000  training loss: 51.82389831542969 Validation loss 51.76443099975586\n",
            "Epoch 630/6000  training loss: 51.20781326293945 Validation loss 51.14906311035156\n",
            "Epoch 640/6000  training loss: 50.599063873291016 Validation loss 50.541015625\n",
            "Epoch 650/6000  training loss: 49.9975700378418 Validation loss 49.94021224975586\n",
            "Epoch 660/6000  training loss: 49.403236389160156 Validation loss 49.3465690612793\n",
            "Epoch 670/6000  training loss: 48.815982818603516 Validation loss 48.75998306274414\n",
            "Epoch 680/6000  training loss: 48.2357177734375 Validation loss 48.18038558959961\n",
            "Epoch 690/6000  training loss: 47.6623649597168 Validation loss 47.607696533203125\n",
            "Epoch 700/6000  training loss: 47.0958366394043 Validation loss 47.04181671142578\n",
            "Epoch 710/6000  training loss: 46.53605270385742 Validation loss 46.4826774597168\n",
            "Epoch 720/6000  training loss: 45.982933044433594 Validation loss 45.9301872253418\n",
            "Epoch 730/6000  training loss: 45.4364013671875 Validation loss 45.384281158447266\n",
            "Epoch 740/6000  training loss: 44.89636993408203 Validation loss 44.84486770629883\n",
            "Epoch 750/6000  training loss: 44.362762451171875 Validation loss 44.31187057495117\n",
            "Epoch 760/6000  training loss: 43.83551025390625 Validation loss 43.78522491455078\n",
            "Epoch 770/6000  training loss: 43.31452941894531 Validation loss 43.26483917236328\n",
            "Epoch 780/6000  training loss: 42.799747467041016 Validation loss 42.75063705444336\n",
            "Epoch 790/6000  training loss: 42.29108428955078 Validation loss 42.2425651550293\n",
            "Epoch 800/6000  training loss: 41.78847122192383 Validation loss 41.74052047729492\n",
            "Epoch 810/6000  training loss: 41.291839599609375 Validation loss 41.24445343017578\n",
            "Epoch 820/6000  training loss: 40.801109313964844 Validation loss 40.75428009033203\n",
            "Epoch 830/6000  training loss: 40.31621170043945 Validation loss 40.26993942260742\n",
            "Epoch 840/6000  training loss: 39.83707809448242 Validation loss 39.791351318359375\n",
            "Epoch 850/6000  training loss: 39.36363983154297 Validation loss 39.318450927734375\n",
            "Epoch 860/6000  training loss: 38.89583206176758 Validation loss 38.851173400878906\n",
            "Epoch 870/6000  training loss: 38.43357467651367 Validation loss 38.389442443847656\n",
            "Epoch 880/6000  training loss: 37.9768180847168 Validation loss 37.93320083618164\n",
            "Epoch 890/6000  training loss: 37.525474548339844 Validation loss 37.48237609863281\n",
            "Epoch 900/6000  training loss: 37.07949447631836 Validation loss 37.03689956665039\n",
            "Epoch 910/6000  training loss: 36.63881301879883 Validation loss 36.59672164916992\n",
            "Epoch 920/6000  training loss: 36.20335388183594 Validation loss 36.1617546081543\n",
            "Epoch 930/6000  training loss: 35.7730712890625 Validation loss 35.73196029663086\n",
            "Epoch 940/6000  training loss: 35.34788131713867 Validation loss 35.307247161865234\n",
            "Epoch 950/6000  training loss: 34.92774200439453 Validation loss 34.88758850097656\n",
            "Epoch 960/6000  training loss: 34.51258087158203 Validation loss 34.472896575927734\n",
            "Epoch 970/6000  training loss: 34.10234069824219 Validation loss 34.06311798095703\n",
            "Epoch 980/6000  training loss: 33.696964263916016 Validation loss 33.6581916809082\n",
            "Epoch 990/6000  training loss: 33.29637908935547 Validation loss 33.2580680847168\n",
            "Epoch 1000/6000  training loss: 32.90054702758789 Validation loss 32.8626823425293\n",
            "Epoch 1010/6000  training loss: 32.509395599365234 Validation loss 32.47196960449219\n",
            "Epoch 1020/6000  training loss: 32.12287902832031 Validation loss 32.08589172363281\n",
            "Epoch 1030/6000  training loss: 31.740928649902344 Validation loss 31.704368591308594\n",
            "Epoch 1040/6000  training loss: 31.363494873046875 Validation loss 31.327360153198242\n",
            "Epoch 1050/6000  training loss: 30.990522384643555 Validation loss 30.954809188842773\n",
            "Epoch 1060/6000  training loss: 30.621957778930664 Validation loss 30.586654663085938\n",
            "Epoch 1070/6000  training loss: 30.257741928100586 Validation loss 30.22284698486328\n",
            "Epoch 1080/6000  training loss: 29.8978271484375 Validation loss 29.863338470458984\n",
            "Epoch 1090/6000  training loss: 29.542156219482422 Validation loss 29.50806427001953\n",
            "Epoch 1100/6000  training loss: 29.19068145751953 Validation loss 29.156982421875\n",
            "Epoch 1110/6000  training loss: 28.843345642089844 Validation loss 28.810035705566406\n",
            "Epoch 1120/6000  training loss: 28.500099182128906 Validation loss 28.467174530029297\n",
            "Epoch 1130/6000  training loss: 28.1608943939209 Validation loss 28.12835121154785\n",
            "Epoch 1140/6000  training loss: 27.82568359375 Validation loss 27.79351234436035\n",
            "Epoch 1150/6000  training loss: 27.494413375854492 Validation loss 27.462608337402344\n",
            "Epoch 1160/6000  training loss: 27.167030334472656 Validation loss 27.135591506958008\n",
            "Epoch 1170/6000  training loss: 26.843490600585938 Validation loss 26.812416076660156\n",
            "Epoch 1180/6000  training loss: 26.523752212524414 Validation loss 26.493030548095703\n",
            "Epoch 1190/6000  training loss: 26.207763671875 Validation loss 26.17738914489746\n",
            "Epoch 1200/6000  training loss: 25.89547348022461 Validation loss 25.865449905395508\n",
            "Epoch 1210/6000  training loss: 25.586841583251953 Validation loss 25.557161331176758\n",
            "Epoch 1220/6000  training loss: 25.281824111938477 Validation loss 25.252477645874023\n",
            "Epoch 1230/6000  training loss: 24.980371475219727 Validation loss 24.951358795166016\n",
            "Epoch 1240/6000  training loss: 24.68243980407715 Validation loss 24.653757095336914\n",
            "Epoch 1250/6000  training loss: 24.387990951538086 Validation loss 24.35963249206543\n",
            "Epoch 1260/6000  training loss: 24.09697723388672 Validation loss 24.06894302368164\n",
            "Epoch 1270/6000  training loss: 23.809362411499023 Validation loss 23.781641006469727\n",
            "Epoch 1280/6000  training loss: 23.525096893310547 Validation loss 23.497692108154297\n",
            "Epoch 1290/6000  training loss: 23.244144439697266 Validation loss 23.21704864501953\n",
            "Epoch 1300/6000  training loss: 22.96646499633789 Validation loss 22.939674377441406\n",
            "Epoch 1310/6000  training loss: 22.692018508911133 Validation loss 22.665531158447266\n",
            "Epoch 1320/6000  training loss: 22.420766830444336 Validation loss 22.394580841064453\n",
            "Epoch 1330/6000  training loss: 22.152677536010742 Validation loss 22.126785278320312\n",
            "Epoch 1340/6000  training loss: 21.88770294189453 Validation loss 21.862106323242188\n",
            "Epoch 1350/6000  training loss: 21.625812530517578 Validation loss 21.600507736206055\n",
            "Epoch 1360/6000  training loss: 21.36697006225586 Validation loss 21.341957092285156\n",
            "Epoch 1370/6000  training loss: 21.111143112182617 Validation loss 21.086416244506836\n",
            "Epoch 1380/6000  training loss: 20.858291625976562 Validation loss 20.833850860595703\n",
            "Epoch 1390/6000  training loss: 20.6083927154541 Validation loss 20.58423614501953\n",
            "Epoch 1400/6000  training loss: 20.361404418945312 Validation loss 20.337535858154297\n",
            "Epoch 1410/6000  training loss: 20.117305755615234 Validation loss 20.093717575073242\n",
            "Epoch 1420/6000  training loss: 19.876054763793945 Validation loss 19.85275650024414\n",
            "Epoch 1430/6000  training loss: 19.637632369995117 Validation loss 19.614612579345703\n",
            "Epoch 1440/6000  training loss: 19.402008056640625 Validation loss 19.379270553588867\n",
            "Epoch 1450/6000  training loss: 19.169147491455078 Validation loss 19.146696090698242\n",
            "Epoch 1460/6000  training loss: 18.93903160095215 Validation loss 18.91686248779297\n",
            "Epoch 1470/6000  training loss: 18.711627960205078 Validation loss 18.689748764038086\n",
            "Epoch 1480/6000  training loss: 18.486913681030273 Validation loss 18.46531867980957\n",
            "Epoch 1490/6000  training loss: 18.264862060546875 Validation loss 18.243555068969727\n",
            "Epoch 1500/6000  training loss: 18.04545021057129 Validation loss 18.024431228637695\n",
            "Epoch 1510/6000  training loss: 17.828649520874023 Validation loss 17.807918548583984\n",
            "Epoch 1520/6000  training loss: 17.61444091796875 Validation loss 17.593996047973633\n",
            "Epoch 1530/6000  training loss: 17.402795791625977 Validation loss 17.382644653320312\n",
            "Epoch 1540/6000  training loss: 17.193693161010742 Validation loss 17.173830032348633\n",
            "Epoch 1550/6000  training loss: 16.987110137939453 Validation loss 16.967533111572266\n",
            "Epoch 1560/6000  training loss: 16.783018112182617 Validation loss 16.76372528076172\n",
            "Epoch 1570/6000  training loss: 16.58139419555664 Validation loss 16.562387466430664\n",
            "Epoch 1580/6000  training loss: 16.382211685180664 Validation loss 16.363492965698242\n",
            "Epoch 1590/6000  training loss: 16.185455322265625 Validation loss 16.16701316833496\n",
            "Epoch 1600/6000  training loss: 15.99109172821045 Validation loss 15.972928047180176\n",
            "Epoch 1610/6000  training loss: 15.79909896850586 Validation loss 15.781208992004395\n",
            "Epoch 1620/6000  training loss: 15.609448432922363 Validation loss 15.591830253601074\n",
            "Epoch 1630/6000  training loss: 15.422119140625 Validation loss 15.404766082763672\n",
            "Epoch 1640/6000  training loss: 15.237080574035645 Validation loss 15.219989776611328\n",
            "Epoch 1650/6000  training loss: 15.05430793762207 Validation loss 15.0374755859375\n",
            "Epoch 1660/6000  training loss: 14.873774528503418 Validation loss 14.857195854187012\n",
            "Epoch 1670/6000  training loss: 14.695455551147461 Validation loss 14.679122924804688\n",
            "Epoch 1680/6000  training loss: 14.519320487976074 Validation loss 14.503231048583984\n",
            "Epoch 1690/6000  training loss: 14.345346450805664 Validation loss 14.32949447631836\n",
            "Epoch 1700/6000  training loss: 14.173501968383789 Validation loss 14.157882690429688\n",
            "Epoch 1710/6000  training loss: 14.00376033782959 Validation loss 13.988369941711426\n",
            "Epoch 1720/6000  training loss: 13.836098670959473 Validation loss 13.820930480957031\n",
            "Epoch 1730/6000  training loss: 13.670487403869629 Validation loss 13.655534744262695\n",
            "Epoch 1740/6000  training loss: 13.506898880004883 Validation loss 13.49216079711914\n",
            "Epoch 1750/6000  training loss: 13.345308303833008 Validation loss 13.330777168273926\n",
            "Epoch 1760/6000  training loss: 13.185688018798828 Validation loss 13.171361923217773\n",
            "Epoch 1770/6000  training loss: 13.0280122756958 Validation loss 13.013886451721191\n",
            "Epoch 1780/6000  training loss: 12.8722562789917 Validation loss 12.85832405090332\n",
            "Epoch 1790/6000  training loss: 12.71839427947998 Validation loss 12.704655647277832\n",
            "Epoch 1800/6000  training loss: 12.566400527954102 Validation loss 12.552846908569336\n",
            "Epoch 1810/6000  training loss: 12.416251182556152 Validation loss 12.402884483337402\n",
            "Epoch 1820/6000  training loss: 12.267922401428223 Validation loss 12.254734992980957\n",
            "Epoch 1830/6000  training loss: 12.12138843536377 Validation loss 12.108377456665039\n",
            "Epoch 1840/6000  training loss: 11.9766263961792 Validation loss 11.963788986206055\n",
            "Epoch 1850/6000  training loss: 11.833613395690918 Validation loss 11.820945739746094\n",
            "Epoch 1860/6000  training loss: 11.6923246383667 Validation loss 11.679827690124512\n",
            "Epoch 1870/6000  training loss: 11.552739143371582 Validation loss 11.540407180786133\n",
            "Epoch 1880/6000  training loss: 11.414835929870605 Validation loss 11.402666091918945\n",
            "Epoch 1890/6000  training loss: 11.278593063354492 Validation loss 11.266584396362305\n",
            "Epoch 1900/6000  training loss: 11.143988609313965 Validation loss 11.132136344909668\n",
            "Epoch 1910/6000  training loss: 11.011000633239746 Validation loss 10.999303817749023\n",
            "Epoch 1920/6000  training loss: 10.879609107971191 Validation loss 10.868064880371094\n",
            "Epoch 1930/6000  training loss: 10.749794006347656 Validation loss 10.738399505615234\n",
            "Epoch 1940/6000  training loss: 10.621535301208496 Validation loss 10.610288619995117\n",
            "Epoch 1950/6000  training loss: 10.4948148727417 Validation loss 10.483715057373047\n",
            "Epoch 1960/6000  training loss: 10.369612693786621 Validation loss 10.358654975891113\n",
            "Epoch 1970/6000  training loss: 10.245906829833984 Validation loss 10.235093116760254\n",
            "Epoch 1980/6000  training loss: 10.123682975769043 Validation loss 10.113008499145508\n",
            "Epoch 1990/6000  training loss: 10.002922058105469 Validation loss 9.992384910583496\n",
            "Epoch 2000/6000  training loss: 9.883602142333984 Validation loss 9.873202323913574\n",
            "Epoch 2010/6000  training loss: 9.76570987701416 Validation loss 9.755444526672363\n",
            "Epoch 2020/6000  training loss: 9.649226188659668 Validation loss 9.639091491699219\n",
            "Epoch 2030/6000  training loss: 9.53413200378418 Validation loss 9.524128913879395\n",
            "Epoch 2040/6000  training loss: 9.42041301727295 Validation loss 9.410540580749512\n",
            "Epoch 2050/6000  training loss: 9.308052062988281 Validation loss 9.298306465148926\n",
            "Epoch 2060/6000  training loss: 9.19703197479248 Validation loss 9.18741226196289\n",
            "Epoch 2070/6000  training loss: 9.087336540222168 Validation loss 9.077839851379395\n",
            "Epoch 2080/6000  training loss: 8.978949546813965 Validation loss 8.969574928283691\n",
            "Epoch 2090/6000  training loss: 8.871855735778809 Validation loss 8.862602233886719\n",
            "Epoch 2100/6000  training loss: 8.766039848327637 Validation loss 8.756905555725098\n",
            "Epoch 2110/6000  training loss: 8.66148567199707 Validation loss 8.652469635009766\n",
            "Epoch 2120/6000  training loss: 8.558177947998047 Validation loss 8.549278259277344\n",
            "Epoch 2130/6000  training loss: 8.456101417541504 Validation loss 8.447317123413086\n",
            "Epoch 2140/6000  training loss: 8.355242729187012 Validation loss 8.346571922302246\n",
            "Epoch 2150/6000  training loss: 8.255585670471191 Validation loss 8.247026443481445\n",
            "Epoch 2160/6000  training loss: 8.157116889953613 Validation loss 8.148667335510254\n",
            "Epoch 2170/6000  training loss: 8.059823036193848 Validation loss 8.051482200622559\n",
            "Epoch 2180/6000  training loss: 7.963688373565674 Validation loss 7.955455303192139\n",
            "Epoch 2190/6000  training loss: 7.8687005043029785 Validation loss 7.860573768615723\n",
            "Epoch 2200/6000  training loss: 7.774844169616699 Validation loss 7.766822814941406\n",
            "Epoch 2210/6000  training loss: 7.682106971740723 Validation loss 7.674189567565918\n",
            "Epoch 2220/6000  training loss: 7.590476036071777 Validation loss 7.58266019821167\n",
            "Epoch 2230/6000  training loss: 7.499936103820801 Validation loss 7.492222309112549\n",
            "Epoch 2240/6000  training loss: 7.410475730895996 Validation loss 7.402860641479492\n",
            "Epoch 2250/6000  training loss: 7.322082042694092 Validation loss 7.3145670890808105\n",
            "Epoch 2260/6000  training loss: 7.234742641448975 Validation loss 7.227323055267334\n",
            "Epoch 2270/6000  training loss: 7.148444652557373 Validation loss 7.141120910644531\n",
            "Epoch 2280/6000  training loss: 7.063174247741699 Validation loss 7.05594539642334\n",
            "Epoch 2290/6000  training loss: 6.978921413421631 Validation loss 6.971786022186279\n",
            "Epoch 2300/6000  training loss: 6.895672798156738 Validation loss 6.888629913330078\n",
            "Epoch 2310/6000  training loss: 6.813417434692383 Validation loss 6.806465148925781\n",
            "Epoch 2320/6000  training loss: 6.732142448425293 Validation loss 6.725281715393066\n",
            "Epoch 2330/6000  training loss: 6.651835918426514 Validation loss 6.645063877105713\n",
            "Epoch 2340/6000  training loss: 6.572487831115723 Validation loss 6.565803050994873\n",
            "Epoch 2350/6000  training loss: 6.49408483505249 Validation loss 6.487486839294434\n",
            "Epoch 2360/6000  training loss: 6.416617393493652 Validation loss 6.4101057052612305\n",
            "Epoch 2370/6000  training loss: 6.340074062347412 Validation loss 6.333645820617676\n",
            "Epoch 2380/6000  training loss: 6.264444351196289 Validation loss 6.25809907913208\n",
            "Epoch 2390/6000  training loss: 6.189715385437012 Validation loss 6.183452606201172\n",
            "Epoch 2400/6000  training loss: 6.115878105163574 Validation loss 6.109696388244629\n",
            "Epoch 2410/6000  training loss: 6.042922019958496 Validation loss 6.036820888519287\n",
            "Epoch 2420/6000  training loss: 5.970835208892822 Validation loss 5.964813709259033\n",
            "Epoch 2430/6000  training loss: 5.899609565734863 Validation loss 5.893665313720703\n",
            "Epoch 2440/6000  training loss: 5.829232692718506 Validation loss 5.823366641998291\n",
            "Epoch 2450/6000  training loss: 5.759695053100586 Validation loss 5.753905296325684\n",
            "Epoch 2460/6000  training loss: 5.690988063812256 Validation loss 5.685272216796875\n",
            "Epoch 2470/6000  training loss: 5.6230998039245605 Validation loss 5.617458820343018\n",
            "Epoch 2480/6000  training loss: 5.5560221672058105 Validation loss 5.550455093383789\n",
            "Epoch 2490/6000  training loss: 5.489744663238525 Validation loss 5.484250068664551\n",
            "Epoch 2500/6000  training loss: 5.424258232116699 Validation loss 5.418835639953613\n",
            "Epoch 2510/6000  training loss: 5.359553337097168 Validation loss 5.354201316833496\n",
            "Epoch 2520/6000  training loss: 5.295620918273926 Validation loss 5.290338039398193\n",
            "Epoch 2530/6000  training loss: 5.23245096206665 Validation loss 5.227236747741699\n",
            "Epoch 2540/6000  training loss: 5.170035362243652 Validation loss 5.164889335632324\n",
            "Epoch 2550/6000  training loss: 5.108364105224609 Validation loss 5.103285789489746\n",
            "Epoch 2560/6000  training loss: 5.047430038452148 Validation loss 5.042417526245117\n",
            "Epoch 2570/6000  training loss: 4.987222671508789 Validation loss 4.982275485992432\n",
            "Epoch 2580/6000  training loss: 4.927733898162842 Validation loss 4.922852516174316\n",
            "Epoch 2590/6000  training loss: 4.868956089019775 Validation loss 4.864138126373291\n",
            "Epoch 2600/6000  training loss: 4.810879707336426 Validation loss 4.806124687194824\n",
            "Epoch 2610/6000  training loss: 4.7534966468811035 Validation loss 4.748803615570068\n",
            "Epoch 2620/6000  training loss: 4.696799278259277 Validation loss 4.69216775894165\n",
            "Epoch 2630/6000  training loss: 4.640778541564941 Validation loss 4.636207580566406\n",
            "Epoch 2640/6000  training loss: 4.585427284240723 Validation loss 4.580915927886963\n",
            "Epoch 2650/6000  training loss: 4.530736446380615 Validation loss 4.526284694671631\n",
            "Epoch 2660/6000  training loss: 4.476699352264404 Validation loss 4.472306251525879\n",
            "Epoch 2670/6000  training loss: 4.423308372497559 Validation loss 4.418972015380859\n",
            "Epoch 2680/6000  training loss: 4.3705549240112305 Validation loss 4.366275787353516\n",
            "Epoch 2690/6000  training loss: 4.318431377410889 Validation loss 4.314208507537842\n",
            "Epoch 2700/6000  training loss: 4.26693058013916 Validation loss 4.262763977050781\n",
            "Epoch 2710/6000  training loss: 4.216045379638672 Validation loss 4.211933612823486\n",
            "Epoch 2720/6000  training loss: 4.165768623352051 Validation loss 4.161710262298584\n",
            "Epoch 2730/6000  training loss: 4.116091728210449 Validation loss 4.112087726593018\n",
            "Epoch 2740/6000  training loss: 4.067008972167969 Validation loss 4.063057899475098\n",
            "Epoch 2750/6000  training loss: 4.0185136795043945 Validation loss 4.014614105224609\n",
            "Epoch 2760/6000  training loss: 3.9705970287323 Validation loss 3.966749668121338\n",
            "Epoch 2770/6000  training loss: 3.9232535362243652 Validation loss 3.919457197189331\n",
            "Epoch 2780/6000  training loss: 3.876476526260376 Validation loss 3.8727304935455322\n",
            "Epoch 2790/6000  training loss: 3.8302581310272217 Validation loss 3.826561450958252\n",
            "Epoch 2800/6000  training loss: 3.784592628479004 Validation loss 3.780944585800171\n",
            "Epoch 2810/6000  training loss: 3.7394731044769287 Validation loss 3.7358736991882324\n",
            "Epoch 2820/6000  training loss: 3.6948933601379395 Validation loss 3.6913418769836426\n",
            "Epoch 2830/6000  training loss: 3.6508467197418213 Validation loss 3.6473422050476074\n",
            "Epoch 2840/6000  training loss: 3.6073267459869385 Validation loss 3.6038687229156494\n",
            "Epoch 2850/6000  training loss: 3.5643277168273926 Validation loss 3.560915946960449\n",
            "Epoch 2860/6000  training loss: 3.521843671798706 Validation loss 3.518476963043213\n",
            "Epoch 2870/6000  training loss: 3.4798667430877686 Validation loss 3.476545572280884\n",
            "Epoch 2880/6000  training loss: 3.4383931159973145 Validation loss 3.4351155757904053\n",
            "Epoch 2890/6000  training loss: 3.3974151611328125 Validation loss 3.394181728363037\n",
            "Epoch 2900/6000  training loss: 3.3569281101226807 Validation loss 3.3537375926971436\n",
            "Epoch 2910/6000  training loss: 3.316925048828125 Validation loss 3.313777446746826\n",
            "Epoch 2920/6000  training loss: 3.2774012088775635 Validation loss 3.2742955684661865\n",
            "Epoch 2930/6000  training loss: 3.2383503913879395 Validation loss 3.235285997390747\n",
            "Epoch 2940/6000  training loss: 3.1997668743133545 Validation loss 3.1967437267303467\n",
            "Epoch 2950/6000  training loss: 3.16164493560791 Validation loss 3.1586625576019287\n",
            "Epoch 2960/6000  training loss: 3.123980760574341 Validation loss 3.1210379600524902\n",
            "Epoch 2970/6000  training loss: 3.086766481399536 Validation loss 3.0838632583618164\n",
            "Epoch 2980/6000  training loss: 3.0499978065490723 Validation loss 3.0471343994140625\n",
            "Epoch 2990/6000  training loss: 3.0136702060699463 Validation loss 3.0108449459075928\n",
            "Epoch 3000/6000  training loss: 2.9777770042419434 Validation loss 2.974989652633667\n",
            "Epoch 3010/6000  training loss: 2.9423139095306396 Validation loss 2.9395639896392822\n",
            "Epoch 3020/6000  training loss: 2.907275915145874 Validation loss 2.9045629501342773\n",
            "Epoch 3030/6000  training loss: 2.872657537460327 Validation loss 2.869981527328491\n",
            "Epoch 3040/6000  training loss: 2.838453769683838 Validation loss 2.8358137607574463\n",
            "Epoch 3050/6000  training loss: 2.8046600818634033 Validation loss 2.802055597305298\n",
            "Epoch 3060/6000  training loss: 2.771271228790283 Validation loss 2.7687020301818848\n",
            "Epoch 3070/6000  training loss: 2.7382822036743164 Validation loss 2.735748291015625\n",
            "Epoch 3080/6000  training loss: 2.705688953399658 Validation loss 2.7031893730163574\n",
            "Epoch 3090/6000  training loss: 2.6734864711761475 Validation loss 2.6710205078125\n",
            "Epoch 3100/6000  training loss: 2.641669273376465 Validation loss 2.63923716545105\n",
            "Epoch 3110/6000  training loss: 2.610234022140503 Validation loss 2.607834577560425\n",
            "Epoch 3120/6000  training loss: 2.5791759490966797 Validation loss 2.5768091678619385\n",
            "Epoch 3130/6000  training loss: 2.5484893321990967 Validation loss 2.5461552143096924\n",
            "Epoch 3140/6000  training loss: 2.5181713104248047 Validation loss 2.515868902206421\n",
            "Epoch 3150/6000  training loss: 2.4882168769836426 Validation loss 2.485945463180542\n",
            "Epoch 3160/6000  training loss: 2.4586215019226074 Validation loss 2.456381320953369\n",
            "Epoch 3170/6000  training loss: 2.4293808937072754 Validation loss 2.427171468734741\n",
            "Epoch 3180/6000  training loss: 2.400491237640381 Validation loss 2.3983120918273926\n",
            "Epoch 3190/6000  training loss: 2.3719482421875 Validation loss 2.3697986602783203\n",
            "Epoch 3200/6000  training loss: 2.34374737739563 Validation loss 2.341627836227417\n",
            "Epoch 3210/6000  training loss: 2.315885066986084 Validation loss 2.3137943744659424\n",
            "Epoch 3220/6000  training loss: 2.2883567810058594 Validation loss 2.286295175552368\n",
            "Epoch 3230/6000  training loss: 2.2611591815948486 Validation loss 2.2591257095336914\n",
            "Epoch 3240/6000  training loss: 2.234287977218628 Validation loss 2.2322824001312256\n",
            "Epoch 3250/6000  training loss: 2.2077386379241943 Validation loss 2.2057607173919678\n",
            "Epoch 3260/6000  training loss: 2.1815083026885986 Validation loss 2.1795578002929688\n",
            "Epoch 3270/6000  training loss: 2.155592679977417 Validation loss 2.1536693572998047\n",
            "Epoch 3280/6000  training loss: 2.129987955093384 Validation loss 2.128091335296631\n",
            "Epoch 3290/6000  training loss: 2.10469126701355 Validation loss 2.102820873260498\n",
            "Epoch 3300/6000  training loss: 2.079697847366333 Validation loss 2.0778536796569824\n",
            "Epoch 3310/6000  training loss: 2.055004358291626 Validation loss 2.0531859397888184\n",
            "Epoch 3320/6000  training loss: 2.0306077003479004 Validation loss 2.0288143157958984\n",
            "Epoch 3330/6000  training loss: 2.0065038204193115 Validation loss 2.0047354698181152\n",
            "Epoch 3340/6000  training loss: 1.982689380645752 Validation loss 1.9809460639953613\n",
            "Epoch 3350/6000  training loss: 1.9591608047485352 Validation loss 1.9574419260025024\n",
            "Epoch 3360/6000  training loss: 1.935915231704712 Validation loss 1.9342201948165894\n",
            "Epoch 3370/6000  training loss: 1.9129482507705688 Validation loss 1.911277174949646\n",
            "Epoch 3380/6000  training loss: 1.890257477760315 Validation loss 1.8886100053787231\n",
            "Epoch 3390/6000  training loss: 1.8678393363952637 Validation loss 1.8662148714065552\n",
            "Epoch 3400/6000  training loss: 1.845690369606018 Validation loss 1.8440889120101929\n",
            "Epoch 3410/6000  training loss: 1.823807716369629 Validation loss 1.8222289085388184\n",
            "Epoch 3420/6000  training loss: 1.8021880388259888 Validation loss 1.8006315231323242\n",
            "Epoch 3430/6000  training loss: 1.7808277606964111 Validation loss 1.779293417930603\n",
            "Epoch 3440/6000  training loss: 1.759724497795105 Validation loss 1.7582120895385742\n",
            "Epoch 3450/6000  training loss: 1.7388745546340942 Validation loss 1.7373838424682617\n",
            "Epoch 3460/6000  training loss: 1.7182756662368774 Validation loss 1.7168059349060059\n",
            "Epoch 3470/6000  training loss: 1.6979238986968994 Validation loss 1.6964752674102783\n",
            "Epoch 3480/6000  training loss: 1.6778167486190796 Validation loss 1.6763890981674194\n",
            "Epoch 3490/6000  training loss: 1.6579514741897583 Validation loss 1.6565443277359009\n",
            "Epoch 3500/6000  training loss: 1.6383250951766968 Validation loss 1.636938214302063\n",
            "Epoch 3510/6000  training loss: 1.6189345121383667 Validation loss 1.6175674200057983\n",
            "Epoch 3520/6000  training loss: 1.5997769832611084 Validation loss 1.5984300374984741\n",
            "Epoch 3530/6000  training loss: 1.5808500051498413 Validation loss 1.5795223712921143\n",
            "Epoch 3540/6000  training loss: 1.5621505975723267 Validation loss 1.5608422756195068\n",
            "Epoch 3550/6000  training loss: 1.5436758995056152 Validation loss 1.5423864126205444\n",
            "Epoch 3560/6000  training loss: 1.5254234075546265 Validation loss 1.5241529941558838\n",
            "Epoch 3570/6000  training loss: 1.5073904991149902 Validation loss 1.506138563156128\n",
            "Epoch 3580/6000  training loss: 1.4895745515823364 Validation loss 1.4883408546447754\n",
            "Epoch 3590/6000  training loss: 1.4719725847244263 Validation loss 1.4707571268081665\n",
            "Epoch 3600/6000  training loss: 1.4545825719833374 Validation loss 1.4533851146697998\n",
            "Epoch 3610/6000  training loss: 1.4374016523361206 Validation loss 1.436221957206726\n",
            "Epoch 3620/6000  training loss: 1.4204274415969849 Validation loss 1.4192650318145752\n",
            "Epoch 3630/6000  training loss: 1.4036575555801392 Validation loss 1.4025124311447144\n",
            "Epoch 3640/6000  training loss: 1.3870893716812134 Validation loss 1.3859612941741943\n",
            "Epoch 3650/6000  training loss: 1.370720386505127 Validation loss 1.3696093559265137\n",
            "Epoch 3660/6000  training loss: 1.354548454284668 Validation loss 1.3534537553787231\n",
            "Epoch 3670/6000  training loss: 1.3385710716247559 Validation loss 1.3374930620193481\n",
            "Epoch 3680/6000  training loss: 1.3227860927581787 Validation loss 1.3217239379882812\n",
            "Epoch 3690/6000  training loss: 1.3071908950805664 Validation loss 1.3061449527740479\n",
            "Epoch 3700/6000  training loss: 1.291783332824707 Validation loss 1.2907532453536987\n",
            "Epoch 3710/6000  training loss: 1.2765614986419678 Validation loss 1.2755471467971802\n",
            "Epoch 3720/6000  training loss: 1.2615225315093994 Validation loss 1.2605236768722534\n",
            "Epoch 3730/6000  training loss: 1.2466648817062378 Validation loss 1.2456810474395752\n",
            "Epoch 3740/6000  training loss: 1.2319858074188232 Validation loss 1.231016993522644\n",
            "Epoch 3750/6000  training loss: 1.2174835205078125 Validation loss 1.2165298461914062\n",
            "Epoch 3760/6000  training loss: 1.2031558752059937 Validation loss 1.202216625213623\n",
            "Epoch 3770/6000  training loss: 1.1890006065368652 Validation loss 1.1880760192871094\n",
            "Epoch 3780/6000  training loss: 1.1750158071517944 Validation loss 1.1741056442260742\n",
            "Epoch 3790/6000  training loss: 1.1611993312835693 Validation loss 1.1603031158447266\n",
            "Epoch 3800/6000  training loss: 1.147549033164978 Validation loss 1.1466668844223022\n",
            "Epoch 3810/6000  training loss: 1.1340632438659668 Validation loss 1.1331950426101685\n",
            "Epoch 3820/6000  training loss: 1.1207396984100342 Validation loss 1.1198850870132446\n",
            "Epoch 3830/6000  training loss: 1.107576847076416 Validation loss 1.1067357063293457\n",
            "Epoch 3840/6000  training loss: 1.0945724248886108 Validation loss 1.0937446355819702\n",
            "Epoch 3850/6000  training loss: 1.0817245244979858 Validation loss 1.0809098482131958\n",
            "Epoch 3860/6000  training loss: 1.0690313577651978 Validation loss 1.0682296752929688\n",
            "Epoch 3870/6000  training loss: 1.0564912557601929 Validation loss 1.0557023286819458\n",
            "Epoch 3880/6000  training loss: 1.0441019535064697 Validation loss 1.0433257818222046\n",
            "Epoch 3890/6000  training loss: 1.0318617820739746 Validation loss 1.0310982465744019\n",
            "Epoch 3900/6000  training loss: 1.0197691917419434 Validation loss 1.0190179347991943\n",
            "Epoch 3910/6000  training loss: 1.0078221559524536 Validation loss 1.0070831775665283\n",
            "Epoch 3920/6000  training loss: 0.9960192441940308 Validation loss 0.9952921867370605\n",
            "Epoch 3930/6000  training loss: 0.9843581914901733 Validation loss 0.983643114566803\n",
            "Epoch 3940/6000  training loss: 0.9728377461433411 Validation loss 0.9721344113349915\n",
            "Epoch 3950/6000  training loss: 0.9614561200141907 Validation loss 0.960764467716217\n",
            "Epoch 3960/6000  training loss: 0.9502115845680237 Validation loss 0.9495314955711365\n",
            "Epoch 3970/6000  training loss: 0.9391025304794312 Validation loss 0.9384337663650513\n",
            "Epoch 3980/6000  training loss: 0.9281272292137146 Validation loss 0.9274695515632629\n",
            "Epoch 3990/6000  training loss: 0.9172841310501099 Validation loss 0.9166375398635864\n",
            "Epoch 4000/6000  training loss: 0.9065716862678528 Validation loss 0.905936062335968\n",
            "Epoch 4010/6000  training loss: 0.8959882259368896 Validation loss 0.895363450050354\n",
            "Epoch 4020/6000  training loss: 0.8855324387550354 Validation loss 0.884918212890625\n",
            "Epoch 4030/6000  training loss: 0.8752025961875916 Validation loss 0.8745989799499512\n",
            "Epoch 4040/6000  training loss: 0.8649970889091492 Validation loss 0.8644039034843445\n",
            "Epoch 4050/6000  training loss: 0.854914665222168 Validation loss 0.8543316125869751\n",
            "Epoch 4060/6000  training loss: 0.8449536561965942 Validation loss 0.8443809151649475\n",
            "Epoch 4070/6000  training loss: 0.8351128101348877 Validation loss 0.8345500230789185\n",
            "Epoch 4080/6000  training loss: 0.8253903388977051 Validation loss 0.8248374462127686\n",
            "Epoch 4090/6000  training loss: 0.8157851696014404 Validation loss 0.8152419924736023\n",
            "Epoch 4100/6000  training loss: 0.80629563331604 Validation loss 0.8057621717453003\n",
            "Epoch 4110/6000  training loss: 0.796920657157898 Validation loss 0.7963966727256775\n",
            "Epoch 4120/6000  training loss: 0.7876584529876709 Validation loss 0.7871439456939697\n",
            "Epoch 4130/6000  training loss: 0.7785078883171082 Validation loss 0.7780026793479919\n",
            "Epoch 4140/6000  training loss: 0.7694676518440247 Validation loss 0.7689716219902039\n",
            "Epoch 4150/6000  training loss: 0.7605364322662354 Validation loss 0.7600495219230652\n",
            "Epoch 4160/6000  training loss: 0.7517126798629761 Validation loss 0.7512346506118774\n",
            "Epoch 4170/6000  training loss: 0.7429953217506409 Validation loss 0.7425261735916138\n",
            "Epoch 4180/6000  training loss: 0.7343831062316895 Validation loss 0.7339226603507996\n",
            "Epoch 4190/6000  training loss: 0.7258746027946472 Validation loss 0.725422739982605\n",
            "Epoch 4200/6000  training loss: 0.7174686789512634 Validation loss 0.7170254588127136\n",
            "Epoch 4210/6000  training loss: 0.709164023399353 Validation loss 0.7087292075157166\n",
            "Epoch 4220/6000  training loss: 0.7009595632553101 Validation loss 0.7005329728126526\n",
            "Epoch 4230/6000  training loss: 0.6928539276123047 Validation loss 0.6924355626106262\n",
            "Epoch 4240/6000  training loss: 0.6848458647727966 Validation loss 0.6844357848167419\n",
            "Epoch 4250/6000  training loss: 0.6769344806671143 Validation loss 0.6765323281288147\n",
            "Epoch 4260/6000  training loss: 0.6691184639930725 Validation loss 0.6687242388725281\n",
            "Epoch 4270/6000  training loss: 0.6613966226577759 Validation loss 0.6610101461410522\n",
            "Epoch 4280/6000  training loss: 0.6537677645683289 Validation loss 0.653389036655426\n",
            "Epoch 4290/6000  training loss: 0.646230936050415 Validation loss 0.6458598971366882\n",
            "Epoch 4300/6000  training loss: 0.6387849450111389 Validation loss 0.6384214162826538\n",
            "Epoch 4310/6000  training loss: 0.6314286589622498 Validation loss 0.6310725212097168\n",
            "Epoch 4320/6000  training loss: 0.6241610646247864 Validation loss 0.6238123774528503\n",
            "Epoch 4330/6000  training loss: 0.6169810891151428 Validation loss 0.6166395545005798\n",
            "Epoch 4340/6000  training loss: 0.6098875999450684 Validation loss 0.609553337097168\n",
            "Epoch 4350/6000  training loss: 0.6028796434402466 Validation loss 0.6025524735450745\n",
            "Epoch 4360/6000  training loss: 0.5959562063217163 Validation loss 0.5956359505653381\n",
            "Epoch 4370/6000  training loss: 0.5891162157058716 Validation loss 0.5888029336929321\n",
            "Epoch 4380/6000  training loss: 0.5823586583137512 Validation loss 0.5820521712303162\n",
            "Epoch 4390/6000  training loss: 0.575682520866394 Validation loss 0.5753827691078186\n",
            "Epoch 4400/6000  training loss: 0.5690869092941284 Validation loss 0.5687938332557678\n",
            "Epoch 4410/6000  training loss: 0.5625708103179932 Validation loss 0.5622842311859131\n",
            "Epoch 4420/6000  training loss: 0.5561331510543823 Validation loss 0.5558531284332275\n",
            "Epoch 4430/6000  training loss: 0.5497732162475586 Validation loss 0.54949951171875\n",
            "Epoch 4440/6000  training loss: 0.5434898138046265 Validation loss 0.5432225465774536\n",
            "Epoch 4450/6000  training loss: 0.5372822284698486 Validation loss 0.5370212197303772\n",
            "Epoch 4460/6000  training loss: 0.5311494469642639 Validation loss 0.5308945775032043\n",
            "Epoch 4470/6000  training loss: 0.5250905752182007 Validation loss 0.524841845035553\n",
            "Epoch 4480/6000  training loss: 0.5191048383712769 Validation loss 0.5188620090484619\n",
            "Epoch 4490/6000  training loss: 0.5131911039352417 Validation loss 0.5129542946815491\n",
            "Epoch 4500/6000  training loss: 0.5073487162590027 Validation loss 0.5071178078651428\n",
            "Epoch 4510/6000  training loss: 0.5015767812728882 Validation loss 0.5013515949249268\n",
            "Epoch 4520/6000  training loss: 0.4958743751049042 Validation loss 0.49565497040748596\n",
            "Epoch 4530/6000  training loss: 0.49024075269699097 Validation loss 0.4900270402431488\n",
            "Epoch 4540/6000  training loss: 0.48467496037483215 Validation loss 0.4844668209552765\n",
            "Epoch 4550/6000  training loss: 0.47917628288269043 Validation loss 0.4789736866950989\n",
            "Epoch 4560/6000  training loss: 0.4737439453601837 Validation loss 0.47354674339294434\n",
            "Epoch 4570/6000  training loss: 0.468377023935318 Validation loss 0.4681852459907532\n",
            "Epoch 4580/6000  training loss: 0.46307483315467834 Validation loss 0.46288833022117615\n",
            "Epoch 4590/6000  training loss: 0.4578365087509155 Validation loss 0.45765528082847595\n",
            "Epoch 4600/6000  training loss: 0.45266130566596985 Validation loss 0.45248526334762573\n",
            "Epoch 4610/6000  training loss: 0.4475485384464264 Validation loss 0.44737765192985535\n",
            "Epoch 4620/6000  training loss: 0.4424973130226135 Validation loss 0.442331463098526\n",
            "Epoch 4630/6000  training loss: 0.43750709295272827 Validation loss 0.4373461902141571\n",
            "Epoch 4640/6000  training loss: 0.43257689476013184 Validation loss 0.4324209988117218\n",
            "Epoch 4650/6000  training loss: 0.42770618200302124 Validation loss 0.4275550842285156\n",
            "Epoch 4660/6000  training loss: 0.4228942096233368 Validation loss 0.4227479100227356\n",
            "Epoch 4670/6000  training loss: 0.418140172958374 Validation loss 0.41799861192703247\n",
            "Epoch 4680/6000  training loss: 0.4134434163570404 Validation loss 0.4133066236972809\n",
            "Epoch 4690/6000  training loss: 0.4088032841682434 Validation loss 0.408671110868454\n",
            "Epoch 4700/6000  training loss: 0.4042191207408905 Validation loss 0.4040914475917816\n",
            "Epoch 4710/6000  training loss: 0.3996901214122772 Validation loss 0.39956703782081604\n",
            "Epoch 4720/6000  training loss: 0.395215779542923 Validation loss 0.39509710669517517\n",
            "Epoch 4730/6000  training loss: 0.39079535007476807 Validation loss 0.39068111777305603\n",
            "Epoch 4740/6000  training loss: 0.38642817735671997 Validation loss 0.38631829619407654\n",
            "Epoch 4750/6000  training loss: 0.38211366534233093 Validation loss 0.38200807571411133\n",
            "Epoch 4760/6000  training loss: 0.37785109877586365 Validation loss 0.3777497708797455\n",
            "Epoch 4770/6000  training loss: 0.37363991141319275 Validation loss 0.3735427260398865\n",
            "Epoch 4780/6000  training loss: 0.3694794774055481 Validation loss 0.3693864941596985\n",
            "Epoch 4790/6000  training loss: 0.3653692305088043 Validation loss 0.36528027057647705\n",
            "Epoch 4800/6000  training loss: 0.3613085448741913 Validation loss 0.3612235486507416\n",
            "Epoch 4810/6000  training loss: 0.3572966754436493 Validation loss 0.3572157323360443\n",
            "Epoch 4820/6000  training loss: 0.3533332049846649 Validation loss 0.3532561957836151\n",
            "Epoch 4830/6000  training loss: 0.349417507648468 Validation loss 0.3493443727493286\n",
            "Epoch 4840/6000  training loss: 0.34554895758628845 Validation loss 0.34547969698905945\n",
            "Epoch 4850/6000  training loss: 0.34172698855400085 Validation loss 0.3416615426540375\n",
            "Epoch 4860/6000  training loss: 0.3379511535167694 Validation loss 0.3378894031047821\n",
            "Epoch 4870/6000  training loss: 0.3342207968235016 Validation loss 0.33416274189949036\n",
            "Epoch 4880/6000  training loss: 0.3305353820323944 Validation loss 0.3304809629917145\n",
            "Epoch 4890/6000  training loss: 0.3268943727016449 Validation loss 0.3268435597419739\n",
            "Epoch 4900/6000  training loss: 0.3232972323894501 Validation loss 0.3232499957084656\n",
            "Epoch 4910/6000  training loss: 0.31974345445632935 Validation loss 0.3196996748447418\n",
            "Epoch 4920/6000  training loss: 0.3162324130535126 Validation loss 0.3161921203136444\n",
            "Epoch 4930/6000  training loss: 0.3127637505531311 Validation loss 0.3127268850803375\n",
            "Epoch 4940/6000  training loss: 0.30933690071105957 Validation loss 0.3093033730983734\n",
            "Epoch 4950/6000  training loss: 0.30595120787620544 Validation loss 0.30592110753059387\n",
            "Epoch 4960/6000  training loss: 0.30260640382766724 Validation loss 0.3025795519351959\n",
            "Epoch 4970/6000  training loss: 0.2993018925189972 Validation loss 0.2992783188819885\n",
            "Epoch 4980/6000  training loss: 0.2960371971130371 Validation loss 0.2960168123245239\n",
            "Epoch 4990/6000  training loss: 0.2928118109703064 Validation loss 0.2927946448326111\n",
            "Epoch 5000/6000  training loss: 0.28962525725364685 Validation loss 0.28961122035980225\n",
            "Epoch 5010/6000  training loss: 0.28647711873054504 Validation loss 0.286466121673584\n",
            "Epoch 5020/6000  training loss: 0.283366858959198 Validation loss 0.28335896134376526\n",
            "Epoch 5030/6000  training loss: 0.28029412031173706 Validation loss 0.2802892327308655\n",
            "Epoch 5040/6000  training loss: 0.27725839614868164 Validation loss 0.27725645899772644\n",
            "Epoch 5050/6000  training loss: 0.27425920963287354 Validation loss 0.2742602229118347\n",
            "Epoch 5060/6000  training loss: 0.2712961733341217 Validation loss 0.2713000774383545\n",
            "Epoch 5070/6000  training loss: 0.26836881041526794 Validation loss 0.26837560534477234\n",
            "Epoch 5080/6000  training loss: 0.26547670364379883 Validation loss 0.26548638939857483\n",
            "Epoch 5090/6000  training loss: 0.2626194953918457 Validation loss 0.262631893157959\n",
            "Epoch 5100/6000  training loss: 0.2597966492176056 Validation loss 0.2598118782043457\n",
            "Epoch 5110/6000  training loss: 0.25700780749320984 Validation loss 0.25702568888664246\n",
            "Epoch 5120/6000  training loss: 0.2542525827884674 Validation loss 0.2542731463909149\n",
            "Epoch 5130/6000  training loss: 0.2515304684638977 Validation loss 0.2515537142753601\n",
            "Epoch 5140/6000  training loss: 0.24884118139743805 Validation loss 0.24886706471443176\n",
            "Epoch 5150/6000  training loss: 0.246184304356575 Validation loss 0.24621273577213287\n",
            "Epoch 5160/6000  training loss: 0.24355937540531158 Validation loss 0.24359039962291718\n",
            "Epoch 5170/6000  training loss: 0.2409660816192627 Validation loss 0.24099962413311005\n",
            "Epoch 5180/6000  training loss: 0.2384040206670761 Validation loss 0.23844008147716522\n",
            "Epoch 5190/6000  training loss: 0.2358727753162384 Validation loss 0.2359113097190857\n",
            "Epoch 5200/6000  training loss: 0.23337207734584808 Validation loss 0.2334129959344864\n",
            "Epoch 5210/6000  training loss: 0.23090144991874695 Validation loss 0.23094481229782104\n",
            "Epoch 5220/6000  training loss: 0.22846056520938873 Validation loss 0.22850628197193146\n",
            "Epoch 5230/6000  training loss: 0.22604911029338837 Validation loss 0.22609713673591614\n",
            "Epoch 5240/6000  training loss: 0.22366666793823242 Validation loss 0.22371703386306763\n",
            "Epoch 5250/6000  training loss: 0.22131291031837463 Validation loss 0.2213655561208725\n",
            "Epoch 5260/6000  training loss: 0.21898747980594635 Validation loss 0.21904239058494568\n",
            "Epoch 5270/6000  training loss: 0.2166900485754013 Validation loss 0.21674717962741852\n",
            "Epoch 5280/6000  training loss: 0.21442028880119324 Validation loss 0.21447961032390594\n",
            "Epoch 5290/6000  training loss: 0.21217787265777588 Validation loss 0.2122393399477005\n",
            "Epoch 5300/6000  training loss: 0.20996242761611938 Validation loss 0.2100260704755783\n",
            "Epoch 5310/6000  training loss: 0.20777368545532227 Validation loss 0.20783939957618713\n",
            "Epoch 5320/6000  training loss: 0.20561127364635468 Validation loss 0.20567907392978668\n",
            "Epoch 5330/6000  training loss: 0.20347490906715393 Validation loss 0.20354478061199188\n",
            "Epoch 5340/6000  training loss: 0.20136421918869019 Validation loss 0.2014361172914505\n",
            "Epoch 5350/6000  training loss: 0.19927893579006195 Validation loss 0.19935287535190582\n",
            "Epoch 5360/6000  training loss: 0.19721882045269012 Validation loss 0.1972947120666504\n",
            "Epoch 5370/6000  training loss: 0.19518345594406128 Validation loss 0.19526129961013794\n",
            "Epoch 5380/6000  training loss: 0.1931726336479187 Validation loss 0.19325239956378937\n",
            "Epoch 5390/6000  training loss: 0.19118595123291016 Validation loss 0.19126763939857483\n",
            "Epoch 5400/6000  training loss: 0.1892232447862625 Validation loss 0.18930679559707642\n",
            "Epoch 5410/6000  training loss: 0.18728412687778473 Validation loss 0.18736954033374786\n",
            "Epoch 5420/6000  training loss: 0.1853683590888977 Validation loss 0.18545563519001007\n",
            "Epoch 5430/6000  training loss: 0.18347564339637756 Validation loss 0.18356473743915558\n",
            "Epoch 5440/6000  training loss: 0.181605726480484 Validation loss 0.1816965937614441\n",
            "Epoch 5450/6000  training loss: 0.17975829541683197 Validation loss 0.17985093593597412\n",
            "Epoch 5460/6000  training loss: 0.17793311178684235 Validation loss 0.1780274659395218\n",
            "Epoch 5470/6000  training loss: 0.17612987756729126 Validation loss 0.17622597515583038\n",
            "Epoch 5480/6000  training loss: 0.1743483692407608 Validation loss 0.17444613575935364\n",
            "Epoch 5490/6000  training loss: 0.1725882589817047 Validation loss 0.17268773913383484\n",
            "Epoch 5500/6000  training loss: 0.17084935307502747 Validation loss 0.17095047235488892\n",
            "Epoch 5510/6000  training loss: 0.16913138329982758 Validation loss 0.16923414170742035\n",
            "Epoch 5520/6000  training loss: 0.16743408143520355 Validation loss 0.16753843426704407\n",
            "Epoch 5530/6000  training loss: 0.1657571941614151 Validation loss 0.16586314141750336\n",
            "Epoch 5540/6000  training loss: 0.16410048305988312 Validation loss 0.16420800983905792\n",
            "Epoch 5550/6000  training loss: 0.1624636948108673 Validation loss 0.16257275640964508\n",
            "Epoch 5560/6000  training loss: 0.16084660589694977 Validation loss 0.1609572172164917\n",
            "Epoch 5570/6000  training loss: 0.15924900770187378 Validation loss 0.1593611240386963\n",
            "Epoch 5580/6000  training loss: 0.15767060220241547 Validation loss 0.15778422355651855\n",
            "Epoch 5590/6000  training loss: 0.1561111956834793 Validation loss 0.1562262773513794\n",
            "Epoch 5600/6000  training loss: 0.1545705497264862 Validation loss 0.1546870768070221\n",
            "Epoch 5610/6000  training loss: 0.15304844081401825 Validation loss 0.15316639840602875\n",
            "Epoch 5620/6000  training loss: 0.15154461562633514 Validation loss 0.15166398882865906\n",
            "Epoch 5630/6000  training loss: 0.15005889534950256 Validation loss 0.1501796841621399\n",
            "Epoch 5640/6000  training loss: 0.1485910415649414 Validation loss 0.14871321618556976\n",
            "Epoch 5650/6000  training loss: 0.14714087545871735 Validation loss 0.14726442098617554\n",
            "Epoch 5660/6000  training loss: 0.1457081288099289 Validation loss 0.14583301544189453\n",
            "Epoch 5670/6000  training loss: 0.14429263770580292 Validation loss 0.1444188356399536\n",
            "Epoch 5680/6000  training loss: 0.14289416372776031 Validation loss 0.14302167296409607\n",
            "Epoch 5690/6000  training loss: 0.14151251316070557 Validation loss 0.14164131879806519\n",
            "Epoch 5700/6000  training loss: 0.14014746248722076 Validation loss 0.14027754962444305\n",
            "Epoch 5710/6000  training loss: 0.13879883289337158 Validation loss 0.13893018662929535\n",
            "Epoch 5720/6000  training loss: 0.1374664455652237 Validation loss 0.13759905099868774\n",
            "Epoch 5730/6000  training loss: 0.136150062084198 Validation loss 0.13628390431404114\n",
            "Epoch 5740/6000  training loss: 0.13484951853752136 Validation loss 0.1349845677614212\n",
            "Epoch 5750/6000  training loss: 0.13356463611125946 Validation loss 0.13370084762573242\n",
            "Epoch 5760/6000  training loss: 0.13229519128799438 Validation loss 0.13243259489536285\n",
            "Epoch 5770/6000  training loss: 0.1310410052537918 Validation loss 0.1311795711517334\n",
            "Epoch 5780/6000  training loss: 0.1298018991947174 Validation loss 0.12994162738323212\n",
            "Epoch 5790/6000  training loss: 0.12857772409915924 Validation loss 0.1287185549736023\n",
            "Epoch 5800/6000  training loss: 0.12736822664737701 Validation loss 0.127510204911232\n",
            "Epoch 5810/6000  training loss: 0.12617328763008118 Validation loss 0.1263163685798645\n",
            "Epoch 5820/6000  training loss: 0.1249927282333374 Validation loss 0.12513689696788788\n",
            "Epoch 5830/6000  training loss: 0.12382634729146957 Validation loss 0.1239716038107872\n",
            "Epoch 5840/6000  training loss: 0.12267401069402695 Validation loss 0.12282031774520874\n",
            "Epoch 5850/6000  training loss: 0.1215355321764946 Validation loss 0.12168288230895996\n",
            "Epoch 5860/6000  training loss: 0.1204107254743576 Validation loss 0.12055911123752594\n",
            "Epoch 5870/6000  training loss: 0.11929944157600403 Validation loss 0.11944885551929474\n",
            "Epoch 5880/6000  training loss: 0.11820152401924133 Validation loss 0.11835195124149323\n",
            "Epoch 5890/6000  training loss: 0.11711680889129639 Validation loss 0.11726823449134827\n",
            "Epoch 5900/6000  training loss: 0.11604512482881546 Validation loss 0.11619751155376434\n",
            "Epoch 5910/6000  training loss: 0.11498631536960602 Validation loss 0.11513969302177429\n",
            "Epoch 5920/6000  training loss: 0.11394023150205612 Validation loss 0.1140945702791214\n",
            "Epoch 5930/6000  training loss: 0.11290673166513443 Validation loss 0.11306200921535492\n",
            "Epoch 5940/6000  training loss: 0.111885666847229 Validation loss 0.11204186081886292\n",
            "Epoch 5950/6000  training loss: 0.11087685823440552 Validation loss 0.11103397607803345\n",
            "Epoch 5960/6000  training loss: 0.10988018661737442 Validation loss 0.11003820598125458\n",
            "Epoch 5970/6000  training loss: 0.10889549553394318 Validation loss 0.10905441641807556\n",
            "Epoch 5980/6000  training loss: 0.10792263597249985 Validation loss 0.10808243602514267\n",
            "Epoch 5990/6000  training loss: 0.1069614589214325 Validation loss 0.10712215304374695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss, label=\"Taining Loss\")\n",
        "plt.plot(loss_val, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mean Square Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "4Acoy8QfvHWg",
        "outputId": "36b15730-dcc4-4080-9934-bc046778820d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Mean Square Error')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(np.float32(X_val))"
      ],
      "metadata": {
        "id": "0n6UZkupvLTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "print(\"R2 Score is {} and MSE {}\".format(\\\n",
        "       r2_score(y_val, Y_pred),\\\n",
        "       mean_squared_error(y_val, Y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK__HRymvO7m",
        "outputId": "168a1d0e-54f6-4340-9a56-ddad3235f3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score is 0.6767650419256344 and MSE 0.016636407741512267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wine quality classification using MLPs in TensorFlow - Code Sample**"
      ],
      "metadata": {
        "id": "D9p7B8185DZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "OMn1LkLe44k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.relu, tf.nn.sigmoid], learning_rate= 0.001):\n",
        "        self.n_input = n_input # Number of inputs to the neuron\n",
        "        self.act_fn = act_func\n",
        "        seed = 456\n",
        "\n",
        "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n",
        "        self.y = tf.placeholder(tf.float32, name='Y')\n",
        "\n",
        "        # Build the graph for a single neuron\n",
        "        # Hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev=2, seed = seed), name = \"weights\")\n",
        "        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed), name=\"bias\")\n",
        "        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n",
        "        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n",
        "\n",
        "\n",
        "        # Output Layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output], stddev=2, seed = seed), name = \"weights\")\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed), name=\"bias\")\n",
        "        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n",
        "        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n",
        "\n",
        "\n",
        "        activity1 = tf.matmul(self.X, self.W1) + self.b1\n",
        "        h1 = self.act_fn[0](activity1)\n",
        "\n",
        "        activity2 = tf.matmul(h1, self.W2) + self.b2\n",
        "        self.y_hat = self.act_fn[1](activity2)\n",
        "\n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat, labels=self.y))\n",
        "        self.opt =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
        "\n",
        "\n",
        "        tf.summary.scalar(\"loss\",self.loss)\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        self.merge = tf.summary.merge_all()\n",
        "        self.writer = tf.summary.FileWriter(\"logs/\", graph=tf.get_default_graph())\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
        "        epoch = 0\n",
        "        X, Y = shuffle(X,Y)\n",
        "        loss = []\n",
        "        loss_val = []\n",
        "        while epoch < epochs:\n",
        "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)\n",
        "            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
        "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
        "\n",
        "            loss.append(l)\n",
        "            loss_val.append(l_val)\n",
        "            self.writer.add_summary(merge, epoch)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
        "                      format(epoch,epochs,l, l_val ))\n",
        "\n",
        "\n",
        "            epoch += 1\n",
        "        return loss, loss_val\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
      ],
      "metadata": {
        "id": "AGjImGgu57W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'winequality-red.csv'\n",
        "df = pd.read_csv(filename, sep=';')\n",
        "columns = df.columns.values\n",
        "# Preprocessing and Categorizing wine into two categories\n",
        "X, Y = df[columns[0:-1]], df[columns[-1]]\n",
        "scaler = MinMaxScaler()\n",
        "X_new = scaler.fit_transform(X)\n",
        "#Y.loc[(Y<3.5)]=3\n",
        "Y.loc[(Y<5.5) ] = 2\n",
        "Y.loc[(Y>=5.5)] = 1\n",
        "Y_new = pd.get_dummies(Y)  # One hot encode\n",
        "X_train, X_val, Y_train, y_val = \\\n",
        "  train_test_split(X_new, Y_new, test_size=0.2, random_state=333)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V550A_xp6AsB",
        "outputId": "6d8b3ca3-3b21-40ab-eeb3-3bc6d4b727d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-5dc1c98865bd>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Y.loc[(Y<5.5) ] = 2\n",
            "<ipython-input-64-5dc1c98865bd>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Y.loc[(Y>=5.5)] = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of features (inputs) from the training data\n",
        "_, d = X_train.shape  # Get the number of features from the shape of X_train\n",
        "_, n = Y_train.shape  # Get the number of classes from the shape of Y_train\n",
        "\n",
        "# Initialize the Artificial Neuron model with the determined number of inputs\n",
        "model = ArtificialNeuron(N=d)\n",
        "\n",
        "# Train the model on the training data and validate on the validation data\n",
        "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 10000)  # Train for 10,000 epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xv3nNXu6IiU",
        "outputId": "a6d84900-8fa8-43d4-c7d1-5e23ca4aa38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10000  training loss: 0.29491132497787476 Validation loss 0.29648494720458984\n",
            "Epoch 10/10000  training loss: 0.2948431372642517 Validation loss 0.2964131534099579\n",
            "Epoch 20/10000  training loss: 0.2947750985622406 Validation loss 0.2963414788246155\n",
            "Epoch 30/10000  training loss: 0.29470720887184143 Validation loss 0.2962699830532074\n",
            "Epoch 40/10000  training loss: 0.2946394979953766 Validation loss 0.29619863629341125\n",
            "Epoch 50/10000  training loss: 0.2945719361305237 Validation loss 0.2961274981498718\n",
            "Epoch 60/10000  training loss: 0.2945045232772827 Validation loss 0.29605647921562195\n",
            "Epoch 70/10000  training loss: 0.2944372892379761 Validation loss 0.2959856390953064\n",
            "Epoch 80/10000  training loss: 0.29437020421028137 Validation loss 0.2959149479866028\n",
            "Epoch 90/10000  training loss: 0.29430320858955383 Validation loss 0.2958443760871887\n",
            "Epoch 100/10000  training loss: 0.2942363917827606 Validation loss 0.295773983001709\n",
            "Epoch 110/10000  training loss: 0.29416972398757935 Validation loss 0.2957037389278412\n",
            "Epoch 120/10000  training loss: 0.2941032350063324 Validation loss 0.2956336438655853\n",
            "Epoch 130/10000  training loss: 0.2940368056297302 Validation loss 0.2955636978149414\n",
            "Epoch 140/10000  training loss: 0.29397058486938477 Validation loss 0.29549387097358704\n",
            "Epoch 150/10000  training loss: 0.29390451312065125 Validation loss 0.295424222946167\n",
            "Epoch 160/10000  training loss: 0.2938385605812073 Validation loss 0.2953546941280365\n",
            "Epoch 170/10000  training loss: 0.29377272725105286 Validation loss 0.29528531432151794\n",
            "Epoch 180/10000  training loss: 0.293707013130188 Validation loss 0.29521608352661133\n",
            "Epoch 190/10000  training loss: 0.29364150762557983 Validation loss 0.29514700174331665\n",
            "Epoch 200/10000  training loss: 0.29357612133026123 Validation loss 0.2950780987739563\n",
            "Epoch 210/10000  training loss: 0.29351088404655457 Validation loss 0.2950092852115631\n",
            "Epoch 220/10000  training loss: 0.29344573616981506 Validation loss 0.29494062066078186\n",
            "Epoch 230/10000  training loss: 0.2933807969093323 Validation loss 0.29487210512161255\n",
            "Epoch 240/10000  training loss: 0.29331594705581665 Validation loss 0.29480376839637756\n",
            "Epoch 250/10000  training loss: 0.29325124621391296 Validation loss 0.29473552107810974\n",
            "Epoch 260/10000  training loss: 0.29318666458129883 Validation loss 0.29466742277145386\n",
            "Epoch 270/10000  training loss: 0.2931223511695862 Validation loss 0.29459959268569946\n",
            "Epoch 280/10000  training loss: 0.2930581569671631 Validation loss 0.2945318818092346\n",
            "Epoch 290/10000  training loss: 0.29299405217170715 Validation loss 0.2944642901420593\n",
            "Epoch 300/10000  training loss: 0.29293012619018555 Validation loss 0.2943968176841736\n",
            "Epoch 310/10000  training loss: 0.2928663194179535 Validation loss 0.2943294942378998\n",
            "Epoch 320/10000  training loss: 0.292802631855011 Validation loss 0.2942623496055603\n",
            "Epoch 330/10000  training loss: 0.2927390933036804 Validation loss 0.29419535398483276\n",
            "Epoch 340/10000  training loss: 0.2926757335662842 Validation loss 0.2941284775733948\n",
            "Epoch 350/10000  training loss: 0.2926124632358551 Validation loss 0.29406172037124634\n",
            "Epoch 360/10000  training loss: 0.29254934191703796 Validation loss 0.2939951419830322\n",
            "Epoch 370/10000  training loss: 0.29248639941215515 Validation loss 0.29392868280410767\n",
            "Epoch 380/10000  training loss: 0.2924235761165619 Validation loss 0.29386240243911743\n",
            "Epoch 390/10000  training loss: 0.2923608422279358 Validation loss 0.29379624128341675\n",
            "Epoch 400/10000  training loss: 0.2922983169555664 Validation loss 0.2937301993370056\n",
            "Epoch 410/10000  training loss: 0.2922359108924866 Validation loss 0.2936643362045288\n",
            "Epoch 420/10000  training loss: 0.2921735644340515 Validation loss 0.29359859228134155\n",
            "Epoch 430/10000  training loss: 0.29211145639419556 Validation loss 0.2935330271720886\n",
            "Epoch 440/10000  training loss: 0.29204946756362915 Validation loss 0.29346752166748047\n",
            "Epoch 450/10000  training loss: 0.2919875681400299 Validation loss 0.29340219497680664\n",
            "Epoch 460/10000  training loss: 0.291925847530365 Validation loss 0.29333701729774475\n",
            "Epoch 470/10000  training loss: 0.2918642461299896 Validation loss 0.2932719886302948\n",
            "Epoch 480/10000  training loss: 0.29180285334587097 Validation loss 0.2932071387767792\n",
            "Epoch 490/10000  training loss: 0.2917415499687195 Validation loss 0.2931424677371979\n",
            "Epoch 500/10000  training loss: 0.29168039560317993 Validation loss 0.29307788610458374\n",
            "Epoch 510/10000  training loss: 0.2916194498538971 Validation loss 0.2930135130882263\n",
            "Epoch 520/10000  training loss: 0.2915585935115814 Validation loss 0.29294925928115845\n",
            "Epoch 530/10000  training loss: 0.2914979159832001 Validation loss 0.2928851246833801\n",
            "Epoch 540/10000  training loss: 0.2914372980594635 Validation loss 0.29282113909721375\n",
            "Epoch 550/10000  training loss: 0.29137685894966125 Validation loss 0.2927572727203369\n",
            "Epoch 560/10000  training loss: 0.29131653904914856 Validation loss 0.292693555355072\n",
            "Epoch 570/10000  training loss: 0.2912563681602478 Validation loss 0.29263001680374146\n",
            "Epoch 580/10000  training loss: 0.2911962866783142 Validation loss 0.29256656765937805\n",
            "Epoch 590/10000  training loss: 0.29113641381263733 Validation loss 0.292503297328949\n",
            "Epoch 600/10000  training loss: 0.2910766303539276 Validation loss 0.29244014620780945\n",
            "Epoch 610/10000  training loss: 0.29101699590682983 Validation loss 0.2923771142959595\n",
            "Epoch 620/10000  training loss: 0.2909574806690216 Validation loss 0.29231423139572144\n",
            "Epoch 630/10000  training loss: 0.2908981740474701 Validation loss 0.2922515273094177\n",
            "Epoch 640/10000  training loss: 0.29083892703056335 Validation loss 0.29218894243240356\n",
            "Epoch 650/10000  training loss: 0.29077982902526855 Validation loss 0.29212650656700134\n",
            "Epoch 660/10000  training loss: 0.2907208800315857 Validation loss 0.2920641601085663\n",
            "Epoch 670/10000  training loss: 0.2906620502471924 Validation loss 0.29200202226638794\n",
            "Epoch 680/10000  training loss: 0.290603369474411 Validation loss 0.29193997383117676\n",
            "Epoch 690/10000  training loss: 0.2905448079109192 Validation loss 0.2918780744075775\n",
            "Epoch 700/10000  training loss: 0.2904863655567169 Validation loss 0.2918163239955902\n",
            "Epoch 710/10000  training loss: 0.29042819142341614 Validation loss 0.2917547821998596\n",
            "Epoch 720/10000  training loss: 0.29037004709243774 Validation loss 0.2916933596134186\n",
            "Epoch 730/10000  training loss: 0.29031211137771606 Validation loss 0.2916320860385895\n",
            "Epoch 740/10000  training loss: 0.29025426506996155 Validation loss 0.2915709316730499\n",
            "Epoch 750/10000  training loss: 0.29019656777381897 Validation loss 0.2915099263191223\n",
            "Epoch 760/10000  training loss: 0.29013901948928833 Validation loss 0.29144901037216187\n",
            "Epoch 770/10000  training loss: 0.29008159041404724 Validation loss 0.2913883328437805\n",
            "Epoch 780/10000  training loss: 0.2900243103504181 Validation loss 0.2913277745246887\n",
            "Epoch 790/10000  training loss: 0.28996720910072327 Validation loss 0.2912673056125641\n",
            "Epoch 800/10000  training loss: 0.2899101972579956 Validation loss 0.2912070155143738\n",
            "Epoch 810/10000  training loss: 0.2898533046245575 Validation loss 0.2911468744277954\n",
            "Epoch 820/10000  training loss: 0.28979653120040894 Validation loss 0.2910868525505066\n",
            "Epoch 830/10000  training loss: 0.2897399365901947 Validation loss 0.2910269796848297\n",
            "Epoch 840/10000  training loss: 0.2896834909915924 Validation loss 0.2909672260284424\n",
            "Epoch 850/10000  training loss: 0.2896271049976349 Validation loss 0.2909075915813446\n",
            "Epoch 860/10000  training loss: 0.28957095742225647 Validation loss 0.29084810614585876\n",
            "Epoch 870/10000  training loss: 0.28951483964920044 Validation loss 0.29078879952430725\n",
            "Epoch 880/10000  training loss: 0.28945887088775635 Validation loss 0.2907295823097229\n",
            "Epoch 890/10000  training loss: 0.2894030809402466 Validation loss 0.2906705141067505\n",
            "Epoch 900/10000  training loss: 0.289347380399704 Validation loss 0.2906115651130676\n",
            "Epoch 910/10000  training loss: 0.2892918288707733 Validation loss 0.2905527651309967\n",
            "Epoch 920/10000  training loss: 0.2892364263534546 Validation loss 0.2904941141605377\n",
            "Epoch 930/10000  training loss: 0.2891811728477478 Validation loss 0.29043567180633545\n",
            "Epoch 940/10000  training loss: 0.28912603855133057 Validation loss 0.29037731885910034\n",
            "Epoch 950/10000  training loss: 0.28907108306884766 Validation loss 0.29031914472579956\n",
            "Epoch 960/10000  training loss: 0.2890162467956543 Validation loss 0.29026108980178833\n",
            "Epoch 970/10000  training loss: 0.2889615595340729 Validation loss 0.29020312428474426\n",
            "Epoch 980/10000  training loss: 0.28890690207481384 Validation loss 0.2901453375816345\n",
            "Epoch 990/10000  training loss: 0.2888524532318115 Validation loss 0.29008764028549194\n",
            "Epoch 1000/10000  training loss: 0.28879812359809875 Validation loss 0.2900301218032837\n",
            "Epoch 1010/10000  training loss: 0.28874388337135315 Validation loss 0.289972722530365\n",
            "Epoch 1020/10000  training loss: 0.28868982195854187 Validation loss 0.2899154722690582\n",
            "Epoch 1030/10000  training loss: 0.28863587975502014 Validation loss 0.28985828161239624\n",
            "Epoch 1040/10000  training loss: 0.28858205676078796 Validation loss 0.28980129957199097\n",
            "Epoch 1050/10000  training loss: 0.28852835297584534 Validation loss 0.28974443674087524\n",
            "Epoch 1060/10000  training loss: 0.28847479820251465 Validation loss 0.2896876931190491\n",
            "Epoch 1070/10000  training loss: 0.2884213924407959 Validation loss 0.28963109850883484\n",
            "Epoch 1080/10000  training loss: 0.2883680760860443 Validation loss 0.28957465291023254\n",
            "Epoch 1090/10000  training loss: 0.2883148789405823 Validation loss 0.2895182967185974\n",
            "Epoch 1100/10000  training loss: 0.2882618308067322 Validation loss 0.2894620895385742\n",
            "Epoch 1110/10000  training loss: 0.288208931684494 Validation loss 0.2894060015678406\n",
            "Epoch 1120/10000  training loss: 0.288156121969223 Validation loss 0.28935009241104126\n",
            "Epoch 1130/10000  training loss: 0.2881034314632416 Validation loss 0.2892942428588867\n",
            "Epoch 1140/10000  training loss: 0.28805091977119446 Validation loss 0.2892385721206665\n",
            "Epoch 1150/10000  training loss: 0.2879985272884369 Validation loss 0.2891830801963806\n",
            "Epoch 1160/10000  training loss: 0.28794625401496887 Validation loss 0.2891277074813843\n",
            "Epoch 1170/10000  training loss: 0.28789418935775757 Validation loss 0.2890724837779999\n",
            "Epoch 1180/10000  training loss: 0.2878422141075134 Validation loss 0.28901737928390503\n",
            "Epoch 1190/10000  training loss: 0.28779035806655884 Validation loss 0.2889624238014221\n",
            "Epoch 1200/10000  training loss: 0.2877385914325714 Validation loss 0.28890758752822876\n",
            "Epoch 1210/10000  training loss: 0.2876870036125183 Validation loss 0.28885287046432495\n",
            "Epoch 1220/10000  training loss: 0.28763547539711 Validation loss 0.2887982726097107\n",
            "Epoch 1230/10000  training loss: 0.2875841557979584 Validation loss 0.2887438237667084\n",
            "Epoch 1240/10000  training loss: 0.28753289580345154 Validation loss 0.2886894643306732\n",
            "Epoch 1250/10000  training loss: 0.28748178482055664 Validation loss 0.2886352837085724\n",
            "Epoch 1260/10000  training loss: 0.2874307632446289 Validation loss 0.2885812222957611\n",
            "Epoch 1270/10000  training loss: 0.2873799800872803 Validation loss 0.28852730989456177\n",
            "Epoch 1280/10000  training loss: 0.28732922673225403 Validation loss 0.2884734869003296\n",
            "Epoch 1290/10000  training loss: 0.28727859258651733 Validation loss 0.28841981291770935\n",
            "Epoch 1300/10000  training loss: 0.28722813725471497 Validation loss 0.28836625814437866\n",
            "Epoch 1310/10000  training loss: 0.28717777132987976 Validation loss 0.2883128523826599\n",
            "Epoch 1320/10000  training loss: 0.2871274948120117 Validation loss 0.2882595658302307\n",
            "Epoch 1330/10000  training loss: 0.2870773673057556 Validation loss 0.2882063686847687\n",
            "Epoch 1340/10000  training loss: 0.28702738881111145 Validation loss 0.2881533205509186\n",
            "Epoch 1350/10000  training loss: 0.28697752952575684 Validation loss 0.2881004214286804\n",
            "Epoch 1360/10000  training loss: 0.2869277894496918 Validation loss 0.2880476415157318\n",
            "Epoch 1370/10000  training loss: 0.28687816858291626 Validation loss 0.28799501061439514\n",
            "Epoch 1380/10000  training loss: 0.2868287265300751 Validation loss 0.2879425585269928\n",
            "Epoch 1390/10000  training loss: 0.28677940368652344 Validation loss 0.28789022564888\n",
            "Epoch 1400/10000  training loss: 0.28673020005226135 Validation loss 0.28783801198005676\n",
            "Epoch 1410/10000  training loss: 0.2866811156272888 Validation loss 0.2877858579158783\n",
            "Epoch 1420/10000  training loss: 0.28663215041160583 Validation loss 0.28773391246795654\n",
            "Epoch 1430/10000  training loss: 0.2865833342075348 Validation loss 0.28768205642700195\n",
            "Epoch 1440/10000  training loss: 0.2865346074104309 Validation loss 0.2876303791999817\n",
            "Epoch 1450/10000  training loss: 0.28648602962493896 Validation loss 0.287578821182251\n",
            "Epoch 1460/10000  training loss: 0.2864375710487366 Validation loss 0.2875273823738098\n",
            "Epoch 1470/10000  training loss: 0.28638923168182373 Validation loss 0.2874760627746582\n",
            "Epoch 1480/10000  training loss: 0.2863410413265228 Validation loss 0.28742486238479614\n",
            "Epoch 1490/10000  training loss: 0.2862929701805115 Validation loss 0.2873738408088684\n",
            "Epoch 1500/10000  training loss: 0.2862449586391449 Validation loss 0.28732290863990784\n",
            "Epoch 1510/10000  training loss: 0.28619712591171265 Validation loss 0.2872720956802368\n",
            "Epoch 1520/10000  training loss: 0.28614941239356995 Validation loss 0.28722140192985535\n",
            "Epoch 1530/10000  training loss: 0.2861017882823944 Validation loss 0.2871708273887634\n",
            "Epoch 1540/10000  training loss: 0.2860543429851532 Validation loss 0.28712040185928345\n",
            "Epoch 1550/10000  training loss: 0.28600695729255676 Validation loss 0.287070095539093\n",
            "Epoch 1560/10000  training loss: 0.28595972061157227 Validation loss 0.28701987862586975\n",
            "Epoch 1570/10000  training loss: 0.28591254353523254 Validation loss 0.2869698107242584\n",
            "Epoch 1580/10000  training loss: 0.28586554527282715 Validation loss 0.28691989183425903\n",
            "Epoch 1590/10000  training loss: 0.2858186662197113 Validation loss 0.2868700623512268\n",
            "Epoch 1600/10000  training loss: 0.2857719361782074 Validation loss 0.2868204116821289\n",
            "Epoch 1610/10000  training loss: 0.28572529554367065 Validation loss 0.28677088022232056\n",
            "Epoch 1620/10000  training loss: 0.28567880392074585 Validation loss 0.28672146797180176\n",
            "Epoch 1630/10000  training loss: 0.2856324315071106 Validation loss 0.2866722047328949\n",
            "Epoch 1640/10000  training loss: 0.2855862081050873 Validation loss 0.2866230607032776\n",
            "Epoch 1650/10000  training loss: 0.28554004430770874 Validation loss 0.28657400608062744\n",
            "Epoch 1660/10000  training loss: 0.28549399971961975 Validation loss 0.28652507066726685\n",
            "Epoch 1670/10000  training loss: 0.2854481041431427 Validation loss 0.2864763140678406\n",
            "Epoch 1680/10000  training loss: 0.2854023277759552 Validation loss 0.2864276170730591\n",
            "Epoch 1690/10000  training loss: 0.28535664081573486 Validation loss 0.28637903928756714\n",
            "Epoch 1700/10000  training loss: 0.2853110730648041 Validation loss 0.2863306403160095\n",
            "Epoch 1710/10000  training loss: 0.28526565432548523 Validation loss 0.28628233075141907\n",
            "Epoch 1720/10000  training loss: 0.28522032499313354 Validation loss 0.28623414039611816\n",
            "Epoch 1730/10000  training loss: 0.2851751148700714 Validation loss 0.2861860692501068\n",
            "Epoch 1740/10000  training loss: 0.28513002395629883 Validation loss 0.286138117313385\n",
            "Epoch 1750/10000  training loss: 0.2850850522518158 Validation loss 0.28609031438827515\n",
            "Epoch 1760/10000  training loss: 0.2850401699542999 Validation loss 0.28604260087013245\n",
            "Epoch 1770/10000  training loss: 0.284995436668396 Validation loss 0.2859950065612793\n",
            "Epoch 1780/10000  training loss: 0.28495079278945923 Validation loss 0.2859475016593933\n",
            "Epoch 1790/10000  training loss: 0.284906268119812 Validation loss 0.28590017557144165\n",
            "Epoch 1800/10000  training loss: 0.28486189246177673 Validation loss 0.28585296869277954\n",
            "Epoch 1810/10000  training loss: 0.28481757640838623 Validation loss 0.2858058214187622\n",
            "Epoch 1820/10000  training loss: 0.28477343916893005 Validation loss 0.2857588231563568\n",
            "Epoch 1830/10000  training loss: 0.28472939133644104 Validation loss 0.28571200370788574\n",
            "Epoch 1840/10000  training loss: 0.28468549251556396 Validation loss 0.2856653034687042\n",
            "Epoch 1850/10000  training loss: 0.28464171290397644 Validation loss 0.28561872243881226\n",
            "Epoch 1860/10000  training loss: 0.2845980226993561 Validation loss 0.28557223081588745\n",
            "Epoch 1870/10000  training loss: 0.28455445170402527 Validation loss 0.2855258584022522\n",
            "Epoch 1880/10000  training loss: 0.284510999917984 Validation loss 0.2854796051979065\n",
            "Epoch 1890/10000  training loss: 0.2844676971435547 Validation loss 0.28543350100517273\n",
            "Epoch 1900/10000  training loss: 0.28442442417144775 Validation loss 0.28538745641708374\n",
            "Epoch 1910/10000  training loss: 0.28438130021095276 Validation loss 0.2853415906429291\n",
            "Epoch 1920/10000  training loss: 0.2843382954597473 Validation loss 0.2852958142757416\n",
            "Epoch 1930/10000  training loss: 0.2842954099178314 Validation loss 0.28525015711784363\n",
            "Epoch 1940/10000  training loss: 0.2842526435852051 Validation loss 0.28520461916923523\n",
            "Epoch 1950/10000  training loss: 0.2842099368572235 Validation loss 0.2851592004299164\n",
            "Epoch 1960/10000  training loss: 0.28416740894317627 Validation loss 0.2851139008998871\n",
            "Epoch 1970/10000  training loss: 0.2841249704360962 Validation loss 0.28506869077682495\n",
            "Epoch 1980/10000  training loss: 0.28408265113830566 Validation loss 0.28502362966537476\n",
            "Epoch 1990/10000  training loss: 0.2840404212474823 Validation loss 0.2849786877632141\n",
            "Epoch 2000/10000  training loss: 0.2839983105659485 Validation loss 0.28493380546569824\n",
            "Epoch 2010/10000  training loss: 0.28395628929138184 Validation loss 0.2848890721797943\n",
            "Epoch 2020/10000  training loss: 0.28391438722610474 Validation loss 0.28484445810317993\n",
            "Epoch 2030/10000  training loss: 0.2838726341724396 Validation loss 0.2847999632358551\n",
            "Epoch 2040/10000  training loss: 0.2838309705257416 Validation loss 0.28475555777549744\n",
            "Epoch 2050/10000  training loss: 0.28378936648368835 Validation loss 0.2847113013267517\n",
            "Epoch 2060/10000  training loss: 0.28374791145324707 Validation loss 0.28466713428497314\n",
            "Epoch 2070/10000  training loss: 0.2837066054344177 Validation loss 0.2846231162548065\n",
            "Epoch 2080/10000  training loss: 0.28366538882255554 Validation loss 0.28457921743392944\n",
            "Epoch 2090/10000  training loss: 0.2836243212223053 Validation loss 0.2845354676246643\n",
            "Epoch 2100/10000  training loss: 0.28358331322669983 Validation loss 0.28449180722236633\n",
            "Epoch 2110/10000  training loss: 0.2835424542427063 Validation loss 0.28444820642471313\n",
            "Epoch 2120/10000  training loss: 0.2835017442703247 Validation loss 0.28440481424331665\n",
            "Epoch 2130/10000  training loss: 0.2834610939025879 Validation loss 0.28436151146888733\n",
            "Epoch 2140/10000  training loss: 0.2834205627441406 Validation loss 0.28431829810142517\n",
            "Epoch 2150/10000  training loss: 0.2833801805973053 Validation loss 0.28427523374557495\n",
            "Epoch 2160/10000  training loss: 0.28333988785743713 Validation loss 0.28423231840133667\n",
            "Epoch 2170/10000  training loss: 0.28329968452453613 Validation loss 0.28418946266174316\n",
            "Epoch 2180/10000  training loss: 0.2832596004009247 Validation loss 0.2841467261314392\n",
            "Epoch 2190/10000  training loss: 0.283219575881958 Validation loss 0.28410404920578003\n",
            "Epoch 2200/10000  training loss: 0.28317970037460327 Validation loss 0.2840615510940552\n",
            "Epoch 2210/10000  training loss: 0.2831399738788605 Validation loss 0.2840191423892975\n",
            "Epoch 2220/10000  training loss: 0.2831002473831177 Validation loss 0.28397685289382935\n",
            "Epoch 2230/10000  training loss: 0.2830607295036316 Validation loss 0.28393465280532837\n",
            "Epoch 2240/10000  training loss: 0.2830212414264679 Validation loss 0.28389257192611694\n",
            "Epoch 2250/10000  training loss: 0.28298190236091614 Validation loss 0.28385061025619507\n",
            "Epoch 2260/10000  training loss: 0.28294265270233154 Validation loss 0.28380876779556274\n",
            "Epoch 2270/10000  training loss: 0.2829034924507141 Validation loss 0.2837669849395752\n",
            "Epoch 2280/10000  training loss: 0.282864511013031 Validation loss 0.283725380897522\n",
            "Epoch 2290/10000  training loss: 0.2828255593776703 Validation loss 0.2836838364601135\n",
            "Epoch 2300/10000  training loss: 0.2827867567539215 Validation loss 0.283642441034317\n",
            "Epoch 2310/10000  training loss: 0.2827480137348175 Validation loss 0.2836011052131653\n",
            "Epoch 2320/10000  training loss: 0.28270941972732544 Validation loss 0.2835599482059479\n",
            "Epoch 2330/10000  training loss: 0.2826709449291229 Validation loss 0.28351888060569763\n",
            "Epoch 2340/10000  training loss: 0.2826325595378876 Validation loss 0.2834779620170593\n",
            "Epoch 2350/10000  training loss: 0.2825942635536194 Validation loss 0.2834371030330658\n",
            "Epoch 2360/10000  training loss: 0.2825561463832855 Validation loss 0.2833963632583618\n",
            "Epoch 2370/10000  training loss: 0.28251802921295166 Validation loss 0.2833557724952698\n",
            "Epoch 2380/10000  training loss: 0.2824801206588745 Validation loss 0.2833152413368225\n",
            "Epoch 2390/10000  training loss: 0.28244227170944214 Validation loss 0.2832748293876648\n",
            "Epoch 2400/10000  training loss: 0.28240448236465454 Validation loss 0.28323450684547424\n",
            "Epoch 2410/10000  training loss: 0.2823668420314789 Validation loss 0.28319430351257324\n",
            "Epoch 2420/10000  training loss: 0.282329261302948 Validation loss 0.2831542193889618\n",
            "Epoch 2430/10000  training loss: 0.28229179978370667 Validation loss 0.2831142544746399\n",
            "Epoch 2440/10000  training loss: 0.28225448727607727 Validation loss 0.2830743193626404\n",
            "Epoch 2450/10000  training loss: 0.2822171747684479 Validation loss 0.2830345630645752\n",
            "Epoch 2460/10000  training loss: 0.2821800708770752 Validation loss 0.2829948961734772\n",
            "Epoch 2470/10000  training loss: 0.2821429669857025 Validation loss 0.2829553484916687\n",
            "Epoch 2480/10000  training loss: 0.28210607171058655 Validation loss 0.2829158902168274\n",
            "Epoch 2490/10000  training loss: 0.2820691764354706 Validation loss 0.28287655115127563\n",
            "Epoch 2500/10000  training loss: 0.28203245997428894 Validation loss 0.28283730149269104\n",
            "Epoch 2510/10000  training loss: 0.2819958031177521 Validation loss 0.2827981412410736\n",
            "Epoch 2520/10000  training loss: 0.28195926547050476 Validation loss 0.28275907039642334\n",
            "Epoch 2530/10000  training loss: 0.2819228172302246 Validation loss 0.282720148563385\n",
            "Epoch 2540/10000  training loss: 0.2818864583969116 Validation loss 0.28268128633499146\n",
            "Epoch 2550/10000  training loss: 0.2818501889705658 Validation loss 0.28264254331588745\n",
            "Epoch 2560/10000  training loss: 0.2818140983581543 Validation loss 0.2826039493083954\n",
            "Epoch 2570/10000  training loss: 0.2817780077457428 Validation loss 0.2825654447078705\n",
            "Epoch 2580/10000  training loss: 0.281742125749588 Validation loss 0.28252705931663513\n",
            "Epoch 2590/10000  training loss: 0.2817063331604004 Validation loss 0.28248879313468933\n",
            "Epoch 2600/10000  training loss: 0.28167060017585754 Validation loss 0.2824506163597107\n",
            "Epoch 2610/10000  training loss: 0.28163498640060425 Validation loss 0.2824125587940216\n",
            "Epoch 2620/10000  training loss: 0.28159940242767334 Validation loss 0.2823745608329773\n",
            "Epoch 2630/10000  training loss: 0.28156399726867676 Validation loss 0.2823367118835449\n",
            "Epoch 2640/10000  training loss: 0.28152868151664734 Validation loss 0.2822989523410797\n",
            "Epoch 2650/10000  training loss: 0.2814934253692627 Validation loss 0.2822612226009369\n",
            "Epoch 2660/10000  training loss: 0.2814582884311676 Validation loss 0.2822237014770508\n",
            "Epoch 2670/10000  training loss: 0.2814232409000397 Validation loss 0.28218621015548706\n",
            "Epoch 2680/10000  training loss: 0.2813882827758789 Validation loss 0.2821488380432129\n",
            "Epoch 2690/10000  training loss: 0.2813534438610077 Validation loss 0.2821115553379059\n",
            "Epoch 2700/10000  training loss: 0.28131866455078125 Validation loss 0.2820743918418884\n",
            "Epoch 2710/10000  training loss: 0.28128400444984436 Validation loss 0.2820373475551605\n",
            "Epoch 2720/10000  training loss: 0.281249463558197 Validation loss 0.2820003628730774\n",
            "Epoch 2730/10000  training loss: 0.28121498227119446 Validation loss 0.2819634974002838\n",
            "Epoch 2740/10000  training loss: 0.28118064999580383 Validation loss 0.2819267213344574\n",
            "Epoch 2750/10000  training loss: 0.281146377325058 Validation loss 0.28189006447792053\n",
            "Epoch 2760/10000  training loss: 0.2811121940612793 Validation loss 0.28185349702835083\n",
            "Epoch 2770/10000  training loss: 0.281078040599823 Validation loss 0.2818170189857483\n",
            "Epoch 2780/10000  training loss: 0.28104403614997864 Validation loss 0.2817806601524353\n",
            "Epoch 2790/10000  training loss: 0.28101015090942383 Validation loss 0.2817443609237671\n",
            "Epoch 2800/10000  training loss: 0.2809763550758362 Validation loss 0.2817081809043884\n",
            "Epoch 2810/10000  training loss: 0.2809426486492157 Validation loss 0.2816721498966217\n",
            "Epoch 2820/10000  training loss: 0.28090909123420715 Validation loss 0.28163617849349976\n",
            "Epoch 2830/10000  training loss: 0.28087562322616577 Validation loss 0.2816004157066345\n",
            "Epoch 2840/10000  training loss: 0.28084224462509155 Validation loss 0.2815646529197693\n",
            "Epoch 2850/10000  training loss: 0.2808089554309845 Validation loss 0.281529039144516\n",
            "Epoch 2860/10000  training loss: 0.2807757556438446 Validation loss 0.28149351477622986\n",
            "Epoch 2870/10000  training loss: 0.28074267506599426 Validation loss 0.2814580798149109\n",
            "Epoch 2880/10000  training loss: 0.2807096540927887 Validation loss 0.28142276406288147\n",
            "Epoch 2890/10000  training loss: 0.2806767523288727 Validation loss 0.2813875079154968\n",
            "Epoch 2900/10000  training loss: 0.28064391016960144 Validation loss 0.28135234117507935\n",
            "Epoch 2910/10000  training loss: 0.28061121702194214 Validation loss 0.2813173234462738\n",
            "Epoch 2920/10000  training loss: 0.2805785536766052 Validation loss 0.28128236532211304\n",
            "Epoch 2930/10000  training loss: 0.28054600954055786 Validation loss 0.2812475264072418\n",
            "Epoch 2940/10000  training loss: 0.28051358461380005 Validation loss 0.2812127470970154\n",
            "Epoch 2950/10000  training loss: 0.280481219291687 Validation loss 0.2811781167984009\n",
            "Epoch 2960/10000  training loss: 0.28044894337654114 Validation loss 0.28114351630210876\n",
            "Epoch 2970/10000  training loss: 0.2804167568683624 Validation loss 0.2811090350151062\n",
            "Epoch 2980/10000  training loss: 0.2803846597671509 Validation loss 0.2810746729373932\n",
            "Epoch 2990/10000  training loss: 0.2803526222705841 Validation loss 0.28104037046432495\n",
            "Epoch 3000/10000  training loss: 0.2803207039833069 Validation loss 0.2810061573982239\n",
            "Epoch 3010/10000  training loss: 0.2802888751029968 Validation loss 0.28097206354141235\n",
            "Epoch 3020/10000  training loss: 0.2802571654319763 Validation loss 0.280938059091568\n",
            "Epoch 3030/10000  training loss: 0.2802255153656006 Validation loss 0.2809041440486908\n",
            "Epoch 3040/10000  training loss: 0.28019392490386963 Validation loss 0.28087031841278076\n",
            "Epoch 3050/10000  training loss: 0.2801624834537506 Validation loss 0.28083664178848267\n",
            "Epoch 3060/10000  training loss: 0.28013110160827637 Validation loss 0.28080296516418457\n",
            "Epoch 3070/10000  training loss: 0.2800998091697693 Validation loss 0.280769407749176\n",
            "Epoch 3080/10000  training loss: 0.28006863594055176 Validation loss 0.2807359993457794\n",
            "Epoch 3090/10000  training loss: 0.280037522315979 Validation loss 0.28070268034935\n",
            "Epoch 3100/10000  training loss: 0.2800065279006958 Validation loss 0.2806694507598877\n",
            "Epoch 3110/10000  training loss: 0.27997562289237976 Validation loss 0.2806363105773926\n",
            "Epoch 3120/10000  training loss: 0.2799447774887085 Validation loss 0.2806032598018646\n",
            "Epoch 3130/10000  training loss: 0.2799140512943268 Validation loss 0.28057026863098145\n",
            "Epoch 3140/10000  training loss: 0.27988341450691223 Validation loss 0.2805374264717102\n",
            "Epoch 3150/10000  training loss: 0.27985283732414246 Validation loss 0.28050464391708374\n",
            "Epoch 3160/10000  training loss: 0.27982231974601746 Validation loss 0.2804719805717468\n",
            "Epoch 3170/10000  training loss: 0.27979201078414917 Validation loss 0.2804393768310547\n",
            "Epoch 3180/10000  training loss: 0.2797616720199585 Validation loss 0.2804068624973297\n",
            "Epoch 3190/10000  training loss: 0.2797314524650574 Validation loss 0.2803744673728943\n",
            "Epoch 3200/10000  training loss: 0.2797013521194458 Validation loss 0.28034213185310364\n",
            "Epoch 3210/10000  training loss: 0.2796712815761566 Validation loss 0.28030991554260254\n",
            "Epoch 3220/10000  training loss: 0.2796413004398346 Validation loss 0.28027772903442383\n",
            "Epoch 3230/10000  training loss: 0.2796114385128021 Validation loss 0.28024569153785706\n",
            "Epoch 3240/10000  training loss: 0.27958163619041443 Validation loss 0.28021374344825745\n",
            "Epoch 3250/10000  training loss: 0.2795519232749939 Validation loss 0.2801818251609802\n",
            "Epoch 3260/10000  training loss: 0.2795222997665405 Validation loss 0.28015002608299255\n",
            "Epoch 3270/10000  training loss: 0.2794928252696991 Validation loss 0.28011834621429443\n",
            "Epoch 3280/10000  training loss: 0.2794633209705353 Validation loss 0.2800866961479187\n",
            "Epoch 3290/10000  training loss: 0.2794339656829834 Validation loss 0.2800552248954773\n",
            "Epoch 3300/10000  training loss: 0.2794046401977539 Validation loss 0.2800237536430359\n",
            "Epoch 3310/10000  training loss: 0.27937543392181396 Validation loss 0.27999240159988403\n",
            "Epoch 3320/10000  training loss: 0.2793463468551636 Validation loss 0.27996116876602173\n",
            "Epoch 3330/10000  training loss: 0.27931728959083557 Validation loss 0.2799299657344818\n",
            "Epoch 3340/10000  training loss: 0.2792883515357971 Validation loss 0.27989888191223145\n",
            "Epoch 3350/10000  training loss: 0.27925947308540344 Validation loss 0.27986788749694824\n",
            "Epoch 3360/10000  training loss: 0.2792307138442993 Validation loss 0.2798369824886322\n",
            "Epoch 3370/10000  training loss: 0.27920204401016235 Validation loss 0.2798062264919281\n",
            "Epoch 3380/10000  training loss: 0.27917343378067017 Validation loss 0.2797755002975464\n",
            "Epoch 3390/10000  training loss: 0.27914494276046753 Validation loss 0.27974486351013184\n",
            "Epoch 3400/10000  training loss: 0.27911651134490967 Validation loss 0.27971434593200684\n",
            "Epoch 3410/10000  training loss: 0.27908819913864136 Validation loss 0.2796838879585266\n",
            "Epoch 3420/10000  training loss: 0.27905988693237305 Validation loss 0.27965354919433594\n",
            "Epoch 3430/10000  training loss: 0.2790317237377167 Validation loss 0.27962321043014526\n",
            "Epoch 3440/10000  training loss: 0.2790036201477051 Validation loss 0.2795930504798889\n",
            "Epoch 3450/10000  training loss: 0.27897554636001587 Validation loss 0.27956292033195496\n",
            "Epoch 3460/10000  training loss: 0.278947651386261 Validation loss 0.27953284978866577\n",
            "Epoch 3470/10000  training loss: 0.2789197266101837 Validation loss 0.2795029282569885\n",
            "Epoch 3480/10000  training loss: 0.2788919508457184 Validation loss 0.27947306632995605\n",
            "Epoch 3490/10000  training loss: 0.2788642346858978 Validation loss 0.27944329380989075\n",
            "Epoch 3500/10000  training loss: 0.27883660793304443 Validation loss 0.2794136106967926\n",
            "Epoch 3510/10000  training loss: 0.2788091003894806 Validation loss 0.27938398718833923\n",
            "Epoch 3520/10000  training loss: 0.27878162264823914 Validation loss 0.27935442328453064\n",
            "Epoch 3530/10000  training loss: 0.27875420451164246 Validation loss 0.279325008392334\n",
            "Epoch 3540/10000  training loss: 0.2787269055843353 Validation loss 0.2792956233024597\n",
            "Epoch 3550/10000  training loss: 0.278699666261673 Validation loss 0.2792663872241974\n",
            "Epoch 3560/10000  training loss: 0.27867254614830017 Validation loss 0.27923721075057983\n",
            "Epoch 3570/10000  training loss: 0.2786455452442169 Validation loss 0.27920812368392944\n",
            "Epoch 3580/10000  training loss: 0.2786185145378113 Validation loss 0.2791791260242462\n",
            "Epoch 3590/10000  training loss: 0.2785916030406952 Validation loss 0.2791501581668854\n",
            "Epoch 3600/10000  training loss: 0.27856484055519104 Validation loss 0.2791213393211365\n",
            "Epoch 3610/10000  training loss: 0.2785380780696869 Validation loss 0.27909258008003235\n",
            "Epoch 3620/10000  training loss: 0.2785114347934723 Validation loss 0.279063880443573\n",
            "Epoch 3630/10000  training loss: 0.2784848213195801 Validation loss 0.2790353298187256\n",
            "Epoch 3640/10000  training loss: 0.2784583568572998 Validation loss 0.27900680899620056\n",
            "Epoch 3650/10000  training loss: 0.2784320116043091 Validation loss 0.2789784371852875\n",
            "Epoch 3660/10000  training loss: 0.27840563654899597 Validation loss 0.2789500951766968\n",
            "Epoch 3670/10000  training loss: 0.2783794105052948 Validation loss 0.27892184257507324\n",
            "Epoch 3680/10000  training loss: 0.278353214263916 Validation loss 0.2788936495780945\n",
            "Epoch 3690/10000  training loss: 0.2783271074295044 Validation loss 0.2788655757904053\n",
            "Epoch 3700/10000  training loss: 0.27830109000205994 Validation loss 0.2788375914096832\n",
            "Epoch 3710/10000  training loss: 0.27827513217926025 Validation loss 0.27880963683128357\n",
            "Epoch 3720/10000  training loss: 0.27824923396110535 Validation loss 0.27878180146217346\n",
            "Epoch 3730/10000  training loss: 0.2782234251499176 Validation loss 0.27875399589538574\n",
            "Epoch 3740/10000  training loss: 0.2781977355480194 Validation loss 0.27872633934020996\n",
            "Epoch 3750/10000  training loss: 0.2781720757484436 Validation loss 0.27869874238967896\n",
            "Epoch 3760/10000  training loss: 0.27814650535583496 Validation loss 0.27867117524147034\n",
            "Epoch 3770/10000  training loss: 0.2781210243701935 Validation loss 0.27864375710487366\n",
            "Epoch 3780/10000  training loss: 0.2780956029891968 Validation loss 0.278616338968277\n",
            "Epoch 3790/10000  training loss: 0.27807021141052246 Validation loss 0.27858906984329224\n",
            "Epoch 3800/10000  training loss: 0.27804499864578247 Validation loss 0.2785618305206299\n",
            "Epoch 3810/10000  training loss: 0.2780197560787201 Validation loss 0.2785347104072571\n",
            "Epoch 3820/10000  training loss: 0.2779946029186249 Validation loss 0.27850762009620667\n",
            "Epoch 3830/10000  training loss: 0.2779695391654968 Validation loss 0.278480589389801\n",
            "Epoch 3840/10000  training loss: 0.27794456481933594 Validation loss 0.27845367789268494\n",
            "Epoch 3850/10000  training loss: 0.2779196798801422 Validation loss 0.2784268260002136\n",
            "Epoch 3860/10000  training loss: 0.2778947949409485 Validation loss 0.27840009331703186\n",
            "Epoch 3870/10000  training loss: 0.2778700590133667 Validation loss 0.2783734202384949\n",
            "Epoch 3880/10000  training loss: 0.2778453826904297 Validation loss 0.27834680676460266\n",
            "Epoch 3890/10000  training loss: 0.27782076597213745 Validation loss 0.2783202528953552\n",
            "Epoch 3900/10000  training loss: 0.2777961790561676 Validation loss 0.27829378843307495\n",
            "Epoch 3910/10000  training loss: 0.2777717411518097 Validation loss 0.27826744318008423\n",
            "Epoch 3920/10000  training loss: 0.27774736285209656 Validation loss 0.2782411575317383\n",
            "Epoch 3930/10000  training loss: 0.2777230441570282 Validation loss 0.2782149314880371\n",
            "Epoch 3940/10000  training loss: 0.2776987850666046 Validation loss 0.2781888246536255\n",
            "Epoch 3950/10000  training loss: 0.2776746451854706 Validation loss 0.27816277742385864\n",
            "Epoch 3960/10000  training loss: 0.27765053510665894 Validation loss 0.2781367897987366\n",
            "Epoch 3970/10000  training loss: 0.27762648463249207 Validation loss 0.2781108617782593\n",
            "Epoch 3980/10000  training loss: 0.27760255336761475 Validation loss 0.27808505296707153\n",
            "Epoch 3990/10000  training loss: 0.2775786817073822 Validation loss 0.27805930376052856\n",
            "Epoch 4000/10000  training loss: 0.2775548994541168 Validation loss 0.27803364396095276\n",
            "Epoch 4010/10000  training loss: 0.27753111720085144 Validation loss 0.27800798416137695\n",
            "Epoch 4020/10000  training loss: 0.2775074243545532 Validation loss 0.2779824733734131\n",
            "Epoch 4030/10000  training loss: 0.27748382091522217 Validation loss 0.277957022190094\n",
            "Epoch 4040/10000  training loss: 0.2774602770805359 Validation loss 0.2779316306114197\n",
            "Epoch 4050/10000  training loss: 0.2774368226528168 Validation loss 0.2779063284397125\n",
            "Epoch 4060/10000  training loss: 0.27741342782974243 Validation loss 0.27788108587265015\n",
            "Epoch 4070/10000  training loss: 0.27739012241363525 Validation loss 0.27785587310791016\n",
            "Epoch 4080/10000  training loss: 0.2773668169975281 Validation loss 0.2778307795524597\n",
            "Epoch 4090/10000  training loss: 0.27734366059303284 Validation loss 0.27780574560165405\n",
            "Epoch 4100/10000  training loss: 0.27732053399086 Validation loss 0.27778083086013794\n",
            "Epoch 4110/10000  training loss: 0.2772974967956543 Validation loss 0.2777559161186218\n",
            "Epoch 4120/10000  training loss: 0.2772745192050934 Validation loss 0.2777310907840729\n",
            "Epoch 4130/10000  training loss: 0.27725157141685486 Validation loss 0.2777063846588135\n",
            "Epoch 4140/10000  training loss: 0.2772287130355835 Validation loss 0.27768173813819885\n",
            "Epoch 4150/10000  training loss: 0.2772059440612793 Validation loss 0.2776571214199066\n",
            "Epoch 4160/10000  training loss: 0.2771832346916199 Validation loss 0.27763259410858154\n",
            "Epoch 4170/10000  training loss: 0.2771605849266052 Validation loss 0.27760815620422363\n",
            "Epoch 4180/10000  training loss: 0.27713799476623535 Validation loss 0.2775837779045105\n",
            "Epoch 4190/10000  training loss: 0.27711549401283264 Validation loss 0.27755945920944214\n",
            "Epoch 4200/10000  training loss: 0.2770930528640747 Validation loss 0.27753520011901855\n",
            "Epoch 4210/10000  training loss: 0.27707064151763916 Validation loss 0.27751103043556213\n",
            "Epoch 4220/10000  training loss: 0.27704834938049316 Validation loss 0.2774869203567505\n",
            "Epoch 4230/10000  training loss: 0.27702611684799194 Validation loss 0.277462899684906\n",
            "Epoch 4240/10000  training loss: 0.2770039737224579 Validation loss 0.2774389684200287\n",
            "Epoch 4250/10000  training loss: 0.2769818902015686 Validation loss 0.27741512656211853\n",
            "Epoch 4260/10000  training loss: 0.2769598662853241 Validation loss 0.27739131450653076\n",
            "Epoch 4270/10000  training loss: 0.276937872171402 Validation loss 0.27736759185791016\n",
            "Epoch 4280/10000  training loss: 0.2769159972667694 Validation loss 0.2773439288139343\n",
            "Epoch 4290/10000  training loss: 0.27689415216445923 Validation loss 0.27732032537460327\n",
            "Epoch 4300/10000  training loss: 0.2768723666667938 Validation loss 0.27729684114456177\n",
            "Epoch 4310/10000  training loss: 0.27685070037841797 Validation loss 0.27727335691452026\n",
            "Epoch 4320/10000  training loss: 0.2768290340900421 Validation loss 0.2772499620914459\n",
            "Epoch 4330/10000  training loss: 0.27680742740631104 Validation loss 0.27722662687301636\n",
            "Epoch 4340/10000  training loss: 0.2767859399318695 Validation loss 0.27720338106155396\n",
            "Epoch 4350/10000  training loss: 0.27676451206207275 Validation loss 0.2771802544593811\n",
            "Epoch 4360/10000  training loss: 0.2767431139945984 Validation loss 0.27715712785720825\n",
            "Epoch 4370/10000  training loss: 0.2767218351364136 Validation loss 0.27713409066200256\n",
            "Epoch 4380/10000  training loss: 0.27670061588287354 Validation loss 0.27711111307144165\n",
            "Epoch 4390/10000  training loss: 0.27667945623397827 Validation loss 0.2770882248878479\n",
            "Epoch 4400/10000  training loss: 0.2766583263874054 Validation loss 0.2770654261112213\n",
            "Epoch 4410/10000  training loss: 0.2766372859477997 Validation loss 0.2770426571369171\n",
            "Epoch 4420/10000  training loss: 0.27661633491516113 Validation loss 0.2770199179649353\n",
            "Epoch 4430/10000  training loss: 0.2765953838825226 Validation loss 0.27699732780456543\n",
            "Epoch 4440/10000  training loss: 0.2765745222568512 Validation loss 0.27697473764419556\n",
            "Epoch 4450/10000  training loss: 0.2765537202358246 Validation loss 0.27695223689079285\n",
            "Epoch 4460/10000  training loss: 0.27653300762176514 Validation loss 0.2769297957420349\n",
            "Epoch 4470/10000  training loss: 0.2765122950077057 Validation loss 0.27690744400024414\n",
            "Epoch 4480/10000  training loss: 0.2764917016029358 Validation loss 0.27688512206077576\n",
            "Epoch 4490/10000  training loss: 0.2764711380004883 Validation loss 0.2768629193305969\n",
            "Epoch 4500/10000  training loss: 0.27645066380500793 Validation loss 0.2768407464027405\n",
            "Epoch 4510/10000  training loss: 0.27643024921417236 Validation loss 0.2768186032772064\n",
            "Epoch 4520/10000  training loss: 0.27640992403030396 Validation loss 0.2767965793609619\n",
            "Epoch 4530/10000  training loss: 0.27638959884643555 Validation loss 0.2767745852470398\n",
            "Epoch 4540/10000  training loss: 0.2763693630695343 Validation loss 0.2767527103424072\n",
            "Epoch 4550/10000  training loss: 0.27634918689727783 Validation loss 0.27673089504241943\n",
            "Epoch 4560/10000  training loss: 0.27632907032966614 Validation loss 0.2767091393470764\n",
            "Epoch 4570/10000  training loss: 0.276309072971344 Validation loss 0.2766874432563782\n",
            "Epoch 4580/10000  training loss: 0.27628907561302185 Validation loss 0.2766657769680023\n",
            "Epoch 4590/10000  training loss: 0.2762691378593445 Validation loss 0.27664417028427124\n",
            "Epoch 4600/10000  training loss: 0.2762492597103119 Validation loss 0.2766227126121521\n",
            "Epoch 4610/10000  training loss: 0.27622950077056885 Validation loss 0.27660125494003296\n",
            "Epoch 4620/10000  training loss: 0.2762097418308258 Validation loss 0.2765798568725586\n",
            "Epoch 4630/10000  training loss: 0.27619001269340515 Validation loss 0.276558518409729\n",
            "Epoch 4640/10000  training loss: 0.27617040276527405 Validation loss 0.2765372693538666\n",
            "Epoch 4650/10000  training loss: 0.2761508822441101 Validation loss 0.2765160799026489\n",
            "Epoch 4660/10000  training loss: 0.2761313319206238 Validation loss 0.27649492025375366\n",
            "Epoch 4670/10000  training loss: 0.2761118710041046 Validation loss 0.27647385001182556\n",
            "Epoch 4680/10000  training loss: 0.2760924696922302 Validation loss 0.27645283937454224\n",
            "Epoch 4690/10000  training loss: 0.276073157787323 Validation loss 0.2764318883419037\n",
            "Epoch 4700/10000  training loss: 0.27605387568473816 Validation loss 0.2764109969139099\n",
            "Epoch 4710/10000  training loss: 0.2760346531867981 Validation loss 0.2763901650905609\n",
            "Epoch 4720/10000  training loss: 0.2760155200958252 Validation loss 0.2763693928718567\n",
            "Epoch 4730/10000  training loss: 0.2759964168071747 Validation loss 0.27634868025779724\n",
            "Epoch 4740/10000  training loss: 0.27597734332084656 Validation loss 0.27632802724838257\n",
            "Epoch 4750/10000  training loss: 0.2759583592414856 Validation loss 0.27630743384361267\n",
            "Epoch 4760/10000  training loss: 0.2759394347667694 Validation loss 0.27628692984580994\n",
            "Epoch 4770/10000  training loss: 0.2759205400943756 Validation loss 0.276266485452652\n",
            "Epoch 4780/10000  training loss: 0.27590176463127136 Validation loss 0.2762460708618164\n",
            "Epoch 4790/10000  training loss: 0.2758829891681671 Validation loss 0.2762257158756256\n",
            "Epoch 4800/10000  training loss: 0.27586424350738525 Validation loss 0.276205450296402\n",
            "Epoch 4810/10000  training loss: 0.27584564685821533 Validation loss 0.27618521451950073\n",
            "Epoch 4820/10000  training loss: 0.2758270502090454 Validation loss 0.27616506814956665\n",
            "Epoch 4830/10000  training loss: 0.27580851316452026 Validation loss 0.27614492177963257\n",
            "Epoch 4840/10000  training loss: 0.2757900357246399 Validation loss 0.27612486481666565\n",
            "Epoch 4850/10000  training loss: 0.2757715880870819 Validation loss 0.2761048674583435\n",
            "Epoch 4860/10000  training loss: 0.2757532596588135 Validation loss 0.2760849595069885\n",
            "Epoch 4870/10000  training loss: 0.27573496103286743 Validation loss 0.2760651111602783\n",
            "Epoch 4880/10000  training loss: 0.27571672201156616 Validation loss 0.2760453224182129\n",
            "Epoch 4890/10000  training loss: 0.2756985127925873 Validation loss 0.27602559328079224\n",
            "Epoch 4900/10000  training loss: 0.2756803631782532 Validation loss 0.27600589394569397\n",
            "Epoch 4910/10000  training loss: 0.2756623327732086 Validation loss 0.27598631381988525\n",
            "Epoch 4920/10000  training loss: 0.27564430236816406 Validation loss 0.27596673369407654\n",
            "Epoch 4930/10000  training loss: 0.2756263315677643 Validation loss 0.2759472727775574\n",
            "Epoch 4940/10000  training loss: 0.27560845017433167 Validation loss 0.2759278118610382\n",
            "Epoch 4950/10000  training loss: 0.27559056878089905 Validation loss 0.2759084105491638\n",
            "Epoch 4960/10000  training loss: 0.2755727469921112 Validation loss 0.2758890688419342\n",
            "Epoch 4970/10000  training loss: 0.2755550146102905 Validation loss 0.27586978673934937\n",
            "Epoch 4980/10000  training loss: 0.2755373418331146 Validation loss 0.2758505940437317\n",
            "Epoch 4990/10000  training loss: 0.2755196988582611 Validation loss 0.275831401348114\n",
            "Epoch 5000/10000  training loss: 0.27550208568573 Validation loss 0.2758122980594635\n",
            "Epoch 5010/10000  training loss: 0.27548453211784363 Validation loss 0.27579325437545776\n",
            "Epoch 5020/10000  training loss: 0.27546703815460205 Validation loss 0.2757742404937744\n",
            "Epoch 5030/10000  training loss: 0.27544963359832764 Validation loss 0.2757553160190582\n",
            "Epoch 5040/10000  training loss: 0.2754322588443756 Validation loss 0.2757364511489868\n",
            "Epoch 5050/10000  training loss: 0.27541494369506836 Validation loss 0.2757176458835602\n",
            "Epoch 5060/10000  training loss: 0.2753976583480835 Validation loss 0.27569884061813354\n",
            "Epoch 5070/10000  training loss: 0.2753804624080658 Validation loss 0.2756801247596741\n",
            "Epoch 5080/10000  training loss: 0.2753632664680481 Validation loss 0.27566149830818176\n",
            "Epoch 5090/10000  training loss: 0.27534615993499756 Validation loss 0.27564287185668945\n",
            "Epoch 5100/10000  training loss: 0.2753291130065918 Validation loss 0.2756243348121643\n",
            "Epoch 5110/10000  training loss: 0.2753120958805084 Validation loss 0.27560585737228394\n",
            "Epoch 5120/10000  training loss: 0.27529510855674744 Validation loss 0.27558737993240356\n",
            "Epoch 5130/10000  training loss: 0.275278240442276 Validation loss 0.27556902170181274\n",
            "Epoch 5140/10000  training loss: 0.27526137232780457 Validation loss 0.2755506932735443\n",
            "Epoch 5150/10000  training loss: 0.2752445340156555 Validation loss 0.27553242444992065\n",
            "Epoch 5160/10000  training loss: 0.27522778511047363 Validation loss 0.2755141854286194\n",
            "Epoch 5170/10000  training loss: 0.2752111256122589 Validation loss 0.27549606561660767\n",
            "Epoch 5180/10000  training loss: 0.2751944363117218 Validation loss 0.27547791600227356\n",
            "Epoch 5190/10000  training loss: 0.27517786622047424 Validation loss 0.2754598557949066\n",
            "Epoch 5200/10000  training loss: 0.2751612961292267 Validation loss 0.27544185519218445\n",
            "Epoch 5210/10000  training loss: 0.2751448154449463 Validation loss 0.27542394399642944\n",
            "Epoch 5220/10000  training loss: 0.27512839436531067 Validation loss 0.2754060626029968\n",
            "Epoch 5230/10000  training loss: 0.2751120328903198 Validation loss 0.27538827061653137\n",
            "Epoch 5240/10000  training loss: 0.27509570121765137 Validation loss 0.2753705084323883\n",
            "Epoch 5250/10000  training loss: 0.2750794291496277 Validation loss 0.2753527760505676\n",
            "Epoch 5260/10000  training loss: 0.2750631868839264 Validation loss 0.2753351330757141\n",
            "Epoch 5270/10000  training loss: 0.27504706382751465 Validation loss 0.27531754970550537\n",
            "Epoch 5280/10000  training loss: 0.2750309109687805 Validation loss 0.27529996633529663\n",
            "Epoch 5290/10000  training loss: 0.27501487731933594 Validation loss 0.27528250217437744\n",
            "Epoch 5300/10000  training loss: 0.27499881386756897 Validation loss 0.27526506781578064\n",
            "Epoch 5310/10000  training loss: 0.27498286962509155 Validation loss 0.2752476930618286\n",
            "Epoch 5320/10000  training loss: 0.27496692538261414 Validation loss 0.275230348110199\n",
            "Epoch 5330/10000  training loss: 0.2749510705471039 Validation loss 0.2752130627632141\n",
            "Epoch 5340/10000  training loss: 0.27493521571159363 Validation loss 0.275195837020874\n",
            "Epoch 5350/10000  training loss: 0.2749194800853729 Validation loss 0.27517861127853394\n",
            "Epoch 5360/10000  training loss: 0.27490371465682983 Validation loss 0.2751615345478058\n",
            "Epoch 5370/10000  training loss: 0.2748880684375763 Validation loss 0.27514442801475525\n",
            "Epoch 5380/10000  training loss: 0.27487242221832275 Validation loss 0.2751274108886719\n",
            "Epoch 5390/10000  training loss: 0.274856835603714 Validation loss 0.2751104235649109\n",
            "Epoch 5400/10000  training loss: 0.2748412489891052 Validation loss 0.2750934660434723\n",
            "Epoch 5410/10000  training loss: 0.2748258113861084 Validation loss 0.27507656812667847\n",
            "Epoch 5420/10000  training loss: 0.2748103439807892 Validation loss 0.2750597596168518\n",
            "Epoch 5430/10000  training loss: 0.27479493618011475 Validation loss 0.27504298090934753\n",
            "Epoch 5440/10000  training loss: 0.2747795879840851 Validation loss 0.27502626180648804\n",
            "Epoch 5450/10000  training loss: 0.2747642993927002 Validation loss 0.2750095725059509\n",
            "Epoch 5460/10000  training loss: 0.2747490406036377 Validation loss 0.2749929428100586\n",
            "Epoch 5470/10000  training loss: 0.27473384141921997 Validation loss 0.27497637271881104\n",
            "Epoch 5480/10000  training loss: 0.27471861243247986 Validation loss 0.27495983242988586\n",
            "Epoch 5490/10000  training loss: 0.2747035324573517 Validation loss 0.27494335174560547\n",
            "Epoch 5500/10000  training loss: 0.2746884524822235 Validation loss 0.27492693066596985\n",
            "Epoch 5510/10000  training loss: 0.2746734619140625 Validation loss 0.2749105393886566\n",
            "Epoch 5520/10000  training loss: 0.2746584415435791 Validation loss 0.27489420771598816\n",
            "Epoch 5530/10000  training loss: 0.27464351058006287 Validation loss 0.2748779356479645\n",
            "Epoch 5540/10000  training loss: 0.2746286690235138 Validation loss 0.2748616933822632\n",
            "Epoch 5550/10000  training loss: 0.27461379766464233 Validation loss 0.27484551072120667\n",
            "Epoch 5560/10000  training loss: 0.27459901571273804 Validation loss 0.27482932806015015\n",
            "Epoch 5570/10000  training loss: 0.27458423376083374 Validation loss 0.2748132646083832\n",
            "Epoch 5580/10000  training loss: 0.274569571018219 Validation loss 0.2747972309589386\n",
            "Epoch 5590/10000  training loss: 0.27455487847328186 Validation loss 0.2747812271118164\n",
            "Epoch 5600/10000  training loss: 0.2745403051376343 Validation loss 0.2747653126716614\n",
            "Epoch 5610/10000  training loss: 0.2745257616043091 Validation loss 0.2747494578361511\n",
            "Epoch 5620/10000  training loss: 0.2745112478733063 Validation loss 0.27473360300064087\n",
            "Epoch 5630/10000  training loss: 0.27449676394462585 Validation loss 0.2747178077697754\n",
            "Epoch 5640/10000  training loss: 0.2744823396205902 Validation loss 0.2747020721435547\n",
            "Epoch 5650/10000  training loss: 0.27446797490119934 Validation loss 0.27468636631965637\n",
            "Epoch 5660/10000  training loss: 0.27445363998413086 Validation loss 0.27467072010040283\n",
            "Epoch 5670/10000  training loss: 0.27443933486938477 Validation loss 0.2746551036834717\n",
            "Epoch 5680/10000  training loss: 0.27442508935928345 Validation loss 0.2746395468711853\n",
            "Epoch 5690/10000  training loss: 0.2744108736515045 Validation loss 0.2746240198612213\n",
            "Epoch 5700/10000  training loss: 0.2743966579437256 Validation loss 0.2746085524559021\n",
            "Epoch 5710/10000  training loss: 0.2743825912475586 Validation loss 0.27459314465522766\n",
            "Epoch 5720/10000  training loss: 0.2743684649467468 Validation loss 0.2745777666568756\n",
            "Epoch 5730/10000  training loss: 0.2743544578552246 Validation loss 0.27456241846084595\n",
            "Epoch 5740/10000  training loss: 0.2743404507637024 Validation loss 0.27454715967178345\n",
            "Epoch 5750/10000  training loss: 0.27432653307914734 Validation loss 0.27453190088272095\n",
            "Epoch 5760/10000  training loss: 0.2743126451969147 Validation loss 0.2745167315006256\n",
            "Epoch 5770/10000  training loss: 0.274298757314682 Validation loss 0.27450159192085266\n",
            "Epoch 5780/10000  training loss: 0.2742849290370941 Validation loss 0.2744864821434021\n",
            "Epoch 5790/10000  training loss: 0.2742711901664734 Validation loss 0.2744714319705963\n",
            "Epoch 5800/10000  training loss: 0.2742574214935303 Validation loss 0.2744564414024353\n",
            "Epoch 5810/10000  training loss: 0.2742437720298767 Validation loss 0.2744414508342743\n",
            "Epoch 5820/10000  training loss: 0.27423006296157837 Validation loss 0.27442654967308044\n",
            "Epoch 5830/10000  training loss: 0.2742164731025696 Validation loss 0.2744116187095642\n",
            "Epoch 5840/10000  training loss: 0.2742028832435608 Validation loss 0.2743968367576599\n",
            "Epoch 5850/10000  training loss: 0.27418938279151917 Validation loss 0.2743820250034332\n",
            "Epoch 5860/10000  training loss: 0.27417588233947754 Validation loss 0.2743672728538513\n",
            "Epoch 5870/10000  training loss: 0.2741624414920807 Validation loss 0.2743525803089142\n",
            "Epoch 5880/10000  training loss: 0.2741490304470062 Validation loss 0.27433791756629944\n",
            "Epoch 5890/10000  training loss: 0.27413564920425415 Validation loss 0.2743232846260071\n",
            "Epoch 5900/10000  training loss: 0.27412235736846924 Validation loss 0.2743087410926819\n",
            "Epoch 5910/10000  training loss: 0.2741090655326843 Validation loss 0.2742941975593567\n",
            "Epoch 5920/10000  training loss: 0.2740958333015442 Validation loss 0.27427974343299866\n",
            "Epoch 5930/10000  training loss: 0.27408266067504883 Validation loss 0.2742653489112854\n",
            "Epoch 5940/10000  training loss: 0.2740694582462311 Validation loss 0.27425092458724976\n",
            "Epoch 5950/10000  training loss: 0.27405640482902527 Validation loss 0.2742365598678589\n",
            "Epoch 5960/10000  training loss: 0.27404332160949707 Validation loss 0.2742222547531128\n",
            "Epoch 5970/10000  training loss: 0.27403029799461365 Validation loss 0.2742080092430115\n",
            "Epoch 5980/10000  training loss: 0.274017333984375 Validation loss 0.27419382333755493\n",
            "Epoch 5990/10000  training loss: 0.27400436997413635 Validation loss 0.2741796374320984\n",
            "Epoch 6000/10000  training loss: 0.2739914655685425 Validation loss 0.274165540933609\n",
            "Epoch 6010/10000  training loss: 0.2739786207675934 Validation loss 0.27415141463279724\n",
            "Epoch 6020/10000  training loss: 0.2739657759666443 Validation loss 0.27413737773895264\n",
            "Epoch 6030/10000  training loss: 0.27395299077033997 Validation loss 0.2741233706474304\n",
            "Epoch 6040/10000  training loss: 0.27394020557403564 Validation loss 0.274109423160553\n",
            "Epoch 6050/10000  training loss: 0.2739275097846985 Validation loss 0.27409547567367554\n",
            "Epoch 6060/10000  training loss: 0.2739148437976837 Validation loss 0.27408164739608765\n",
            "Epoch 6070/10000  training loss: 0.2739022374153137 Validation loss 0.27406781911849976\n",
            "Epoch 6080/10000  training loss: 0.27388960123062134 Validation loss 0.27405402064323425\n",
            "Epoch 6090/10000  training loss: 0.2738771140575409 Validation loss 0.2740402817726135\n",
            "Epoch 6100/10000  training loss: 0.2738645672798157 Validation loss 0.2740265727043152\n",
            "Epoch 6110/10000  training loss: 0.27385208010673523 Validation loss 0.27401289343833923\n",
            "Epoch 6120/10000  training loss: 0.27383965253829956 Validation loss 0.27399927377700806\n",
            "Epoch 6130/10000  training loss: 0.2738272547721863 Validation loss 0.27398571372032166\n",
            "Epoch 6140/10000  training loss: 0.2738148868083954 Validation loss 0.27397218346595764\n",
            "Epoch 6150/10000  training loss: 0.2738025486469269 Validation loss 0.27395862340927124\n",
            "Epoch 6160/10000  training loss: 0.27379024028778076 Validation loss 0.2739451825618744\n",
            "Epoch 6170/10000  training loss: 0.2737780511379242 Validation loss 0.27393174171447754\n",
            "Epoch 6180/10000  training loss: 0.27376583218574524 Validation loss 0.27391836047172546\n",
            "Epoch 6190/10000  training loss: 0.27375364303588867 Validation loss 0.2739050090312958\n",
            "Epoch 6200/10000  training loss: 0.2737415134906769 Validation loss 0.27389171719551086\n",
            "Epoch 6210/10000  training loss: 0.27372944355010986 Validation loss 0.2738784849643707\n",
            "Epoch 6220/10000  training loss: 0.27371740341186523 Validation loss 0.273865282535553\n",
            "Epoch 6230/10000  training loss: 0.273705393075943 Validation loss 0.2738521099090576\n",
            "Epoch 6240/10000  training loss: 0.2736934423446655 Validation loss 0.27383899688720703\n",
            "Epoch 6250/10000  training loss: 0.27368149161338806 Validation loss 0.27382588386535645\n",
            "Epoch 6260/10000  training loss: 0.27366960048675537 Validation loss 0.273812860250473\n",
            "Epoch 6270/10000  training loss: 0.27365773916244507 Validation loss 0.2737998366355896\n",
            "Epoch 6280/10000  training loss: 0.27364590764045715 Validation loss 0.27378687262535095\n",
            "Epoch 6290/10000  training loss: 0.273634135723114 Validation loss 0.2737739384174347\n",
            "Epoch 6300/10000  training loss: 0.27362239360809326 Validation loss 0.2737610638141632\n",
            "Epoch 6310/10000  training loss: 0.2736106812953949 Validation loss 0.2737482190132141\n",
            "Epoch 6320/10000  training loss: 0.2735989987850189 Validation loss 0.273735374212265\n",
            "Epoch 6330/10000  training loss: 0.27358734607696533 Validation loss 0.2737225890159607\n",
            "Epoch 6340/10000  training loss: 0.2735757529735565 Validation loss 0.27370986342430115\n",
            "Epoch 6350/10000  training loss: 0.2735641896724701 Validation loss 0.2736971378326416\n",
            "Epoch 6360/10000  training loss: 0.27355262637138367 Validation loss 0.2736845314502716\n",
            "Epoch 6370/10000  training loss: 0.2735411524772644 Validation loss 0.2736719250679016\n",
            "Epoch 6380/10000  training loss: 0.27352970838546753 Validation loss 0.2736593186855316\n",
            "Epoch 6390/10000  training loss: 0.27351826429367065 Validation loss 0.2736467719078064\n",
            "Epoch 6400/10000  training loss: 0.27350687980651855 Validation loss 0.27363428473472595\n",
            "Epoch 6410/10000  training loss: 0.27349552512168884 Validation loss 0.2736218273639679\n",
            "Epoch 6420/10000  training loss: 0.2734842300415039 Validation loss 0.2736093997955322\n",
            "Epoch 6430/10000  training loss: 0.27347293496131897 Validation loss 0.27359700202941895\n",
            "Epoch 6440/10000  training loss: 0.2734617292881012 Validation loss 0.27358466386795044\n",
            "Epoch 6450/10000  training loss: 0.27345049381256104 Validation loss 0.2735723555088043\n",
            "Epoch 6460/10000  training loss: 0.27343928813934326 Validation loss 0.2735600471496582\n",
            "Epoch 6470/10000  training loss: 0.27342817187309265 Validation loss 0.27354782819747925\n",
            "Epoch 6480/10000  training loss: 0.27341705560684204 Validation loss 0.2735356092453003\n",
            "Epoch 6490/10000  training loss: 0.2734059691429138 Validation loss 0.2735234498977661\n",
            "Epoch 6500/10000  training loss: 0.27339494228363037 Validation loss 0.2735113203525543\n",
            "Epoch 6510/10000  training loss: 0.2733839452266693 Validation loss 0.27349919080734253\n",
            "Epoch 6520/10000  training loss: 0.27337294816970825 Validation loss 0.2734871506690979\n",
            "Epoch 6530/10000  training loss: 0.27336204051971436 Validation loss 0.27347511053085327\n",
            "Epoch 6540/10000  training loss: 0.27335113286972046 Validation loss 0.2734631597995758\n",
            "Epoch 6550/10000  training loss: 0.27334025502204895 Validation loss 0.27345117926597595\n",
            "Epoch 6560/10000  training loss: 0.2733294367790222 Validation loss 0.27343931794166565\n",
            "Epoch 6570/10000  training loss: 0.2733186185359955 Validation loss 0.27342739701271057\n",
            "Epoch 6580/10000  training loss: 0.27330783009529114 Validation loss 0.27341559529304504\n",
            "Epoch 6590/10000  training loss: 0.27329713106155396 Validation loss 0.2734037935733795\n",
            "Epoch 6600/10000  training loss: 0.2732864022254944 Validation loss 0.2733920216560364\n",
            "Epoch 6610/10000  training loss: 0.273275762796402 Validation loss 0.2733802795410156\n",
            "Epoch 6620/10000  training loss: 0.2732650935649872 Validation loss 0.2733685374259949\n",
            "Epoch 6630/10000  training loss: 0.27325448393821716 Validation loss 0.2733568549156189\n",
            "Epoch 6640/10000  training loss: 0.2732439339160919 Validation loss 0.2733452618122101\n",
            "Epoch 6650/10000  training loss: 0.2732333838939667 Validation loss 0.2733336389064789\n",
            "Epoch 6660/10000  training loss: 0.2732228636741638 Validation loss 0.27332210540771484\n",
            "Epoch 6670/10000  training loss: 0.27321240305900574 Validation loss 0.2733105719089508\n",
            "Epoch 6680/10000  training loss: 0.27320197224617004 Validation loss 0.27329906821250916\n",
            "Epoch 6690/10000  training loss: 0.27319154143333435 Validation loss 0.2732875943183899\n",
            "Epoch 6700/10000  training loss: 0.27318111062049866 Validation loss 0.2732761800289154\n",
            "Epoch 6710/10000  training loss: 0.2731707990169525 Validation loss 0.2732647955417633\n",
            "Epoch 6720/10000  training loss: 0.27316051721572876 Validation loss 0.2732534408569336\n",
            "Epoch 6730/10000  training loss: 0.2731502056121826 Validation loss 0.27324211597442627\n",
            "Epoch 6740/10000  training loss: 0.27313998341560364 Validation loss 0.2732308506965637\n",
            "Epoch 6750/10000  training loss: 0.27312976121902466 Validation loss 0.27321958541870117\n",
            "Epoch 6760/10000  training loss: 0.27311959862709045 Validation loss 0.2732083201408386\n",
            "Epoch 6770/10000  training loss: 0.27310943603515625 Validation loss 0.2731971740722656\n",
            "Epoch 6780/10000  training loss: 0.27309930324554443 Validation loss 0.2731860280036926\n",
            "Epoch 6790/10000  training loss: 0.273089200258255 Validation loss 0.273174911737442\n",
            "Epoch 6800/10000  training loss: 0.27307912707328796 Validation loss 0.2731638252735138\n",
            "Epoch 6810/10000  training loss: 0.2730691432952881 Validation loss 0.27315276861190796\n",
            "Epoch 6820/10000  training loss: 0.2730591297149658 Validation loss 0.2731417715549469\n",
            "Epoch 6830/10000  training loss: 0.27304914593696594 Validation loss 0.27313077449798584\n",
            "Epoch 6840/10000  training loss: 0.27303922176361084 Validation loss 0.27311986684799194\n",
            "Epoch 6850/10000  training loss: 0.2730293273925781 Validation loss 0.27310892939567566\n",
            "Epoch 6860/10000  training loss: 0.2730194330215454 Validation loss 0.27309805154800415\n",
            "Epoch 6870/10000  training loss: 0.27300959825515747 Validation loss 0.27308720350265503\n",
            "Epoch 6880/10000  training loss: 0.2729997932910919 Validation loss 0.2730763852596283\n",
            "Epoch 6890/10000  training loss: 0.27298998832702637 Validation loss 0.27306562662124634\n",
            "Epoch 6900/10000  training loss: 0.2729802429676056 Validation loss 0.273054838180542\n",
            "Epoch 6910/10000  training loss: 0.2729705274105072 Validation loss 0.2730441391468048\n",
            "Epoch 6920/10000  training loss: 0.2729608416557312 Validation loss 0.2730334401130676\n",
            "Epoch 6930/10000  training loss: 0.2729511559009552 Validation loss 0.2730228006839752\n",
            "Epoch 6940/10000  training loss: 0.27294155955314636 Validation loss 0.2730121612548828\n",
            "Epoch 6950/10000  training loss: 0.2729319632053375 Validation loss 0.2730015814304352\n",
            "Epoch 6960/10000  training loss: 0.2729223668575287 Validation loss 0.27299103140830994\n",
            "Epoch 6970/10000  training loss: 0.27291280031204224 Validation loss 0.2729805111885071\n",
            "Epoch 6980/10000  training loss: 0.27290332317352295 Validation loss 0.2729700207710266\n",
            "Epoch 6990/10000  training loss: 0.2728938162326813 Validation loss 0.27295953035354614\n",
            "Epoch 7000/10000  training loss: 0.2728843688964844 Validation loss 0.27294906973838806\n",
            "Epoch 7010/10000  training loss: 0.27287495136260986 Validation loss 0.27293866872787476\n",
            "Epoch 7020/10000  training loss: 0.27286553382873535 Validation loss 0.2729283273220062\n",
            "Epoch 7030/10000  training loss: 0.2728561758995056 Validation loss 0.2729179859161377\n",
            "Epoch 7040/10000  training loss: 0.2728468179702759 Validation loss 0.27290767431259155\n",
            "Epoch 7050/10000  training loss: 0.27283748984336853 Validation loss 0.2728973925113678\n",
            "Epoch 7060/10000  training loss: 0.27282825112342834 Validation loss 0.27288714051246643\n",
            "Epoch 7070/10000  training loss: 0.27281898260116577 Validation loss 0.27287691831588745\n",
            "Epoch 7080/10000  training loss: 0.272809773683548 Validation loss 0.27286678552627563\n",
            "Epoch 7090/10000  training loss: 0.27280059456825256 Validation loss 0.27285662293434143\n",
            "Epoch 7100/10000  training loss: 0.27279138565063477 Validation loss 0.272846519947052\n",
            "Epoch 7110/10000  training loss: 0.27278226613998413 Validation loss 0.2728363871574402\n",
            "Epoch 7120/10000  training loss: 0.2727731466293335 Validation loss 0.27282631397247314\n",
            "Epoch 7130/10000  training loss: 0.27276405692100525 Validation loss 0.2728162705898285\n",
            "Epoch 7140/10000  training loss: 0.2727550268173218 Validation loss 0.2728062570095062\n",
            "Epoch 7150/10000  training loss: 0.2727459967136383 Validation loss 0.27279630303382874\n",
            "Epoch 7160/10000  training loss: 0.27273696660995483 Validation loss 0.27278637886047363\n",
            "Epoch 7170/10000  training loss: 0.27272799611091614 Validation loss 0.27277645468711853\n",
            "Epoch 7180/10000  training loss: 0.2727190852165222 Validation loss 0.2727665603160858\n",
            "Epoch 7190/10000  training loss: 0.2727101445198059 Validation loss 0.2727566957473755\n",
            "Epoch 7200/10000  training loss: 0.272701233625412 Validation loss 0.27274686098098755\n",
            "Epoch 7210/10000  training loss: 0.27269235253334045 Validation loss 0.2727370858192444\n",
            "Epoch 7220/10000  training loss: 0.27268359065055847 Validation loss 0.2727273106575012\n",
            "Epoch 7230/10000  training loss: 0.2726747393608093 Validation loss 0.27271753549575806\n",
            "Epoch 7240/10000  training loss: 0.27266597747802734 Validation loss 0.27270784974098206\n",
            "Epoch 7250/10000  training loss: 0.27265724539756775 Validation loss 0.27269816398620605\n",
            "Epoch 7260/10000  training loss: 0.27264851331710815 Validation loss 0.27268850803375244\n",
            "Epoch 7270/10000  training loss: 0.27263978123664856 Validation loss 0.2726789116859436\n",
            "Epoch 7280/10000  training loss: 0.27263113856315613 Validation loss 0.27266931533813477\n",
            "Epoch 7290/10000  training loss: 0.2726224958896637 Validation loss 0.2726597785949707\n",
            "Epoch 7300/10000  training loss: 0.2726138234138489 Validation loss 0.27265021204948425\n",
            "Epoch 7310/10000  training loss: 0.2726052403450012 Validation loss 0.27264073491096497\n",
            "Epoch 7320/10000  training loss: 0.27259671688079834 Validation loss 0.2726312577724457\n",
            "Epoch 7330/10000  training loss: 0.27258816361427307 Validation loss 0.2726218104362488\n",
            "Epoch 7340/10000  training loss: 0.2725796699523926 Validation loss 0.27261239290237427\n",
            "Epoch 7350/10000  training loss: 0.2725711762905121 Validation loss 0.27260303497314453\n",
            "Epoch 7360/10000  training loss: 0.272562712430954 Validation loss 0.2725936770439148\n",
            "Epoch 7370/10000  training loss: 0.27255430817604065 Validation loss 0.27258437871932983\n",
            "Epoch 7380/10000  training loss: 0.2725459337234497 Validation loss 0.2725750803947449\n",
            "Epoch 7390/10000  training loss: 0.27253755927085876 Validation loss 0.2725658416748047\n",
            "Epoch 7400/10000  training loss: 0.2725292146205902 Validation loss 0.2725566029548645\n",
            "Epoch 7410/10000  training loss: 0.27252089977264404 Validation loss 0.2725473642349243\n",
            "Epoch 7420/10000  training loss: 0.2725125849246979 Validation loss 0.2725381851196289\n",
            "Epoch 7430/10000  training loss: 0.2725043296813965 Validation loss 0.2725290358066559\n",
            "Epoch 7440/10000  training loss: 0.2724961042404175 Validation loss 0.27251991629600525\n",
            "Epoch 7450/10000  training loss: 0.27248790860176086 Validation loss 0.272510826587677\n",
            "Epoch 7460/10000  training loss: 0.27247971296310425 Validation loss 0.27250176668167114\n",
            "Epoch 7470/10000  training loss: 0.27247151732444763 Validation loss 0.2724927067756653\n",
            "Epoch 7480/10000  training loss: 0.2724634110927582 Validation loss 0.2724836766719818\n",
            "Epoch 7490/10000  training loss: 0.2724553048610687 Validation loss 0.2724747061729431\n",
            "Epoch 7500/10000  training loss: 0.27244722843170166 Validation loss 0.2724657654762268\n",
            "Epoch 7510/10000  training loss: 0.2724391520023346 Validation loss 0.2724568247795105\n",
            "Epoch 7520/10000  training loss: 0.27243107557296753 Validation loss 0.2724479138851166\n",
            "Epoch 7530/10000  training loss: 0.2724230885505676 Validation loss 0.27243903279304504\n",
            "Epoch 7540/10000  training loss: 0.2724151015281677 Validation loss 0.2724301815032959\n",
            "Epoch 7550/10000  training loss: 0.2724071443080902 Validation loss 0.27242133021354675\n",
            "Epoch 7560/10000  training loss: 0.2723991572856903 Validation loss 0.2724125385284424\n",
            "Epoch 7570/10000  training loss: 0.2723912298679352 Validation loss 0.2724037766456604\n",
            "Epoch 7580/10000  training loss: 0.27238333225250244 Validation loss 0.27239498496055603\n",
            "Epoch 7590/10000  training loss: 0.2723754942417145 Validation loss 0.27238625288009644\n",
            "Epoch 7600/10000  training loss: 0.2723676562309265 Validation loss 0.27237755060195923\n",
            "Epoch 7610/10000  training loss: 0.2723598778247833 Validation loss 0.2723688781261444\n",
            "Epoch 7620/10000  training loss: 0.27235203981399536 Validation loss 0.272360235452652\n",
            "Epoch 7630/10000  training loss: 0.2723442614078522 Validation loss 0.27235162258148193\n",
            "Epoch 7640/10000  training loss: 0.27233654260635376 Validation loss 0.2723430395126343\n",
            "Epoch 7650/10000  training loss: 0.27232879400253296 Validation loss 0.272334486246109\n",
            "Epoch 7660/10000  training loss: 0.27232110500335693 Validation loss 0.27232593297958374\n",
            "Epoch 7670/10000  training loss: 0.2723133862018585 Validation loss 0.27231740951538086\n",
            "Epoch 7680/10000  training loss: 0.27230575680732727 Validation loss 0.27230891585350037\n",
            "Epoch 7690/10000  training loss: 0.272298127412796 Validation loss 0.2723004221916199\n",
            "Epoch 7700/10000  training loss: 0.27229055762290955 Validation loss 0.27229198813438416\n",
            "Epoch 7710/10000  training loss: 0.2722829580307007 Validation loss 0.2722835838794708\n",
            "Epoch 7720/10000  training loss: 0.2722753584384918 Validation loss 0.2722751796245575\n",
            "Epoch 7730/10000  training loss: 0.2722678482532501 Validation loss 0.27226683497428894\n",
            "Epoch 7740/10000  training loss: 0.27226030826568604 Validation loss 0.272258460521698\n",
            "Epoch 7750/10000  training loss: 0.2722528576850891 Validation loss 0.27225014567375183\n",
            "Epoch 7760/10000  training loss: 0.2722453474998474 Validation loss 0.27224186062812805\n",
            "Epoch 7770/10000  training loss: 0.2722378969192505 Validation loss 0.2722335755825043\n",
            "Epoch 7780/10000  training loss: 0.27223050594329834 Validation loss 0.2722253203392029\n",
            "Epoch 7790/10000  training loss: 0.2722230851650238 Validation loss 0.27221712470054626\n",
            "Epoch 7800/10000  training loss: 0.27221569418907166 Validation loss 0.27220892906188965\n",
            "Epoch 7810/10000  training loss: 0.2722083628177643 Validation loss 0.2722007632255554\n",
            "Epoch 7820/10000  training loss: 0.2722010314464569 Validation loss 0.2721926271915436\n",
            "Epoch 7830/10000  training loss: 0.2721937298774719 Validation loss 0.2721845209598541\n",
            "Epoch 7840/10000  training loss: 0.27218642830848694 Validation loss 0.2721764147281647\n",
            "Epoch 7850/10000  training loss: 0.27217915654182434 Validation loss 0.2721683382987976\n",
            "Epoch 7860/10000  training loss: 0.27217191457748413 Validation loss 0.27216029167175293\n",
            "Epoch 7870/10000  training loss: 0.2721647024154663 Validation loss 0.27215227484703064\n",
            "Epoch 7880/10000  training loss: 0.2721575200557709 Validation loss 0.27214425802230835\n",
            "Epoch 7890/10000  training loss: 0.27215030789375305 Validation loss 0.27213630080223083\n",
            "Epoch 7900/10000  training loss: 0.2721432149410248 Validation loss 0.2721283435821533\n",
            "Epoch 7910/10000  training loss: 0.27213606238365173 Validation loss 0.2721204161643982\n",
            "Epoch 7920/10000  training loss: 0.2721289396286011 Validation loss 0.27211251854896545\n",
            "Epoch 7930/10000  training loss: 0.2721218466758728 Validation loss 0.2721046507358551\n",
            "Epoch 7940/10000  training loss: 0.2721147835254669 Validation loss 0.27209678292274475\n",
            "Epoch 7950/10000  training loss: 0.27210772037506104 Validation loss 0.2720889449119568\n",
            "Epoch 7960/10000  training loss: 0.2721007168292999 Validation loss 0.2720811367034912\n",
            "Epoch 7970/10000  training loss: 0.27209365367889404 Validation loss 0.27207332849502563\n",
            "Epoch 7980/10000  training loss: 0.2720867097377777 Validation loss 0.27206555008888245\n",
            "Epoch 7990/10000  training loss: 0.272079735994339 Validation loss 0.27205783128738403\n",
            "Epoch 8000/10000  training loss: 0.27207279205322266 Validation loss 0.27205008268356323\n",
            "Epoch 8010/10000  training loss: 0.2720658481121063 Validation loss 0.2720423936843872\n",
            "Epoch 8020/10000  training loss: 0.27205896377563477 Validation loss 0.2720347046852112\n",
            "Epoch 8030/10000  training loss: 0.2720521092414856 Validation loss 0.27202707529067993\n",
            "Epoch 8040/10000  training loss: 0.2720452845096588 Validation loss 0.2720194458961487\n",
            "Epoch 8050/10000  training loss: 0.27203842997550964 Validation loss 0.2720118463039398\n",
            "Epoch 8060/10000  training loss: 0.27203163504600525 Validation loss 0.27200427651405334\n",
            "Epoch 8070/10000  training loss: 0.27202481031417847 Validation loss 0.27199670672416687\n",
            "Epoch 8080/10000  training loss: 0.27201804518699646 Validation loss 0.2719891667366028\n",
            "Epoch 8090/10000  training loss: 0.27201128005981445 Validation loss 0.2719816267490387\n",
            "Epoch 8100/10000  training loss: 0.27200454473495483 Validation loss 0.2719741463661194\n",
            "Epoch 8110/10000  training loss: 0.2719978392124176 Validation loss 0.2719666659832001\n",
            "Epoch 8120/10000  training loss: 0.27199113368988037 Validation loss 0.27195921540260315\n",
            "Epoch 8130/10000  training loss: 0.2719844877719879 Validation loss 0.2719517946243286\n",
            "Epoch 8140/10000  training loss: 0.27197787165641785 Validation loss 0.2719443738460541\n",
            "Epoch 8150/10000  training loss: 0.271971195936203 Validation loss 0.27193698287010193\n",
            "Epoch 8160/10000  training loss: 0.27196457982063293 Validation loss 0.27192965149879456\n",
            "Epoch 8170/10000  training loss: 0.27195799350738525 Validation loss 0.2719222903251648\n",
            "Epoch 8180/10000  training loss: 0.2719514071941376 Validation loss 0.2719149589538574\n",
            "Epoch 8190/10000  training loss: 0.27194491028785706 Validation loss 0.27190765738487244\n",
            "Epoch 8200/10000  training loss: 0.27193838357925415 Validation loss 0.27190035581588745\n",
            "Epoch 8210/10000  training loss: 0.27193182706832886 Validation loss 0.27189311385154724\n",
            "Epoch 8220/10000  training loss: 0.27192533016204834 Validation loss 0.27188587188720703\n",
            "Epoch 8230/10000  training loss: 0.2719188630580902 Validation loss 0.2718786597251892\n",
            "Epoch 8240/10000  training loss: 0.27191242575645447 Validation loss 0.2718714773654938\n",
            "Epoch 8250/10000  training loss: 0.2719059884548187 Validation loss 0.2718643248081207\n",
            "Epoch 8260/10000  training loss: 0.27189958095550537 Validation loss 0.2718571722507477\n",
            "Epoch 8270/10000  training loss: 0.271893173456192 Validation loss 0.271850049495697\n",
            "Epoch 8280/10000  training loss: 0.27188679575920105 Validation loss 0.271842896938324\n",
            "Epoch 8290/10000  training loss: 0.27188044786453247 Validation loss 0.2718358635902405\n",
            "Epoch 8300/10000  training loss: 0.2718741297721863 Validation loss 0.2718288004398346\n",
            "Epoch 8310/10000  training loss: 0.2718677818775177 Validation loss 0.2718217372894287\n",
            "Epoch 8320/10000  training loss: 0.2718615233898163 Validation loss 0.2718147337436676\n",
            "Epoch 8330/10000  training loss: 0.27185526490211487 Validation loss 0.2718077301979065\n",
            "Epoch 8340/10000  training loss: 0.27184897661209106 Validation loss 0.2718007564544678\n",
            "Epoch 8350/10000  training loss: 0.27184274792671204 Validation loss 0.27179378271102905\n",
            "Epoch 8360/10000  training loss: 0.271836519241333 Validation loss 0.2717868387699127\n",
            "Epoch 8370/10000  training loss: 0.271830290555954 Validation loss 0.2717799246311188\n",
            "Epoch 8380/10000  training loss: 0.27182409167289734 Validation loss 0.2717730402946472\n",
            "Epoch 8390/10000  training loss: 0.27181798219680786 Validation loss 0.27176612615585327\n",
            "Epoch 8400/10000  training loss: 0.2718117833137512 Validation loss 0.2717592716217041\n",
            "Epoch 8410/10000  training loss: 0.27180567383766174 Validation loss 0.2717524468898773\n",
            "Epoch 8420/10000  training loss: 0.27179956436157227 Validation loss 0.27174562215805054\n",
            "Epoch 8430/10000  training loss: 0.2717934548854828 Validation loss 0.27173882722854614\n",
            "Epoch 8440/10000  training loss: 0.2717874050140381 Validation loss 0.27173203229904175\n",
            "Epoch 8450/10000  training loss: 0.2717813551425934 Validation loss 0.27172526717185974\n",
            "Epoch 8460/10000  training loss: 0.2717753052711487 Validation loss 0.2717185616493225\n",
            "Epoch 8470/10000  training loss: 0.27176928520202637 Validation loss 0.2717118263244629\n",
            "Epoch 8480/10000  training loss: 0.27176329493522644 Validation loss 0.27170512080192566\n",
            "Epoch 8490/10000  training loss: 0.2717573046684265 Validation loss 0.2716984450817108\n",
            "Epoch 8500/10000  training loss: 0.271751344203949 Validation loss 0.27169179916381836\n",
            "Epoch 8510/10000  training loss: 0.27174538373947144 Validation loss 0.2716851234436035\n",
            "Epoch 8520/10000  training loss: 0.2717394530773163 Validation loss 0.27167853713035583\n",
            "Epoch 8530/10000  training loss: 0.2717335522174835 Validation loss 0.27167192101478577\n",
            "Epoch 8540/10000  training loss: 0.27172771096229553 Validation loss 0.2716653347015381\n",
            "Epoch 8550/10000  training loss: 0.2717217803001404 Validation loss 0.2716587483882904\n",
            "Epoch 8560/10000  training loss: 0.27171590924263 Validation loss 0.2716522216796875\n",
            "Epoch 8570/10000  training loss: 0.2717100977897644 Validation loss 0.2716456949710846\n",
            "Epoch 8580/10000  training loss: 0.2717042863368988 Validation loss 0.2716391980648041\n",
            "Epoch 8590/10000  training loss: 0.2716984748840332 Validation loss 0.27163270115852356\n",
            "Epoch 8600/10000  training loss: 0.2716926634311676 Validation loss 0.27162623405456543\n",
            "Epoch 8610/10000  training loss: 0.2716869115829468 Validation loss 0.2716197967529297\n",
            "Epoch 8620/10000  training loss: 0.27168112993240356 Validation loss 0.27161335945129395\n",
            "Epoch 8630/10000  training loss: 0.2716754078865051 Validation loss 0.2716069519519806\n",
            "Epoch 8640/10000  training loss: 0.2716696858406067 Validation loss 0.27160054445266724\n",
            "Epoch 8650/10000  training loss: 0.27166399359703064 Validation loss 0.2715941369533539\n",
            "Epoch 8660/10000  training loss: 0.2716583013534546 Validation loss 0.2715877890586853\n",
            "Epoch 8670/10000  training loss: 0.2716526389122009 Validation loss 0.2715814709663391\n",
            "Epoch 8680/10000  training loss: 0.27164697647094727 Validation loss 0.2715751528739929\n",
            "Epoch 8690/10000  training loss: 0.271641343832016 Validation loss 0.27156883478164673\n",
            "Epoch 8700/10000  training loss: 0.2716357111930847 Validation loss 0.2715625762939453\n",
            "Epoch 8710/10000  training loss: 0.2716301381587982 Validation loss 0.2715563178062439\n",
            "Epoch 8720/10000  training loss: 0.2716245651245117 Validation loss 0.27155008912086487\n",
            "Epoch 8730/10000  training loss: 0.2716190218925476 Validation loss 0.2715438902378082\n",
            "Epoch 8740/10000  training loss: 0.2716135084629059 Validation loss 0.2715376615524292\n",
            "Epoch 8750/10000  training loss: 0.2716079652309418 Validation loss 0.27153149247169495\n",
            "Epoch 8760/10000  training loss: 0.27160245180130005 Validation loss 0.2715253233909607\n",
            "Epoch 8770/10000  training loss: 0.2715969383716583 Validation loss 0.27151918411254883\n",
            "Epoch 8780/10000  training loss: 0.2715914845466614 Validation loss 0.27151307463645935\n",
            "Epoch 8790/10000  training loss: 0.27158603072166443 Validation loss 0.2715069651603699\n",
            "Epoch 8800/10000  training loss: 0.2715805768966675 Validation loss 0.2715008854866028\n",
            "Epoch 8810/10000  training loss: 0.2715751826763153 Validation loss 0.2714948058128357\n",
            "Epoch 8820/10000  training loss: 0.27156975865364075 Validation loss 0.2714887261390686\n",
            "Epoch 8830/10000  training loss: 0.2715643644332886 Validation loss 0.2714827060699463\n",
            "Epoch 8840/10000  training loss: 0.2715590000152588 Validation loss 0.271476686000824\n",
            "Epoch 8850/10000  training loss: 0.271553635597229 Validation loss 0.27147069573402405\n",
            "Epoch 8860/10000  training loss: 0.2715483009815216 Validation loss 0.27146467566490173\n",
            "Epoch 8870/10000  training loss: 0.2715429663658142 Validation loss 0.2714587152004242\n",
            "Epoch 8880/10000  training loss: 0.2715376615524292 Validation loss 0.27145278453826904\n",
            "Epoch 8890/10000  training loss: 0.2715323269367218 Validation loss 0.2714468538761139\n",
            "Epoch 8900/10000  training loss: 0.27152708172798157 Validation loss 0.27144092321395874\n",
            "Epoch 8910/10000  training loss: 0.27152183651924133 Validation loss 0.271435022354126\n",
            "Epoch 8920/10000  training loss: 0.2715165615081787 Validation loss 0.2714291214942932\n",
            "Epoch 8930/10000  training loss: 0.27151134610176086 Validation loss 0.2714232802391052\n",
            "Epoch 8940/10000  training loss: 0.271506130695343 Validation loss 0.27141743898391724\n",
            "Epoch 8950/10000  training loss: 0.27150094509124756 Validation loss 0.27141159772872925\n",
            "Epoch 8960/10000  training loss: 0.2714957296848297 Validation loss 0.27140581607818604\n",
            "Epoch 8970/10000  training loss: 0.27149057388305664 Validation loss 0.2714000344276428\n",
            "Epoch 8980/10000  training loss: 0.27148541808128357 Validation loss 0.2713942229747772\n",
            "Epoch 8990/10000  training loss: 0.2714802920818329 Validation loss 0.2713884711265564\n",
            "Epoch 9000/10000  training loss: 0.2714751362800598 Validation loss 0.27138274908065796\n",
            "Epoch 9010/10000  training loss: 0.2714700400829315 Validation loss 0.27137699723243713\n",
            "Epoch 9020/10000  training loss: 0.2714649438858032 Validation loss 0.2713713049888611\n",
            "Epoch 9030/10000  training loss: 0.2714598774909973 Validation loss 0.27136558294296265\n",
            "Epoch 9040/10000  training loss: 0.271454781293869 Validation loss 0.271359920501709\n",
            "Epoch 9050/10000  training loss: 0.2714497745037079 Validation loss 0.2713542580604553\n",
            "Epoch 9060/10000  training loss: 0.271444708108902 Validation loss 0.27134862542152405\n",
            "Epoch 9070/10000  training loss: 0.27143970131874084 Validation loss 0.2713429927825928\n",
            "Epoch 9080/10000  training loss: 0.2714346647262573 Validation loss 0.2713373601436615\n",
            "Epoch 9090/10000  training loss: 0.2714296877384186 Validation loss 0.271331787109375\n",
            "Epoch 9100/10000  training loss: 0.27142471075057983 Validation loss 0.2713261842727661\n",
            "Epoch 9110/10000  training loss: 0.2714197635650635 Validation loss 0.2713206112384796\n",
            "Epoch 9120/10000  training loss: 0.27141478657722473 Validation loss 0.2713150382041931\n",
            "Epoch 9130/10000  training loss: 0.27140986919403076 Validation loss 0.2713095247745514\n",
            "Epoch 9140/10000  training loss: 0.2714049518108368 Validation loss 0.2713039815425873\n",
            "Epoch 9150/10000  training loss: 0.2714000344276428 Validation loss 0.27129846811294556\n",
            "Epoch 9160/10000  training loss: 0.27139514684677124 Validation loss 0.2712930142879486\n",
            "Epoch 9170/10000  training loss: 0.27139025926589966 Validation loss 0.2712875306606293\n",
            "Epoch 9180/10000  training loss: 0.2713853716850281 Validation loss 0.2712821066379547\n",
            "Epoch 9190/10000  training loss: 0.27138054370880127 Validation loss 0.27127665281295776\n",
            "Epoch 9200/10000  training loss: 0.27137577533721924 Validation loss 0.2712712287902832\n",
            "Epoch 9210/10000  training loss: 0.27137094736099243 Validation loss 0.27126580476760864\n",
            "Epoch 9220/10000  training loss: 0.2713661193847656 Validation loss 0.27126041054725647\n",
            "Epoch 9230/10000  training loss: 0.2713613510131836 Validation loss 0.2712550759315491\n",
            "Epoch 9240/10000  training loss: 0.2713565230369568 Validation loss 0.2712496817111969\n",
            "Epoch 9250/10000  training loss: 0.27135181427001953 Validation loss 0.2712443470954895\n",
            "Epoch 9260/10000  training loss: 0.2713470458984375 Validation loss 0.2712389826774597\n",
            "Epoch 9270/10000  training loss: 0.27134230732917786 Validation loss 0.2712336778640747\n",
            "Epoch 9280/10000  training loss: 0.2713375985622406 Validation loss 0.2712283730506897\n",
            "Epoch 9290/10000  training loss: 0.27133288979530334 Validation loss 0.2712230980396271\n",
            "Epoch 9300/10000  training loss: 0.2713281810283661 Validation loss 0.27121782302856445\n",
            "Epoch 9310/10000  training loss: 0.2713235020637512 Validation loss 0.2712125778198242\n",
            "Epoch 9320/10000  training loss: 0.27131885290145874 Validation loss 0.271207332611084\n",
            "Epoch 9330/10000  training loss: 0.27131417393684387 Validation loss 0.27120208740234375\n",
            "Epoch 9340/10000  training loss: 0.27130958437919617 Validation loss 0.2711969017982483\n",
            "Epoch 9350/10000  training loss: 0.2713049352169037 Validation loss 0.27119168639183044\n",
            "Epoch 9360/10000  training loss: 0.2713003158569336 Validation loss 0.271186500787735\n",
            "Epoch 9370/10000  training loss: 0.2712956964969635 Validation loss 0.2711813449859619\n",
            "Epoch 9380/10000  training loss: 0.2712911367416382 Validation loss 0.27117618918418884\n",
            "Epoch 9390/10000  training loss: 0.27128657698631287 Validation loss 0.27117103338241577\n",
            "Epoch 9400/10000  training loss: 0.27128198742866516 Validation loss 0.2711659073829651\n",
            "Epoch 9410/10000  training loss: 0.27127745747566223 Validation loss 0.2711608111858368\n",
            "Epoch 9420/10000  training loss: 0.2712729573249817 Validation loss 0.2711557149887085\n",
            "Epoch 9430/10000  training loss: 0.27126842737197876 Validation loss 0.2711506485939026\n",
            "Epoch 9440/10000  training loss: 0.27126389741897583 Validation loss 0.2711455821990967\n",
            "Epoch 9450/10000  training loss: 0.2712594270706177 Validation loss 0.27114051580429077\n",
            "Epoch 9460/10000  training loss: 0.27125492691993713 Validation loss 0.27113547921180725\n",
            "Epoch 9470/10000  training loss: 0.271250456571579 Validation loss 0.2711304724216461\n",
            "Epoch 9480/10000  training loss: 0.2712460458278656 Validation loss 0.271125465631485\n",
            "Epoch 9490/10000  training loss: 0.27124160528182983 Validation loss 0.27112048864364624\n",
            "Epoch 9500/10000  training loss: 0.27123719453811646 Validation loss 0.2711154818534851\n",
            "Epoch 9510/10000  training loss: 0.2712327539920807 Validation loss 0.27111053466796875\n",
            "Epoch 9520/10000  training loss: 0.2712283730506897 Validation loss 0.2711055874824524\n",
            "Epoch 9530/10000  training loss: 0.2712239623069763 Validation loss 0.2711006700992584\n",
            "Epoch 9540/10000  training loss: 0.2712196111679077 Validation loss 0.2710956931114197\n",
            "Epoch 9550/10000  training loss: 0.2712152600288391 Validation loss 0.2710908055305481\n",
            "Epoch 9560/10000  training loss: 0.2712109088897705 Validation loss 0.2710859179496765\n",
            "Epoch 9570/10000  training loss: 0.2712065577507019 Validation loss 0.27108103036880493\n",
            "Epoch 9580/10000  training loss: 0.2712022364139557 Validation loss 0.27107614278793335\n",
            "Epoch 9590/10000  training loss: 0.2711979150772095 Validation loss 0.27107134461402893\n",
            "Epoch 9600/10000  training loss: 0.27119362354278564 Validation loss 0.27106648683547974\n",
            "Epoch 9610/10000  training loss: 0.27118930220603943 Validation loss 0.27106165885925293\n",
            "Epoch 9620/10000  training loss: 0.2711850702762604 Validation loss 0.2710568308830261\n",
            "Epoch 9630/10000  training loss: 0.27118080854415894 Validation loss 0.2710520327091217\n",
            "Epoch 9640/10000  training loss: 0.2711765766143799 Validation loss 0.2710472643375397\n",
            "Epoch 9650/10000  training loss: 0.27117231488227844 Validation loss 0.27104246616363525\n",
            "Epoch 9660/10000  training loss: 0.271168053150177 Validation loss 0.2710377275943756\n",
            "Epoch 9670/10000  training loss: 0.2711638808250427 Validation loss 0.2710329592227936\n",
            "Epoch 9680/10000  training loss: 0.27115964889526367 Validation loss 0.27102822065353394\n",
            "Epoch 9690/10000  training loss: 0.271155446767807 Validation loss 0.2710235118865967\n",
            "Epoch 9700/10000  training loss: 0.27115127444267273 Validation loss 0.2710188031196594\n",
            "Epoch 9710/10000  training loss: 0.27114710211753845 Validation loss 0.27101412415504456\n",
            "Epoch 9720/10000  training loss: 0.27114295959472656 Validation loss 0.2710094451904297\n",
            "Epoch 9730/10000  training loss: 0.2711388170719147 Validation loss 0.2710047662258148\n",
            "Epoch 9740/10000  training loss: 0.27113473415374756 Validation loss 0.27100011706352234\n",
            "Epoch 9750/10000  training loss: 0.2711305618286133 Validation loss 0.27099546790122986\n",
            "Epoch 9760/10000  training loss: 0.27112647891044617 Validation loss 0.27099084854125977\n",
            "Epoch 9770/10000  training loss: 0.27112236618995667 Validation loss 0.2709862291812897\n",
            "Epoch 9780/10000  training loss: 0.27111825346946716 Validation loss 0.27098163962364197\n",
            "Epoch 9790/10000  training loss: 0.27111420035362244 Validation loss 0.2709770202636719\n",
            "Epoch 9800/10000  training loss: 0.2711101770401001 Validation loss 0.27097246050834656\n",
            "Epoch 9810/10000  training loss: 0.27110612392425537 Validation loss 0.27096790075302124\n",
            "Epoch 9820/10000  training loss: 0.27110207080841064 Validation loss 0.2709633708000183\n",
            "Epoch 9830/10000  training loss: 0.2710980176925659 Validation loss 0.270958811044693\n",
            "Epoch 9840/10000  training loss: 0.2710939943790436 Validation loss 0.27095428109169006\n",
            "Epoch 9850/10000  training loss: 0.27109000086784363 Validation loss 0.27094975113868713\n",
            "Epoch 9860/10000  training loss: 0.2710860073566437 Validation loss 0.2709452509880066\n",
            "Epoch 9870/10000  training loss: 0.2710820436477661 Validation loss 0.27094078063964844\n",
            "Epoch 9880/10000  training loss: 0.27107807993888855 Validation loss 0.2709363102912903\n",
            "Epoch 9890/10000  training loss: 0.2710740864276886 Validation loss 0.27093183994293213\n",
            "Epoch 9900/10000  training loss: 0.2710701525211334 Validation loss 0.270927369594574\n",
            "Epoch 9910/10000  training loss: 0.27106618881225586 Validation loss 0.2709229588508606\n",
            "Epoch 9920/10000  training loss: 0.27106228470802307 Validation loss 0.27091851830482483\n",
            "Epoch 9930/10000  training loss: 0.2710583806037903 Validation loss 0.27091407775878906\n",
            "Epoch 9940/10000  training loss: 0.2710544466972351 Validation loss 0.27090969681739807\n",
            "Epoch 9950/10000  training loss: 0.2710505723953247 Validation loss 0.2709053158760071\n",
            "Epoch 9960/10000  training loss: 0.2710466980934143 Validation loss 0.2709009051322937\n",
            "Epoch 9970/10000  training loss: 0.2710428237915039 Validation loss 0.2708965837955475\n",
            "Epoch 9980/10000  training loss: 0.2710389792919159 Validation loss 0.2708922028541565\n",
            "Epoch 9990/10000  training loss: 0.2710351347923279 Validation loss 0.2708878517150879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss\n",
        "plt.plot(loss, label=\"Training Loss\")\n",
        "plt.plot(loss_val, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "\n",
        "# I'm not sure why the graph looks different than the textbook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "a1fb7BbW8vQN",
        "outputId": "6951496d-69a7-4690-b766-9f36d4f43bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cross Entropy Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(np.float32(X_val))  # Make predictions on the validation data"
      ],
      "metadata": {
        "id": "3RCctJMm9mIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score  # Import mean squared error and R-squared score\n",
        "accuracy = sum(np.argmax(Y_pred,1) == np.argmax(np.array(y_val),1))/len(y_val)  # Calculate accuracy\n",
        "print(accuracy)  # Print the accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dUbpQuT-rg7",
        "outputId": "fd282bfc-b21a-49d2-b3fa-3f1d2ceb0454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.540625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score  # Import confusion matrix and accuracy score\n",
        "accuracy = accuracy_score(np.argmax(np.array(y_val),1), np.argmax(Y_pred,1))  # Calculate accuracy\n",
        "print(accuracy)  # Print the accuracy\n",
        "import seaborn as sns\n",
        "cm = confusion_matrix(np.argmax(np.array(y_val),1), np.argmax(Y_pred,1))  # Calculate confusion matrix\n",
        "sns.heatmap(cm,annot=True,fmt='2.0f')  # Plot the confusion matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "rVzvd45--v5P",
        "outputId": "7fd2d780-96a2-44c2-c615-31a76f82eec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.value_counts()  # Check the distribution of the target variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LK-Alks-3DN",
        "outputId": "d1c54576-5141-4603-9f22-887572f36ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quality\n",
              "1    855\n",
              "2    744\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning Use Case Example**"
      ],
      "metadata": {
        "id": "z_6LLf-9vskv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning can be used to train a neural network to recognize objects in images. For example, you can train a model to identify whether a photo contains a beach or a mountain. By feeding the model thousands of labeled images of beaches and mountains, the neural network can learn to distinguish between the two. Once trained, the model can accurately predict and recognize images of either.\n"
      ],
      "metadata": {
        "id": "YMJ-7_UovwkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My experience using this tool**"
      ],
      "metadata": {
        "id": "rQ8mfE5cwwOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding TensorFlow and building neural networks were initially fairly difficult to overcome. I had to revert to an earlier version of TensorFlow, which is to be expected. The code from the textbook doesn't always agree with tensorflow 2.0. Despite these challenges, I managed to learn a lot about neural networks and their applications. I successfully implemented a single neuron model and worked through various issues with data normalization and model training. The classifcation and regressions were naturally more complex. Overall, the experience was challenging it gave me valuable insights into deep learning and how MLP models are constructed. I believe I can take them and apply them to a future project.\n",
        "\n"
      ],
      "metadata": {
        "id": "mnbajADmw8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Networks**"
      ],
      "metadata": {
        "id": "5L0kzagQhksm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are computational models inspired by the human brain. They consist of interconnected nodes (neurons) organized in layers. Each neuron receives inputs, processes them using weights, and applies an activation function to generate outputs. Neural networks can handle complex patterns and large datasets, making them ideal for tasks like image recognition, natural language processing, and IoT applications."
      ],
      "metadata": {
        "id": "BLBNaeCNjekP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Flow**"
      ],
      "metadata": {
        "id": "SC_1EEienMAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow is an open-source library created by Google for building and training machine learning models. It provides a flexible platform for building various types of neural networks, including deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). TensorFlow uses tensors (multi-dimensional arrays) to represent data and operations on data. It is designed to run on different platforms, such as CPUs, GPUs, and even mobile devices, making it very versatile. TensorFlow is widely used for tasks like image recognition, natural language processing, and time series analysis due to its efficiency and scalability."
      ],
      "metadata": {
        "id": "KXzXRAEInOz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "wnVsEzP2kh0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) are a type of neural network particularly good at processing images. They work by using special layers called convolutional layers that automatically detect important features in images, like edges, textures, and shapes. These layers scan the image with small filters and create feature maps that highlight these important parts. CNNs are widely used for tasks such as image recognition, object detection, and facial recognition because they can handle the complex structure of images very efficiently. They are also used in applications like self-driving cars, where they help recognize and understand surroundings."
      ],
      "metadata": {
        "id": "kU3P-gGRhoAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Networks - LeNet to recognize handwritten digits - Code Sample**"
      ],
      "metadata": {
        "id": "4jqt7SQjjGHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NaJuFEkMhmdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your Architecture here\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# from tensorflow.contrib.layers import flatten <---- Tensoflow.contrib is not available in Tensowflow 2.x\n",
        "# instead I used updated keras code appropriate for Tensor flow 2.X below\n",
        "\n",
        "# Flatten a tensor\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "\n",
        "class my_LeNet:\n",
        "    def __init__(self, d, n, mu = 0, sigma = 0.1, lr = 0.001):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.n = n\n",
        "        self.x = tf.placeholder(tf.float32, (None, d, d, 1))  # place holder for input image dimension 28 x 28\n",
        "        self.y = tf.placeholder(tf.int32, (None,n))\n",
        "        self.keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
        "\n",
        "\n",
        "        self.logits = self.model(self.x)\n",
        "\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)\n",
        "        self.loss = tf.reduce_mean(cross_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "        self.train = optimizer.minimize(self.loss)\n",
        "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "\n",
        "    def model(self,x):\n",
        "        # Build Architecture\n",
        "        keep_prob = 0.7\n",
        "        # Layer 1: Convolutional. Filter 5x5 num_filters = 6 Input_depth =1\n",
        "        conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = self.mu, stddev = self.sigma))\n",
        "        conv1_b = tf.Variable(tf.zeros(6))\n",
        "        conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
        "        conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "        # Max Pool 1\n",
        "        self.conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "\n",
        "        # Layer 2: Convolutional. Filter 5x5 num_filters = 16  Input_depth =6\n",
        "        conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = self.mu, stddev = self.sigma))\n",
        "        conv2_b = tf.Variable(tf.zeros(16))\n",
        "        conv2   = tf.nn.conv2d(self.conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
        "        conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "        # Max Pool 2.\n",
        "        self.conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "        # Flatten.\n",
        "        fc0   = flatten(self.conv2)\n",
        "        print(\"x shape:\",fc0.get_shape())\n",
        "\n",
        "        # Layer 3: Fully Connected. Input = fc0.get_shape[-1]. Output = 120.\n",
        "        fc1_W = tf.Variable(tf.truncated_normal(shape=(256, 120), mean = self.mu, stddev = self.sigma))\n",
        "        fc1_b = tf.Variable(tf.zeros(120))\n",
        "        fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
        "        fc1    = tf.nn.relu(fc1)\n",
        "\n",
        "        # Dropout\n",
        "        x = tf.nn.dropout(fc1, keep_prob)\n",
        "\n",
        "        # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "        fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = self.mu, stddev = self.sigma))\n",
        "        fc2_b  = tf.Variable(tf.zeros(84))\n",
        "        fc2    = tf.matmul(x, fc2_W) + fc2_b\n",
        "        fc2    = tf.nn.relu(fc2)\n",
        "\n",
        "        # Dropout\n",
        "        x = tf.nn.dropout(fc2, keep_prob)\n",
        "\n",
        "        # Layer 6: Fully Connected. Input = 120. Output = n_classes.\n",
        "        fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, self.n), mean = self.mu, stddev = self.sigma))\n",
        "        fc3_b  = tf.Variable(tf.zeros(self.n))\n",
        "        logits = tf.matmul(x, fc3_W) + fc3_b\n",
        "        #logits = tf.nn.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "    def fit(self,X,Y,X_val,Y_val,epochs=10, batch_size=100):\n",
        "        X_train, y_train = X, Y\n",
        "        num_examples = len(X_train)\n",
        "        l = []\n",
        "        val_l = []\n",
        "        max_val = 0\n",
        "        for i in range(epochs):\n",
        "            total = 0\n",
        "            for offset in range(0, num_examples, batch_size):  # Learn Batch wise\n",
        "                end = offset + batch_size\n",
        "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "                _, loss = self.sess.run([self.train,self.loss], feed_dict={self.x: batch_x, self.y: batch_y})\n",
        "                total += loss\n",
        "            l.append(total/num_examples)\n",
        "            accuracy_val = self.sess.run(self.accuracy, feed_dict={self.x: X_val, self.y: Y_val})\n",
        "            accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})\n",
        "            loss_val = self.sess.run(self.loss, feed_dict={self.x:X_val,self.y:Y_val})\n",
        "            val_l.append(loss_val)\n",
        "            print(\"EPOCH {}/{} loss is {:.3f} training_accuracy {:.3f} and validation accuracy is {:.3f}\".\\\n",
        "                  format(i+1,epochs,total/num_examples, accuracy, accuracy_val))\n",
        "            if accuracy_val > max_val:\n",
        "                save_path = self.saver.save(self.sess, \"/tmp/lenet1.ckpt\")\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "                max_val = accuracy_val\n",
        "\n",
        "        self.saver.restore(self.sess, \"/tmp/lenet1.ckpt\")\n",
        "        print(\"Restored model with highest validation accuracy\")\n",
        "        accuracy_val = self.sess.run(self.accuracy, feed_dict={self.x: X_val, self.y: Y_val})\n",
        "        accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})\n",
        "        return l,val_l, accuracy, accuracy_val\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sess.run(self.logits,feed_dict={self.x:X})"
      ],
      "metadata": {
        "id": "xrrrpZfOn0lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Read the data and create train, validation, and test datasets\n",
        "    data = pd.read_csv('train.csv')\n",
        "    train = data.sample(frac=0.8, random_state=255)   # This ensures always 80% of data is training and rest Validation unlike using np.random\n",
        "    val = data.drop(train.index)\n",
        "    test = pd.read_csv('test.csv')\n",
        "    return train, val, test\n",
        "\n",
        "def create_data(df):\n",
        "    labels = df.loc[:]['label']\n",
        "    y_one_hot = pd.get_dummies(labels).astype(np.uint8)\n",
        "    y = y_one_hot.values  # One Hot encode the labels\n",
        "    x = df.iloc[:, :-1].values\n",
        "    x = x.astype(np.float64)    # Use np.float64 instead of np.float\n",
        "    # Normalize data\n",
        "    x = np.multiply(x, 1.0 / 255.0)\n",
        "    x = x.reshape(-1, 32, 32, 1)  # Return each image as 32x32x1\n",
        "    return x, y\n",
        "\n",
        "train, val, test = load_data()\n",
        "X_train, y_train = create_data(train)\n",
        "X_val, y_val = create_data(val)\n",
        "X_test = (test.iloc[:, :-1].values).astype(np.float64)  # Use np.float64 instead of np.float\n",
        "X_test = np.multiply(X_test, 1.0 / 255.0)\n",
        "X_test = X_test.reshape(-1, 32, 32, 1)  # Return each image as 32x32x1"
      ],
      "metadata": {
        "id": "CZC81pbJoerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had to change \"np.float\" to \"np.float64\" because it was depreciated"
      ],
      "metadata": {
        "id": "HFXnEcbPzUzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take subset of training data\n",
        "x_train_subset = X_train[:12]\n",
        "\n",
        "# visualize subset of training data\n",
        "fig = plt.figure(figsize=(20, 2))\n",
        "for i in range(0, len(x_train_subset)):\n",
        "    ax = fig.add_subplot(1, 12, i + 1)\n",
        "    ax.imshow(x_train_subset[i].reshape(32, 32), cmap='gray')  # Reshape to 32x32\n",
        "fig.suptitle('Subset of Original Training Images', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "KqVmkUOOz6B1",
        "outputId": "c4ebc759-11e3-4ac2-8dbb-af485e36e930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-32c4f47a6263>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# take subset of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# visualize subset of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = len(X_train)\n",
        "\n",
        "# Number of validation examples\n",
        "n_validation = len(X_val)\n",
        "\n",
        "# Number of testing examples.\n",
        "n_test = len(X_test)\n",
        "\n",
        "# What's the shape of an handwritten digits?\n",
        "image_shape = X_train.shape[1:-1]\n",
        "\n",
        "# How many unique classes/labels there are in the dataset.\n",
        "n_classes = y_train.shape[-1]\n",
        "\n",
        "print(\"Number of training examples =\", n_train)\n",
        "print(\"Number of Validation examples =\", n_validation)\n",
        "print(\"Number of testing examples =\", n_test)\n",
        "print(\"Image data shape =\", image_shape)\n",
        "print(\"Number of classes =\", n_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "zOGb18E00Wdw",
        "outputId": "3c7a2333-3a6a-46aa-b511-defdfb16aa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d0d86cc8c5fd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Number of validation examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data values\n",
        "d = image_shape[0]\n",
        "n = n_classes\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train,y_train)"
      ],
      "metadata": {
        "id": "JhgusYPZ0Zy1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "a41a5853-eaad-442a-8db5-a434212c7c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_shape' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f6f7413f460e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the data values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_shape' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Model\n",
        "my_model = my_LeNet(d, n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "je17AQax0dB4",
        "outputId": "34bbc9c3-2397-4ac9-b9f8-f86817424d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2c590400740c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_LeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could not continue because I got this error. I think the code is not compatible with tensorflow in Google Collabs."
      ],
      "metadata": {
        "id": "XundPwnc4of8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use case examples**"
      ],
      "metadata": {
        "id": "8qM-H43g5ydE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smartphones like the iPhone use Convolutional Neural Networks to analyze and recognize facial features for unlocking the device. The Convolutional Neural Networks processes the image captured by the front camera, extracting unique features of the user's face and then matches these features to the stored facial data."
      ],
      "metadata": {
        "id": "XI5eQtKI54Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My experience using this tool**"
      ],
      "metadata": {
        "id": "QXW3ZyU_41ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a convuluted neural network using the LeNet architecture was challenging due to Google Collab no longer supporting from TensorFlow 1.x and changing to to 2.x. The primary issue was the deprecation of tf.placeholder, leading to an AttributeError. I initially used tf.compat.v1 to enable compatibility mode but ultimately adopted TensorFlow 2.x practices using tf.keras. I became stuck on the \"# Create the Model\n",
        "my_model = my_LeNet(d, n)\" section of the code because I got an error saying  module 'tensorflow' has no attribute 'placeholder'\n"
      ],
      "metadata": {
        "id": "DncTDERL43pI"
      }
    }
  ]
}